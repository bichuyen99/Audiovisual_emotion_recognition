# Audiovisual Emotion Recognition from Videos

This repository contains code and resources developed for a master's thesis on multimodal emotion recognition. The research focuses on building efficient neural networks that analyze facial expressions and audio cues in videos to recognize emotions. 

**Key Tasks**

The models are evaluated on three affective behavior analysis (ABAW) challenges using the Aff-Wild2 dataset:

1. Expression Recognition (EXPR)
2. Action Unit (AU) Detection
3. Valence-Arousal (VA) Estimation

**Repository Structure**

* **`dataset.py`:**  Preprocesses and prepares the Aff-Wild2 dataset for model input.
* **`extract_features.py`:** 
** Extracts visual features using the https://github.com/av-savchenko/face-emotion-recognition/blob/main/models/affectnet_emotions/enet_b2_8_best.pt[EfficientNet-B2] model from the https://github.com/av-savchenko/face-emotion-recognition/tree/main[HSEmotion library].
** Extracts audio features using https://github.com/tensorflow/models/tree/master/research/audioset/vggish[VGGish] and https://github.com/facebookresearch/fairseq/tree/main/fairseq/models/wav2vec[Wav2Vec2].
* **`metrics.py`:** Calculates performance metrics (F1 score, Mean CCC).
* **`models.py`:**  Implements the core architectures for multimodal fusion:
** Early fusion with Transformer encoder.
** Early fusion with Multilayer Perceptron (MLP).
* **`train_test.py`:**  Contains scripts for model training and evaluation on the Aff-Wild2 dataset.
* **`smooth_prediction.py`:**  Implements smoothing techniques (mean and median filters) for prediction refinement.
* **`adaptive_frame_rate.py`:** Implements an adaptive frame rate strategy for computational efficiency.
* **`audiovisual_emo_reg.ipynb`:** Jupyter Notebook demonstrating the complete workflow (preprocessing to evaluation).
* **`best_model/`:**  Folder for saving the best-performing models for each task.

**Results**

Below table presents a summary of the results achieved on the Aff-Wild2 validation set using different feature combinations and model architectures.  Detailed analysis and discussion can be found in the accompanying thesis document.

[cols="9", options="header"]
|=======
| Facial feature extractor | Audio feature extractor | Model                     | EXPR F1 score | Time   | AU F1 score | Time   | VA Mean CCC | Time  
.8+|EfficientNet-B2 (cropped aligned images)| no   .3+| Transformer Encoder      | 0.31          | 29.5s  | 0.455      | 2.52s  | 0.472       | 1.73s  
                          | VGGish                                               | 0.316         | 33s    | 0.439      | 2.72   | 0.428       | 1.95s  
                          | Wav2Vec2                                             | 0.33          | 34.4s  | 0.449      | 2.75s  | 0.561       | 1.85s  
                          | no                      .3+| Multilayer Perceptron   | 0.327         | 8.93s  | 0.443      | 1.39s  | 0.605       | 773ms  
                          | VGGish                                               | 0.379         | 11.4s  | 0.423      | 1.53s  | 0.608       | 904ms  
                         .3+| Wav2Vec2                                           | 0.394         | 12s    | 0.463      | 1.58s  | 0.647       | 1.51s  
                                            | MLP, smooth all frames    | **0.454**         | 25.64s | **0.496**      | 15.9s  | **0.685**       | 1.022s
                                                    | MLP, adaptive frame rate | 0.448         | 9.93s  | 0.492      | 10.5s  | 0.685       | 0.53s  
|=======

