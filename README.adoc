# Audiovisual Emotion Recognition from Videos

This repository contains code and resources developed for a master's thesis on multimodal emotion recognition. The research focuses on building efficient neural networks that analyze facial expressions and audio cues in videos to recognize emotions. 

**Key Tasks**

The models are evaluated on three affective behavior analysis (ABAW) challenges using the Aff-Wild2 dataset:

1. Expression Recognition (EXPR)
2. Action Unit (AU) Detection
3. Valence-Arousal (VA) Estimation

**Repository Structure**

* **`dataset.py`:**  Preprocesses and prepares the Aff-Wild2 dataset for model input.
* **`extract_features.py`:** 
** Extracts visual features using the https://github.com/av-savchenko/face-emotion-recognition/blob/main/models/affectnet_emotions/enet_b2_8_best.pt[EfficientNet-B2] model from the https://github.com/av-savchenko/face-emotion-recognition/tree/main[HSEmotion library].
** Extracts audio features using https://github.com/tensorflow/models/tree/master/research/audioset/vggish[VGGish] and https://github.com/facebookresearch/fairseq/tree/main/fairseq/models/wav2vec[Wav2Vec2].
* **`metrics.py`:** Calculates performance metrics (F1 score, Mean CCC).
* **`models.py`:**  Implements the core architectures for multimodal fusion:
** Early fusion with Transformer encoder.
** Early fusion with Multilayer Perceptron (MLP).
* **`train_test.py`:**  Contains scripts for model training and evaluation on the Aff-Wild2 dataset.
* **`smooth_prediction.py`:**  Implements smoothing techniques (mean and median filters) for prediction refinement.
* **`adaptive_frame_rate.py`:** Implements an adaptive frame rate strategy for computational efficiency.
* **`audiovisual_emo_reg.ipynb`:** Jupyter Notebook demonstrating the complete workflow.
* **`testing.ipynb`:** Jupyter Notebook demonstrating the evaluation.
* **`best_model/`:**  Folder for saving the best-performing models for each task.

**Results**

Below table presents a summary of the results achieved on the Aff-Wild2 validation set using different feature combinations and model architectures.  Detailed analysis and discussion can be found in the accompanying thesis document.

[cols="9", options="header"]
|=======
| Facial feature extractor | Audio feature extractor | Model                     | EXPR F1 score | Time   | AU F1 score | Time   | VA Mean CCC | Time  
.9+|EfficientNet-B2 (cropped aligned images)| no   .3+| Transformer Encoder      | 0.31          | 29.5s  | 0.509      | 88s  | 0.324       | 55.2s  
                          | VGGish                                               | 0.316         | 33s    | 0.502      | 81s  | 0.376       | 57.4s  
                          | Wav2Vec2                                             | 0.33          | 34.4s  | 0.497      | 89s  | 0.394       | 58.9s  
                          | no                      .3+| Multilayer Perceptron   | 0.327         | 8.93s  | 0.521      | 52.5s  | 0.415       | 26.9s  
                          | VGGish                                               | 0.379         | 11.4s  | 0.52      | 48.6s  | 0.444       |   29.4s
                         .3+| Wav2Vec2                                           | 0.394         | 12s    | 0.513      | 50.3s  | 0.459       | 30.3s  
                                                     | MLP, smooth all frames    | **0.457**      | 31.65s | 0.525      | 15.73s  | 0.493       | 30.3s
                                                    | MLP, adaptive frame rate | 0.455         | 13.91s  | -      | -  | -       | -
                          | Wav2Vec2 + VGGish       | MLP, smooth all frames    | -      | -    | 0.534      | 17.3s  | **0.510**       | 13.6s
| EfficientNet-B2 + EfficientNet-B0 | VGGish | MLP, smooth all frames           | -       | -   | **0.54** | 22.2s   | -       | -
|=======

