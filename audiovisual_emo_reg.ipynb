{"cells":[{"cell_type":"markdown","metadata":{"id":"D-2pUaxmWffM"},"source":["## Library"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29303,"status":"ok","timestamp":1716215928325,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"6KZakgtAWYL4","outputId":"fabafd3f-9889-404e-bd64-45e59e9f9c67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75922,"status":"ok","timestamp":1715620538835,"user":{"displayName":"Nguyễn Nguyễn","userId":"15341849881319858303"},"user_tz":-180},"id":"Rqhy9k1j9L6q","outputId":"4078eef4-2910-4fb2-f27c-7ad9551f0257"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting timm\n","  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting resampy\n","  Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from resampy) (1.25.2)\n","Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.10/dist-packages (from resampy) (0.58.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy) (0.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.11.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->timm)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch->timm)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n","Installing collected packages: pydub, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, resampy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pydub-0.25.1 resampy-0.4.3 timm-0.9.16\n"]}],"source":["!pip install pydub timm resampy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hIUAzwmoJKwz"},"outputs":[],"source":["from pydub import AudioSegment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kz2jt7ThWh34"},"outputs":[],"source":["import os\n","import torch\n","from tqdm import tqdm\n","import json\n","import time\n","import requests\n","import torchaudio\n","import numpy as np\n","import pickle\n","import cv2\n","from PIL import Image\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import pandas as pd\n","import math\n","import torch.nn as nn\n","import random\n","from torch import optim\n","from sklearn.metrics import f1_score, accuracy_score\n","from sklearn.utils.class_weight import compute_class_weight\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOgcvBAfjNTH"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uuVAI-13Affl"},"outputs":[],"source":["root = '/content/drive/MyDrive/MSc/Thesis'"]},{"cell_type":"code","source":["def setup_seed(seed):\n","     torch.manual_seed(seed)\n","     torch.cuda.manual_seed_all(seed)\n","     np.random.seed(seed)\n","     random.seed(seed)\n","     torch.backends.cudnn.deterministic = True"],"metadata":{"id":"zBZofuPUj_tq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["setup_seed(20)"],"metadata":{"id":"yx3Rgnpmrg1_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tgoCljH14WXu"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":361,"status":"ok","timestamp":1715165670150,"user":{"displayName":"Nguyễn Nguyễn","userId":"15341849881319858303"},"user_tz":-180},"id":"XYRIX0SkKJZf","outputId":"fde84f8a-ebec-4e55-fb88-706de15c0c2e"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [00:00<00:00, 218785.29it/s]\n","100%|██████████| 76/76 [00:00<00:00, 121342.64it/s]\n","100%|██████████| 248/248 [00:00<00:00, 211119.83it/s]\n","100%|██████████| 70/70 [00:00<00:00, 226719.14it/s]\n","100%|██████████| 295/295 [00:00<00:00, 196555.95it/s]\n","100%|██████████| 105/105 [00:00<00:00, 110737.22it/s]\n"]}],"source":["#test_list = []\n","a = []\n","for d in ['VA_Estimation_Challenge','EXPR_Recognition_Challenge','AU_Detection_Challenge']:\n","    data_dir=os.path.join(root,'data/Annotations',d)\n","    for k in ['Train_Set','Validation_Set']:\n","        data_label=os.path.join(data_dir,k)\n","        with open(os.path.join(data_dir,f'{k}.txt'), 'w') as f:\n","            for filename in tqdm(os.listdir(data_label)):\n","                fn, ext = os.path.splitext(os.path.basename(filename))\n","                if ext.lower()=='.txt':\n","                    f.write(fn+'\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":572,"status":"ok","timestamp":1715102972996,"user":{"displayName":"Ulysses Vo","userId":"16301520817637769988"},"user_tz":-180},"id":"ea5gmFdmV6EC","outputId":"ab19ebca-9c67-4306-b1c3-871a73abd656"},"outputs":[{"name":"stdout","output_type":"stream","text":["VA_Estimation_Challenge\n","Train_set:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 285/285 [00:00<00:00, 300799.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val_set:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 71/71 [00:00<00:00, 387250.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Test_set:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 76/76 [00:00<00:00, 78090.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["EXPR_Recognition_Challenge\n","Train_set:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 199/199 [00:00<00:00, 674205.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val_set:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 49/49 [00:00<00:00, 349525.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Test_set:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 70/70 [00:00<00:00, 460190.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["AU_Detection_Challenge\n","Train_set:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 236/236 [00:00<00:00, 772118.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Val_set:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 59/59 [00:00<00:00, 391556.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Test_set:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 105/105 [00:00<00:00, 522422.21it/s]\n"]}],"source":["test_list = []\n","\n","for d in ['VA_Estimation_Challenge','EXPR_Recognition_Challenge','AU_Detection_Challenge']:\n","    print(d)\n","    with open(os.path.join(root,f'data/Annotations/{d}/Train_Set.txt'), 'r') as f:\n","        files = f.read().splitlines()\n","    random.shuffle(files)\n","    ratio = int(len(files)/5) # 20%\n","    val_set = files[:ratio]\n","    test_list.extend(val_set)\n","    train_set = files[ratio:]\n","    print('Train_set:')\n","    with open(os.path.join(root,f'data/Annotations/{d}/Train.txt'), 'w') as f:\n","        for ftrain in tqdm(train_set):\n","            f.write(ftrain+'\\n')\n","    print('Val_set:')\n","    with open(os.path.join(root,f'data/Annotations/{d}/Val.txt'), 'w') as f:\n","        for fval in tqdm(val_set):\n","            f.write(fval+'\\n')\n","\n","    with open(os.path.join(root,f'data/Annotations/{d}/Validation_Set.txt'), 'r') as f:\n","        test_set = f.read().splitlines()\n","    test_list.extend(test_set)\n","    print('Test_set:')\n","    with open(os.path.join(root,f'data/Annotations/{d}/Test.txt'), 'w') as f:\n","        for ftest in tqdm(test_set):\n","            f.write(ftest+'\\n')\n","with open(os.path.join(root,'data/test_list.txt'), 'w') as f:\n","    for fn in test_list:\n","        f.write(fn+'\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HU8CMY9CS-7K"},"outputs":[],"source":["def get_names(vid, id):\n","    name = \"\"\n","    if id>=0 and id<10:\n","        name = f\"{vid}/0000\" + str(id) + \".jpg\"\n","    elif id>=10 and id<100:\n","        name = f\"{vid}/000\" + str(id) + \".jpg\"\n","    elif id>=100 and id<1000:\n","        name = f\"{vid}/00\" + str(id) + \".jpg\"\n","    elif id>=1000 and id<10000:\n","        name = f\"{vid}/0\" + str(id) + \".jpg\"\n","    else:\n","        name = f\"{vid}/\" + str(id) + \".jpg\"\n","    return name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NibryvaO4WXv"},"outputs":[],"source":["def load_feature_cache(feature_names):\n","\n","    for feature_name in feature_names:\n","        print('processing:', feature_name)\n","        feat_root = os.path.join(root + '/models/ABAW6', feature_name)\n","        save_root = feat_root+'.pkl'\n","        if os.path.exists(save_root):\n","            continue\n","        filenames = os.listdir(feat_root)[:]\n","        feat = {}\n","        for fname in tqdm(filenames):\n","            vname = fname.split('.')[0]\n","            if filenames[0].endswith('.npy'):\n","                fea = np.load(os.path.join(feat_root, fname), allow_pickle=True).tolist()\n","            elif filenames[0].endswith('.pkl'):\n","                with open(os.path.join(feat_root, fname), 'rb') as f:\n","                    fea = pickle.load(f)\n","            feat[vname] = fea\n","        with open(save_root, 'wb') as f:\n","            pickle.dump(feat, f, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJ26nV_r4WXv"},"outputs":[],"source":["feature_names = [\n","        'visualfeat_enet_b2_8_best_cropped',\n","        'visualfeat_enet_b2_8_best_cropped_aligned',\n","        'audiofeat_vggish',\n","        'audiofeat_wav2vec2'\n","        ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qq3j5Dr_4WXv"},"outputs":[],"source":["class ABAW_dataset(Dataset):\n","    def __init__(self, root, split, typ, task, feature_v, feature_a):\n","        self.root = root\n","        self.split = split\n","        self.typ = typ\n","        self.task = task\n","        self.anno_path = os.path.join(self.root,f'data/Annotations/{self.task}/{self.split}')\n","        self.feature_v, self.feature_a = feature_v, feature_a\n","        self.feature = [self.feature_v, self.feature_a]\n","        with open(os.path.join(root, f'data/Annotations/{self.task}/{self.typ}.txt'), 'r') as f:\n","                self.vidnames = f.read().splitlines()\n","        self.feature_dims = 0\n","        self.data = {}\n","        self.data[self.task] = {}\n","        self.iname = []\n","        for feature_name in self.feature:\n","            if 'visual' in feature_name:\n","                self.data = self.load_feature_v(feature_name)\n","            elif 'audio' in feature_name:\n","                self.data = self.load_feature_a(feature_name)\n","\n","    def get_names(vid, id):\n","            name = \"\"\n","            if id>=0 and id<10:\n","                name = f\"{vid}/0000\" + str(id) + \".jpg\"\n","            elif id>=10 and id<100:\n","                name = f\"{vid}/000\" + str(id) + \".jpg\"\n","            elif id>=100 and id<1000:\n","                name = f\"{vid}/00\" + str(id) + \".jpg\"\n","            elif id>=1000 and id<10000:\n","                name = f\"{vid}/0\" + str(id) + \".jpg\"\n","            else:\n","                name = f\"{vid}/\" + str(id) + \".jpg\"\n","            return name\n","\n","    def load_feature_v(self, feature_v):\n","            print(f'loading visual feature: {feature_v}')\n","            feat_root = os.path.join(root + '/models/ABAW6', feature_v)\n","            filenames = os.listdir(feat_root)[:]\n","            for vname in tqdm(self.vidnames):\n","                    feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","                    with open(os.path.join(self.anno_path, f'{vname}.txt')) as f:\n","                        labels = f.read().splitlines()\n","                    self.data[self.task][vname] = {}\n","\n","                    for imgname, val in feature.items():\n","                        for i,line in enumerate(labels):\n","                            if i > 0:\n","                                imname = get_names(vname, i)\n","                                if imname == imgname:\n","                                    if self.task == 'AU_Detection_Challenge':\n","                                        splitted_line=line.split(',')\n","                                        aus = list(map(int,splitted_line))\n","                                        if min(aus) >= 0:\n","                                            labs = torch.tensor(aus)\n","                                            self.data[self.task][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                                            self.iname.append(imname)\n","                                    elif self.task == 'VA_Estimation_Challenge':\n","                                        splitted_line=line.split(',')\n","                                        valence=float(splitted_line[0])\n","                                        arousal=float(splitted_line[1])\n","                                        if valence >= -1 and arousal >= -1:\n","                                            labs = torch.tensor([valence, arousal])\n","                                            self.data[self.task][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                                            self.iname.append(imname)\n","                                    elif self.task == 'EXPR_Recognition_Challenge':\n","                                        exp = int(line)\n","                                        if exp >= 0:\n","                                            labs = torch.tensor(exp)\n","                                            self.data[self.task][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                                            self.iname.append(imname)\n","                    self.feature_dims += len(self.data[self.task][vname])\n","            return self.data\n","\n","    def load_feature_a(self, feature_a):\n","            print(f'loading audio feature: {feature_a}')\n","            feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","            filenames = os.listdir(feat_root)[:]\n","            for vname in tqdm(self.vidnames):\n","                    feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","                    for imgname, val in feature.items():\n","                        if imgname in self.data[self.task][vname]:\n","                            self.data[self.task][vname][imgname].update({f'{feature_a}': val})\n","\n","                    for img, value in list(self.data[self.task][vname].items()):\n","                        if len(value) < 3:\n","                            self.data[self.task][vname].pop(img)\n","            return self.data\n","\n","    def __getitem__(self, index):\n","            frame = self.iname[index]\n","            vname = frame.split('/')[0]\n","            data = self.data[self.task][vname][frame]\n","            data['frame'] = frame\n","            data['vid'] = vname\n","            data['label'] = self.data[self.task][vname][frame]['label']\n","            return data\n","\n","    def __len__(self):\n","            return self.feature_dims"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yObcGVx9re2M"},"outputs":[],"source":["class ABAW_dataset1(Dataset):\n","    def __init__(self, data, iname, dims, task):\n","        self.data = data\n","        self.iname = iname\n","        self.task = task\n","        self.feature_dims = dims\n","    def __getitem__(self, index):\n","        frame = self.iname[index]\n","        vname = frame.split('/')[0]\n","        data = self.data[self.task][vname][frame]\n","        data['frame'] = frame\n","        data['vid'] = vname\n","        data['label'] = self.data[self.task][vname][frame]['label']\n","        return data\n","\n","    def __len__(self):\n","            return self.feature_dims"]},{"cell_type":"markdown","metadata":{"id":"MrIj0S-0YMHv"},"source":["# Convert"]},{"cell_type":"markdown","metadata":{"id":"1_rLZldeIKPy"},"source":["## Video to Audio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ckW2n3lNYREr"},"outputs":[],"source":["def vid2aud(video_folder, output_folder):\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","    for file_name in tqdm(os.listdir(video_folder)):\n","        video_path = os.path.join(video_folder, file_name)\n","        audio_file_name = os.path.splitext(file_name)[0] + '.wav'\n","        audio_file_path = os.path.join(output_folder, audio_file_name)\n","        if os.path.exists(audio_file_path):\n","            continue\n","        if os.path.isfile(video_path) and file_name.lower().endswith(('.mp4', '.mov', '.avi', '.mkv')):\n","            video_clip = VideoFileClip(video_path)\n","            audio_clip = video_clip.audio\n","            audio_clip.write_audiofile(audio_file_path)\n","            audio_clip.close()\n","            video_clip.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2897,"status":"ok","timestamp":1711873190668,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"PrIqAzrZfvy6","outputId":"9961f33f-c3f2-444c-8ed8-5e3214d7456e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Processing /content/drive/MyDrive/MSc/Thesis/data/video/batch1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 475/475 [00:02<00:00, 192.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Processing /content/drive/MyDrive/MSc/Thesis/data/video/batch2\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 73/73 [00:00<00:00, 2263.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Processing /content/drive/MyDrive/MSc/Thesis/data/video/new_vids\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [00:00<00:00, 540.31it/s]\n"]}],"source":["for i in ['batch1','batch2','new_vids']:\n","    video_folder= os.path.join(root,'data/video', i)\n","    print(f'\\nProcessing {video_folder}')\n","    output_folder= os.path.join(root, 'data/audio')\n","    vid2aud(video_folder, output_folder)"]},{"cell_type":"markdown","metadata":{"id":"EtRjOH5jKB6S"},"source":["## Audio to Text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W7dzFPvZ6Mq-"},"outputs":[],"source":["class Fuxi():\n","    def __init__(self, max_duration=50):\n","        self.appkey = 'phr-fuxi'\n","        self.appsecret = '74c72dee-bf9a-4de2-8c1f-96be1a1ecabd'\n","        self.url = 'http://api-test.vop.netease.com/phone_rec'\n","        self.max_duration = max_duration\n","\n","    def run(self, wav_path, lang='en', type=\"wav\",max_duration=50):\n","        words = []\n","\n","        audio = AudioSegment.from_file(wav_path, format='mp3')\n","        duration_ms = len(audio)\n","        if duration_ms < 500:\n","            return ''\n","        chunk_length_ms = max_duration * 1000  # 60s\n","        text = ''\n","        long_vid = []\n","        for i in range(0, duration_ms,chunk_length_ms):\n","            new_audio = audio[i:min(i+chunk_length_ms,duration_ms)]\n","            byte_io = BytesIO()\n","            new_audio.export(byte_io, format=\"wav\")\n","            speech = byte_io.getvalue()\n","\n","            curtime = str(int(time.time()))\n","            hl = hashlib.md5()\n","            hl.update((self.appkey + curtime).encode(encoding='utf-8'))\n","            sign = hmac.new(self.appsecret.encode('utf-8'),\n","                            hl.hexdigest().encode('utf-8'), hashlib.sha1).digest()\n","            checksum = base64.b64encode(sign)\n","            params = {'appkey': self.appkey, 'lan': lang}\n","            headers = {\n","                'curtime': curtime,\n","                'checksum': checksum,\n","                'content-type': 'audio/wav',\n","                'cuid': 'fuxi-avatarlib'\n","            }\n","            response = requests.post(self.url,\n","                                        params=params,\n","                                        headers=headers,\n","                                        data=speech)\n","            r = response.json()\n","            if r['ret_code'] != 1:\n","                #error = r['ret_msg']\n","                #raise RuntimeError('Hangyan Rec Error: ' + error+f'[{wav_path}]')\n","                long_vid.append(wav_path)\n","                pass\n","            aligned_text = r['result']\n","            text += ' '.join([w['word'] for w in aligned_text if w['word'] != 'sil'])\n","        return text\n","\n","    def split_wave(self, wav_path, save_root, max_duration=50):\n","        wav_name = wav_path.split('/')[-1].replace('.wav', '')\n","        fin = wave.open(wav_path, 'rb')\n","        fs_orig = fin.getframerate()\n","        audio_length = fin.getnframes() * (1/fs_orig)\n","        fin.close()\n","\n","        audio = AudioSegment.from_wav(wav_path)\n","        n = int(audio_length//max_duration + 1)\n","        for i in range(n):\n","            new_audio = audio[i*max_duration*1000:(i+1)*max_duration*1000]\n","            new_audio.export(os.path.join(save_root, wav_name+f'_{i}.wav'), format='wav')\n","        return n\n","\n","    def read_wave(self, wav_path, max_duration=60):\n","        with open(wav_path, 'rb') as f:\n","            wav_data = f.read()\n","        return wav_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LT2vPUqjpfrT"},"outputs":[],"source":["def aud2text():\n","\n","    speech2text = Fuxi()\n","    print('loading model')\n","    root_wav = os.path.join(root, 'data/audio')\n","    wav_files = os.listdir(root_wav)[::-1]\n","\n","    print(f'processing {root_wav}')\n","    save_folder = os.path.join(root, f'models/ABAW6/aud2text')\n","    os.makedirs(save_folder, exist_ok=True)\n","\n","    res_dict = {}\n","    for audio_name in tqdm(wav_files):\n","        audio_file = os.path.join(root_wav, audio_name)\n","        save_path = os.path.join(save_folder, audio_name.split('.')[0]+ '.json')\n","        if os.path.exists(save_path):\n","            with open(save_path, 'r') as f:\n","                text = json.load(f)\n","            continue\n","        else:\n","            text = speech2text.run(audio_file)\n","            with open(save_path, 'w') as f:\n","                json.dump(text, f)\n","        res_dict[audio_name] = text.strip()\n","\n","    with open(os.path.join(save_folder,'alltext.json'), 'w') as f:\n","        json.dump(res_dict, f, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3148589,"status":"ok","timestamp":1710950390441,"user":{"displayName":"Ulysses Vo","userId":"16301520817637769988"},"user_tz":-180},"id":"vTcsP9fAKhey","outputId":"c1ec7628-4d63-4000-f8c2-f4de4a3bbc3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading model\n","processing /content/drive/MyDrive/MSc/Thesis/data/audio\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 173/173 [52:28<00:00, 18.20s/it]\n"]}],"source":["aud2text()"]},{"cell_type":"markdown","metadata":{"id":"pa2wGLmhWRO1"},"source":["# Extract feature"]},{"cell_type":"markdown","metadata":{"id":"oFeoqCZln2-J"},"source":["## Visual feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVngkrvr5uvA"},"outputs":[],"source":["IMG_SIZE=224\n","train_transforms = transforms.Compose(\n","    [\n","        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n","        transforms.RandomRotation(45),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ]\n",")\n","test_transforms = transforms.Compose([\n","        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9sqKG972tHO"},"outputs":[],"source":["with open('/content/drive/MyDrive/MSc/Thesis/data/test_list.txt', 'r') as f:\n","      test_list = f.read().splitlines()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gscHzYjjoNed"},"outputs":[],"source":["def extract_visual_feature(mode_name, typ):\n","    print('loading model:',mode_name)\n","    feature_extractor_model = torch.load(os.path.join(root, f'models/EmotiEffNet/enet/{mode_name}.pt'))\n","    feature_extractor_model.classifier=torch.nn.Identity()\n","    feature_extractor_model=feature_extractor_model.to(device)\n","    feature_extractor_model.eval()\n","\n","    if typ == 'cropped_aligned':\n","        dir = ['cropped_aligned','cropped_aligned_new_50_vids']\n","    elif typ == 'cropped':\n","        dir = ['batch1', 'batch2', 'cropped_new_50_vids']\n","\n","    for d in dir:\n","        root_vis = f'/content/data/{d}'\n","        print(f'processing {root_vis}')\n","        save_folder = os.path.join(root, f'models/ABAW6/visualfeat_{mode_name}_{typ}')\n","        os.makedirs(save_folder, exist_ok=True)\n","        for filename in tqdm(os.listdir(root_vis)):\n","            X_features=[]\n","            img_names=[]\n","            img_feat = {}\n","            imgs=[]\n","            frames_dir=os.path.join(root_vis,filename)\n","\n","            if not os.path.isdir(frames_dir):\n","                continue\n","            save_file = os.path.join(save_folder, filename+'.npy')\n","            if os.path.exists(save_file):\n","                continue\n","            else:\n","                for img_name in os.listdir(frames_dir):\n","                    if img_name.lower().endswith('.jpg'):\n","                        img = Image.open(os.path.join(frames_dir,img_name))\n","                        if filename in test_list:\n","                            img_tensor = test_transforms(img)\n","                        else:\n","                            img_tensor = train_transforms(img)\n","                        if img.size:\n","                            img_names.append(filename+'/'+img_name)\n","                            imgs.append(img_tensor)\n","                            if len(imgs)>= 64:\n","                                features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n","                                features = features.data.cpu().numpy()\n","                                if len(X_features)==0:\n","                                    X_features=features\n","                                else:\n","                                    X_features=np.concatenate((X_features,features),axis=0)\n","                                imgs=[]\n","\n","                if len(imgs)>0:\n","                    features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n","                    features = features.data.cpu().numpy()\n","\n","                    if len(X_features)==0:\n","                        X_features=features\n","                    else:\n","                        X_features=np.concatenate((X_features,features),axis=0)\n","\n","                    imgs=[]\n","                img_feat= {img_name:global_features for img_name,global_features in zip(img_names,X_features)}\n","                np.save(save_file,img_feat)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1423186,"status":"ok","timestamp":1715621977603,"user":{"displayName":"Nguyễn Nguyễn","userId":"15341849881319858303"},"user_tz":-180},"id":"38rGg3H95p7e","outputId":"7e542a3d-776f-428a-9d07-d5ff9e9332bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading model: enet_b2_8_best\n","processing /content/data/batch1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 355/355 [01:01<00:00,  5.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["processing /content/data/batch2\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 209/209 [00:31<00:00,  6.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["processing /content/data/cropped_new_50_vids\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 31/31 [22:04<00:00, 42.72s/it]\n"]}],"source":["# cropped images\n","extract_visual_feature('enet_b2_8_best','cropped')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qdP1WLlO10MM","outputId":"8fa16c6c-3b21-45e2-e420-c8abc8c5ff74"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading model: enet_b2_8_best\n","processing /content/data/cropped_aligned\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 564/564 [2:05:17<00:00, 13.33s/it]\n"]},{"name":"stdout","output_type":"stream","text":["processing /content/data/cropped_aligned_new_50_vids\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 31/31 [14:10<00:00, 27.44s/it]\n"]}],"source":["# cropped_aligned images\n","extract_visual_feature('enet_b2_8_best','cropped_aligned')"]},{"cell_type":"markdown","metadata":{"id":"opY83VrpIBen"},"source":["## Audio feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFWa0pYa4WXu"},"outputs":[],"source":["video2len={}\n","for i in ['batch1', 'batch2', 'new_vids']:\n","    d = os.path.join('/content/video',i)\n","    for filename in os.listdir(d):\n","        fn, ext = os.path.splitext(os.path.basename(filename))\n","        vid=os.path.join(d,filename)\n","        cap = cv2.VideoCapture(vid)\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        video2len[fn]=total_frames+1\n","\n","for filename in os.listdir(os.path.join(root,f'models/ABAW6/visualfeat_enet_b2_8_best_cropped_aligned')):\n","    if 'left' in filename or 'right' in filename:\n","        feature_path = os.path.join(root, filename)\n","        feature = np.load(feature_path, allow_pickle=True).tolist()\n","        fn = filename.split('.')[0]\n","        video2len[fn] = len(feature)\n","\n","with open('/content/drive/MyDrive/MSc/Thesis/data/vid_length.pkl', 'wb') as f:\n","    pickle.dump(video2len, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zz7Y9t3xWVHX"},"outputs":[],"source":["def extract_audio_feature(mode_name):\n","\n","    if mode_name == 'wav2vec2':\n","        bundle = torchaudio.pipelines.WAV2VEC2_BASE\n","        model = bundle.get_model().to(device)\n","    elif mode_name == 'vggish':\n","        model = torch.hub.load('harritaylor/torchvggish', mode_name)\n","        model.eval().to(device)\n","\n","    print('loading model:',mode_name)\n","    root_wav = os.path.join(root, 'data/audio')\n","    wav_files = os.listdir(root_wav)[::-1]\n","    print(f'processing {root_wav}')\n","    save_folder = os.path.join(root, f'models/ABAW6/audiofeat_{mode_name}')\n","    os.makedirs(save_folder, exist_ok=True)\n","\n","    with open('/content/drive/MyDrive/MSc/Thesis/data/vid_length.pkl', 'rb') as f:\n","        video2len = pickle.load(f)\n","\n","    for nwav, frames_count in tqdm(video2len.items()):\n","        if nwav.endswith('_left'):\n","            wav_f = nwav[:-5]\n","        elif nwav.endswith('_right'):\n","            wav_f = nwav[:-6]\n","        else:\n","            wav_f = nwav\n","        audio_features = {}\n","        save_file = os.path.join(save_folder, nwav +'.npy')\n","        if os.path.exists(save_file):\n","            continue\n","        if mode_name == 'vggish':\n","            with torch.no_grad():\n","                reps = model.forward(os.path.join(root_wav, wav_f + '.wav'))\n","                reps = reps.cpu().numpy()/255.\n","        else:\n","            wav, rate = torchaudio.load(os.path.join(root_wav, wav_f + '.wav'))\n","            if rate!= bundle.sample_rate:\n","                wav = torchaudio.functional.resample(wav, rate, bundle.sample_rate)\n","            reps = []\n","            channel, length = wav.shape\n","            max_length = 2500000\n","            with torch.no_grad():\n","                for i in range(length//max_length+1):\n","                    reps.append(model.extract_features(wav.cuda()[:,i*max_length:(i+1)*max_length])[0][-1])\n","            reps = torch.concatenate(reps,dim=1)\n","            if channel !=1:\n","                reps = torch.mean(reps, dim=0).unsqueeze(dim=0)\n","\n","            reps = reps.cpu().numpy().squeeze(0)\n","        audio_scale=len(reps)/frames_count\n","\n","        for frame_number in range(frames_count):\n","            ind=int(frame_number*audio_scale)\n","            nframe = get_names(nwav,frame_number+1)\n","            audio_features[nframe]= reps[ind]\n","\n","        np.save(save_file, audio_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQux-D7j16lm","outputId":"8681203a-7c26-4916-bd89-a487f7ac0d71"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960.pth\n","100%|██████████| 360M/360M [00:01<00:00, 207MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["loading model: wav2vec2\n","processing /content/drive/MyDrive/MSc/Thesis/data/audio\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 627/627 [30:11<00:00,  2.89s/it]\n"]}],"source":["extract_audio_feature('wav2vec2')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mw_hE9fG19bt","outputId":"c1b036a2-16cb-4a3b-d363-77c7ba0024b7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/harritaylor/torchvggish/zipball/master\" to /root/.cache/torch/hub/master.zip\n","Downloading: \"https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish-10086976.pth\" to /root/.cache/torch/hub/checkpoints/vggish-10086976.pth\n","100%|██████████| 275M/275M [00:01<00:00, 213MB/s]\n","Downloading: \"https://github.com/harritaylor/torchvggish/releases/download/v0.1/vggish_pca_params-970ea276.pth\" to /root/.cache/torch/hub/checkpoints/vggish_pca_params-970ea276.pth\n","100%|██████████| 177k/177k [00:00<00:00, 9.32MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["loading model: vggish\n","processing /content/drive/MyDrive/MSc/Thesis/data/audio\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 627/627 [19:59<00:00,  1.91s/it]\n"]}],"source":["extract_audio_feature('vggish')"]},{"cell_type":"markdown","metadata":{"id":"97AGawC8PnRo"},"source":["# Metrics"]},{"cell_type":"markdown","metadata":{"id":"zZvRVuFzAkfc"},"source":["### Compute loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrddZOWU9WGc"},"outputs":[],"source":["def compute_EXP_loss(pred, label, weights):\n","    cri_exp = nn.CrossEntropyLoss(weights)\n","    cls_loss = cri_exp(pred, label)\n","    return cls_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIHYj_DYBHQG"},"outputs":[],"source":["def compute_AU_loss(pred, label, weights):\n","    cri_AU = nn.BCEWithLogitsLoss(weights)\n","    cls_loss = cri_AU(pred, label.float())\n","    return cls_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2BwARxBRcwF"},"outputs":[],"source":["def CCC_loss(x, y):\n","    x, y = x.view(-1), y.view(-1)\n","    vx = x - torch.mean(x)\n","    vy = y - torch.mean(y)\n","    rho =  torch.sum(vx * vy) / (torch.sqrt(torch.sum(torch.pow(vx, 2))) * torch.sqrt(torch.sum(torch.pow(vy, 2)))+1e-8)\n","    x_m, y_m = torch.mean(x), torch.mean(y)\n","    x_s, y_s = torch.std(x), torch.std(y)\n","    ccc = 2*rho*x_s*y_s/(torch.pow(x_s, 2) + torch.pow(y_s, 2) + torch.pow(x_m - y_m, 2))\n","    return 1-ccc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7kdT2piBc_G"},"outputs":[],"source":["def compute_VA_loss(Vout,Aout,label):\n","    ccc_loss = CCC_loss(Vout[:,0],label[:,0]) + CCC_loss(Aout[:,0],label[:,1])\n","    mse_loss = nn.MSELoss()(Vout,label[:,0]) + nn.MSELoss()(Aout,label[:,1])\n","    return mse_loss,ccc_loss"]},{"cell_type":"markdown","metadata":{"id":"gd_uu91kAhJE"},"source":["### Compute F1 score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhrQC_kWxaqa"},"outputs":[],"source":["def compute_EXP_F1(pred, target):\n","    pred_labels = np.argmax(pred, axis=1)\n","    target_labels = np.argmax(target, axis=1)\n","    macro_f1 = f1_score(target_labels,pred_labels,average='macro')\n","    acc = accuracy_score(target_labels, pred_labels)\n","    return macro_f1, acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ke0-RvC2mi69"},"outputs":[],"source":["def f1s_max_AU(label, pred, thresh, i=0):\n","    pred = np.array(pred)\n","    label = np.array(label)\n","    label = label[:,i]\n","    pred = pred[:,i]\n","    acc = []\n","    F1 = []\n","    for i in thresh:\n","        new_pred = ((pred >= i) * 1).flatten()\n","        acc.append(accuracy_score(label.flatten(), new_pred))\n","        F1.append(f1_score(label.flatten(), new_pred))\n","\n","    F1_MAX = max(F1)\n","    if F1_MAX < 0 or math.isnan(F1_MAX):\n","        F1_MAX = 0\n","        F1_THRESH = 0\n","        accuracy = 0\n","    else:\n","        idx_thresh = np.argmax(F1)\n","        F1_THRESH = thresh[idx_thresh]\n","        accuracy = acc[idx_thresh]\n","    return F1, F1_MAX, F1_THRESH, accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdDnfAtpmuKr"},"outputs":[],"source":["def compute_AU_F1(pred,label,thresh=np.arange(0.1,1,0.1)):\n","    F1s = []\n","    F1t = []\n","    acc = []\n","    for i in range(12):\n","        F1, F1_MAX, F1_THRESH, accuracy = f1s_max_AU(label,pred,thresh,i)\n","        F1s.append(F1_MAX)\n","        F1t.append(F1_THRESH)\n","        acc.append(accuracy)\n","    acc = [round(a,3) for a in acc]\n","    return np.mean(F1s),np.mean(F1t),acc, F1t"]},{"cell_type":"markdown","metadata":{"id":"WSYSCjAnLcf3"},"source":["### Concordance Correlation Coefficient"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDWknJ9zKyQA"},"outputs":[],"source":["def CCC_score(x, y):\n","    x = np.array(x)\n","    y = np.array(y)\n","    vx = x - np.mean(x)\n","    vy = y - np.mean(y)\n","    rho = np.sum(vx * vy) / (np.sqrt(np.sum(vx**2)) * np.sqrt(np.sum(vy**2)))\n","    x_m = np.mean(x)\n","    y_m = np.mean(y)\n","    x_s = np.std(x)\n","    y_s = np.std(y)\n","    ccc = 2*rho*x_s*y_s/(x_s**2 + y_s**2 + (x_m - y_m)**2)\n","    return ccc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMqwnXAhKLcO"},"outputs":[],"source":["def compute_VA_CCC(x,y):\n","    x = np.array(x)\n","    y = np.array(y)\n","    x[x>1] = 1\n","    x[x<-1] = -1\n","    ccc1 = CCC_score(x[:,0],y[:,0])\n","    ccc2 = CCC_score(x[:,1],y[:,1])\n","\n","    return ccc1,ccc2"]},{"cell_type":"markdown","metadata":{"id":"iRzfo5z0Lj3f"},"source":["### Pearson’s Correlation Coefficient"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wqizr_7YLGSk"},"outputs":[],"source":["def PCC(x,y):\n","    x = np.array(x)\n","    y = np.array(y)\n","    x[x>1] = 1\n","    x[x<0] = 0\n","    vx = x - np.mean(x)\n","    vy = y - np.mean(y)\n","    pcc = np.sum(vx * vy) / (np.sqrt(np.sum(vx**2)) * np.sqrt(np.sum(vy**2)))\n","    return pcc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9K1LAQKoLH1P"},"outputs":[],"source":["def compute_emo_PCC(x,y):\n","    x = np.array(x)\n","    y = np.array(y)\n","    pccs = []\n","    for i in range(7):\n","        p = PCC(x[:,i],y[:,i])\n","        pccs.append(p)\n","    pccs = np.array(pccs)\n","    mean_pcc = np.mean(pccs)\n","    return pccs, mean_pcc"]},{"cell_type":"markdown","metadata":{"id":"SwMmqGyJwnZg"},"source":["# Challenges"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zhIC4geYv9HJ"},"outputs":[],"source":["task = ['EXPR_Recognition_Challenge','AU_Detection_Challenge','VA_Estimation_Challenge']\n","split = ['Train_Set', 'Validation_Set']\n","typ = ['Train','Val','Test']\n","vis_typ = ['cropped_aligned', 'cropped']\n","visual_feat = 'visualfeat_enet_b2_8_best'\n","audio_feat = ['audiofeat_wav2vec2','audiofeat_vggish','nope']\n","vis_aud = ['visual_wav2vec2','visual_vggish','visual']\n","batch_size = 32\n","model_type = ['fusion', 'mlp']"]},{"cell_type":"markdown","metadata":{"id":"HoofZGhrU6Ih"},"source":["## EXPR Recognition Challenge"]},{"cell_type":"markdown","metadata":{"id":"Ubi-WwKO6a1R"},"source":["### Loading data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBMZbYSi7ya1"},"outputs":[],"source":["# Cropped_aligned images\n","vis = vis_typ[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXTl4J1sWZjW"},"outputs":[],"source":["# Cropped images\n","vis = vis_typ[1]"]},{"cell_type":"markdown","metadata":{"id":"56gxaRmr5pIT"},"source":["#### Effnet + wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IPSj-MXy5pIT"},"outputs":[],"source":["auft = audio_feat[0]\n","viau = vis_aud[0]"]},{"cell_type":"markdown","metadata":{"id":"bx7Mwlg35pIT"},"source":["#### Effnet + vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZpK72FP5pIT"},"outputs":[],"source":["auft = audio_feat[1]\n","viau = vis_aud[1]"]},{"cell_type":"markdown","metadata":{"id":"GM4A0l6x5pIU"},"source":["#### Effnet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5Utm-WR5pIU"},"outputs":[],"source":["auft = audio_feat[2]\n","viau = vis_aud[2]"]},{"cell_type":"markdown","metadata":{"id":"gLXBg1h55pIU"},"source":["#### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5QxjJ9WOFMS"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{task[0]}_{typ[0]}_{viau}.pkl'), 'rb') as f:\n","    data1 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1177,"status":"ok","timestamp":1716016859747,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"GAG_gkOP6Fts","outputId":"d1f0a686-cc49-4c60-dd3b-a21fd4b5090b"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 199/199 [00:00<00:00, 2542.65it/s]\n"]}],"source":["dims = 0\n","iname = []\n","task1 = task[0]\n","with open(os.path.join(root, f'data/Annotations/{task[0]}/{typ[0]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","for vname in tqdm(vidnames):\n","    dims += len(data1[task1][vname])\n","    for img in data1[task1][vname].keys():\n","        iname.append(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEcd8nuV5pIU"},"outputs":[],"source":["dataset = ABAW_dataset1(data1, iname, dims, task1)\n","train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"kAlXrkVW5pIV"},"source":["#### Val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"by-jdZt55pIV"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{task[0]}_{typ[1]}_{viau}.pkl'), 'rb') as f:\n","    data2 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":903,"status":"ok","timestamp":1716016887585,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"Drw-STzDV8yp","outputId":"b22321f0-eb45-443e-c4dd-071318da3c61"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 49/49 [00:00<00:00, 2459.18it/s]\n"]}],"source":["dims = 0\n","iname = []\n","task1 = task[0]\n","with open(os.path.join(root, f'data/Annotations/{task[0]}/{typ[1]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","for vname in tqdm(vidnames):\n","    dims += len(data2[task1][vname])\n","    for img in data2[task1][vname].keys():\n","        iname.append(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KnMKSN3l5pIV"},"outputs":[],"source":["dataset = ABAW_dataset1(data2, iname, dims, task1)\n","val_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"ekKJDTam5pIV"},"source":["#### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0wIAuSo5pIV"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{task[0]}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n","    data3 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":773,"status":"ok","timestamp":1716016967971,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"XPKBKszq5pIV","outputId":"d2d0083e-d4d3-48c0-f321-9bfd490d07c9"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 70/70 [00:00<00:00, 1183.99it/s]\n"]}],"source":["dims = 0\n","iname = []\n","task1 = task[0]\n","with open(os.path.join(root, f'data/Annotations/{task[0]}/{typ[2]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","for vname in tqdm(vidnames):\n","    dims += len(data3[task1][vname])\n","    for img in data3[task1][vname].keys():\n","        iname.append(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBPEYhxI5pIW"},"outputs":[],"source":["dataset = ABAW_dataset1(data3, iname, dims, task1)\n","test_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"UQsS25mKxpIu"},"source":["#### Another way (Effnet + wav2vec2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KaTxGab6qik1"},"outputs":[],"source":["print(f'{task[0]}')\n","print(f'loading {typ[0]}')\n","dataset = ABAW_dataset(root, split[0], typ[0], task[0], feature_v=visual_feat, feature_a=audio_feat)\n","loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n","torch.save(loader, os.path.join(root,f'models/ABAW6/{task[0]}_{typ[0]}.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-8VBt_BqlKc"},"outputs":[],"source":["print(f'{task[0]}')\n","print(f'loading {typ[1]}')\n","dataset = ABAW_dataset(root, split[0], typ[1], task[0], feature_v=visual_feat, feature_a=audio_feat)\n","loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n","torch.save(loader, os.path.join(root,f'models/ABAW6/{task[0]}_{typ[1]}.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtJ6FD4IqmWl"},"outputs":[],"source":["print(f'{task[0]}')\n","print(f'loading {typ[2]}')\n","dataset = ABAW_dataset(root, split[1], typ[2], task[0], feature_v=visual_feat, feature_a=audio_feat)\n","loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2,drop_last=True)\n","torch.save(loader, os.path.join(root,f'models/ABAW6/{task[0]}_{typ[2]}.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WsK6_d4rmPmm"},"outputs":[],"source":["train_loader = torch.load(os.path.join(root,f'models/ABAW6/EXPR/{task[0]}_{typ[0]}.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJ2giDx1bH-_"},"outputs":[],"source":["val_loader = torch.load(os.path.join(root,f'models/ABAW6/EXPR/{task[0]}_{typ[1]}.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W43ekmn8DLLf"},"outputs":[],"source":["test_loader = torch.load(os.path.join(root,f'models/ABAW6/EXPR/{task[0]}_{typ[2]}.pth'))"]},{"cell_type":"markdown","metadata":{"id":"O5qpMbpk6faK"},"source":["### Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKhsZIv4Wod4"},"outputs":[],"source":["class EXP_fusion(nn.Module):\n","    def __init__(self, batchsize = batch_size, audio_ft = auft, hidden_size = [512, 128, batch_size]):\n","        super(EXP_fusion, self).__init__()\n","        self.batchsize = batchsize\n","        if audio_ft == 'audiofeat_wav2vec2':\n","            self.concat_dim = 2176    #1408+768\n","        elif audio_ft == 'audiofeat_vggish':\n","            self.concat_dim = 1536    #1408+128\n","        elif audio_ft == 'nope':\n","            self.concat_dim = 1408    #visual only\n","        self.hidden_size = hidden_size\n","        self.feat_fc = nn.Conv1d(self.concat_dim, hidden_size[0], 1, padding=0)\n","        self.activ = nn.LeakyReLU(0.1)\n","        self.dropout = nn.Dropout(p=0.3)\n","        self.conv1 = nn.Conv1d(hidden_size[0], hidden_size[1], 1, padding=0)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size[1], nhead=4, dim_feedforward=hidden_size[1], dropout=0.3)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n","        self.head = nn.Sequential(\n","                nn.Linear(hidden_size[1], hidden_size[2]),\n","                nn.BatchNorm1d(hidden_size[2]),\n","                nn.Dropout(p=0.3),\n","                nn.Linear(hidden_size[2], 8))\n","\n","    def forward(self, vis_feat, aud_feat):\n","        if aud_feat == None:\n","            feat = vis_feat\n","        else:\n","            inputs = [vis_feat]\n","            inputs.append(aud_feat)\n","            feat = torch.cat(inputs,dim=1)\n","        feat = torch.transpose(feat,0,1)\n","        feat = self.feat_fc(feat)\n","        feat = self.activ(feat)\n","        out = self.conv1(feat)\n","        out = torch.transpose(out,0,1)\n","        out = self.transformer_encoder(out)\n","        out = self.head(out)\n","\n","        return out, torch.softmax(out, dim = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1022,"status":"ok","timestamp":1716016968989,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"p25D4iquOI0I","outputId":"a472ae49-070e-4be7-9687-76eea6d8b178"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["EXP_fusion(\n","  (feat_fc): Conv1d(2176, 512, kernel_size=(1,), stride=(1,))\n","  (activ): LeakyReLU(negative_slope=0.1)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-3): 4 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=128, bias=True)\n","        (dropout): Dropout(p=0.3, inplace=False)\n","        (linear2): Linear(in_features=128, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.3, inplace=False)\n","        (dropout2): Dropout(p=0.3, inplace=False)\n","      )\n","    )\n","  )\n","  (head): Sequential(\n","    (0): Linear(in_features=128, out_features=32, bias=True)\n","    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): Dropout(p=0.3, inplace=False)\n","    (3): Linear(in_features=32, out_features=8, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":33}],"source":["EXP_model = EXP_fusion().to(device)\n","EXP_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvTwWPbHdiqK"},"outputs":[],"source":["class MLPModel(nn.Module):\n","    def __init__(self, audio_ft = auft, num_classes=8):\n","        super(MLPModel, self).__init__()\n","        if audio_ft == 'audiofeat_wav2vec2':\n","            self.concat_dim = 2176    #1408+768\n","        elif audio_ft == 'audiofeat_vggish':\n","            self.concat_dim = 1536    #1408+128\n","        elif audio_ft == 'nope':\n","            self.concat_dim = 1408    #visual only\n","        self.activ = nn.ReLU()\n","        self.fc1 = nn.Linear(self.concat_dim, 128)\n","        self.fc2 = nn.Linear(128, num_classes)\n","\n","    def forward(self, vis_feat, aud_feat):\n","        if aud_feat == None:\n","            feat = vis_feat\n","        else:\n","            inputs = [vis_feat]\n","            inputs.append(aud_feat)\n","            feat = torch.cat(inputs, dim=1)\n","        feat = self.fc1(feat)\n","        feat = self.activ(feat)\n","        out = self.fc2(feat)\n","        return out, torch.softmax(out, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1715757694187,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"NtPaQhzHfHkD","outputId":"6cbe2e94-7bf2-4598-db7a-b99a91ad6ed7"},"outputs":[{"data":{"text/plain":["MLPModel(\n","  (activ): ReLU()\n","  (fc1): Linear(in_features=1536, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",")"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["mlp_model = MLPModel().to(device)\n","mlp_model"]},{"cell_type":"markdown","metadata":{"id":"YYow-aj8W6E8"},"source":["#### Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHATH07vP_HL"},"outputs":[],"source":["y_train = []\n","iterator = iter(train_loader)\n","i = 0\n","while True:\n","    try:\n","        EXPR = next(iterator)\n","        y_train.extend(EXPR['label'].numpy())\n","    except:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":625,"status":"ok","timestamp":1716016977489,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"v_43HFcIXxLf","outputId":"06ffc4c5-0dbf-4bc8-e44a-bff606fe8213"},"outputs":[{"output_type":"stream","name":"stdout","text":["weights = tensor([0.4260, 4.1334, 5.9334, 6.8653, 0.7257, 0.9578, 2.0518, 0.4572],\n","       device='cuda:0')\n"]}],"source":["class_weights=compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n","\n","weights=torch.tensor(class_weights,dtype=torch.float).to(device)\n","print(f'{weights = }')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Z-Yc8Zz4A7_"},"outputs":[],"source":["weights = [0.4260, 4.1334, 5.9334, 6.8653, 0.7257, 0.9578, 2.0518, 0.4572]\n","weights = torch.tensor(weights).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nk0cCxAbhWIe"},"outputs":[],"source":["weights1 = [0.5619842406043042, 0.1331245105716523, 0.4915926179084074, 0.009731543624161074, 0.4858991788569254, 0.3211159481346253, 0.23595084924606013, 0.4854298934682019]\n","weights1 = torch.tensor(weights1).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YiUYQl4UvwV6"},"outputs":[],"source":["def one_hot_transfer(label, class_num):\n","    one_hot = torch.eye(class_num)\n","    one_hot = one_hot.to(device)\n","    return one_hot[label]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjBHb1zf6Eyg"},"outputs":[],"source":["optimizer = optim.AdamW(filter(lambda p: p.requires_grad, EXP_model.parameters()), lr=0.00001, betas=(0.9, 0.999), weight_decay=0.00005)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pgZJkGfnysSq"},"outputs":[],"source":["optimizer = optim.AdamW(filter(lambda p: p.requires_grad, mlp_model.parameters()), lr=0.00001, betas=(0.9, 0.999), weight_decay=0.00005)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)\n","#EXP_class_names = [\"Neutral\",\"Anger\",\"Disgust\",\"Fear\",\"Happiness\",\"Sadness\",\"Surprise\",\"Other\"]"]},{"cell_type":"markdown","metadata":{"id":"2ZIINjb56jUS"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3J-0BjC7LWf9"},"outputs":[],"source":["def train(model, mod_type, train_loader, val_loader, epoch, batch_size, optim, au_feat, weight, vi_au):\n","\n","    model.train(True)\n","    model.eval()\n","    best_loss = float('inf')\n","    f1best, accbest = 0, 0\n","    loss_value = []\n","    loss_train = []\n","    loss_val = []\n","    all_preds = []\n","    all_targets = []\n","\n","    for e in range(epoch):\n","        print(f'Epoch: {e+1}')\n","        iterator = iter(train_loader)\n","        while True:\n","            try:\n","                EXPR = next(iterator)\n","                if au_feat == 'nope':\n","                    vis_feat, y = EXPR[visual_feat], EXPR['label']\n","                    vis_feat, y = vis_feat.to(device), y.to(device)\n","                    aud_feat = None\n","                else:\n","                    vis_feat, aud_feat, y = EXPR[visual_feat], EXPR[au_feat], EXPR['label']\n","                    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n","                y_onehot = one_hot_transfer(y, 8).to(device)\n","                model.zero_grad()\n","                pred, exp_pred = model(vis_feat, aud_feat)\n","                loss = compute_EXP_loss(pred, y_onehot, weight)\n","                loss.backward()\n","                optim.step()\n","                loss_value.append(loss.item())\n","                all_preds.extend(exp_pred.cpu().tolist())\n","                all_targets.extend(y_onehot.cpu().tolist())\n","            except:\n","                break\n","        avg_loss = round(np.mean(loss_value),3)\n","        loss_train.append(avg_loss)\n","        f1_scores, accuracy = compute_EXP_F1(all_preds, all_targets)\n","        print(f'Train Loss: {avg_loss}, Accuracy: {round(accuracy,3)}')\n","\n","        val_loss, f1s, acc = evaluate_model(model, val_loader, au_feat, weight)\n","        loss_val.append(val_loss)\n","\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_{mod_type}_{vi_au}_loss.pth'))\n","            # f1best = f1s\n","            # accbest = acc\n","\n","        if f1s > f1best:\n","            #best_loss = val_loss\n","            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_{mod_type}_{vi_au}_f1s.pth'))\n","            f1best = f1s\n","            #accbest = acc\n","\n","        if acc > accbest:\n","            #best_loss = val_loss\n","            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_{mod_type}_{vi_au}_acc.pth'))\n","            #f1best = f1s\n","            accbest = acc\n","\n","        print(f'Validation Loss: {val_loss}, Accuracy: {acc}')\n","        scheduler.step(val_loss)\n","    return loss_train, loss_val, best_loss, f1best, accbest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53tjkDUDPYEz"},"outputs":[],"source":["def evaluate_model(model, data_loader, au_feat, weight):\n","    model.eval()\n","    total_loss = []\n","    all_preds = []\n","    all_targets = []\n","    with torch.no_grad():\n","        iterator = iter(data_loader)\n","        while True:\n","            try:\n","                EXPR = next(iterator)\n","                if au_feat == 'nope':\n","                    vis_feat, y = EXPR[visual_feat], EXPR['label']\n","                    vis_feat, y = vis_feat.to(device), y.to(device)\n","                    aud_feat = None\n","                else:\n","                    vis_feat, aud_feat, y = EXPR[visual_feat], EXPR[au_feat], EXPR['label']\n","                    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n","                y_onehot = one_hot_transfer(y, 8).to(device)\n","                pred, exp_pred = model(vis_feat, aud_feat)\n","                loss = compute_EXP_loss(pred, y_onehot, weight)\n","                total_loss.append(loss.item())\n","                all_preds.extend(exp_pred.cpu().tolist())\n","                all_targets.extend(y_onehot.cpu().tolist())\n","            except:\n","                break\n","\n","    f1_scores, acc = compute_EXP_F1(all_preds, all_targets)\n","    return round(np.mean(total_loss),3), round(f1_scores,3), round(acc,3)"]},{"cell_type":"markdown","metadata":{"id":"YmUvIMgNVuel"},"source":["#### Cropped_aligned images"]},{"cell_type":"markdown","metadata":{"id":"Z1h3aTTNLEnT"},"source":["##### Effnet + Wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1704176,"status":"ok","timestamp":1715634036816,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"Wv7Tqh0zK_wi","outputId":"4095f809-d3b1-4827-ce25-3ec727577ded"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.68, Accuracy: 0.737\n","Validation Loss: 1.494, Accuracy: 0.468\n","Epoch: 2\n","Train Loss: 0.497, Accuracy: 0.801\n","Validation Loss: 1.709, Accuracy: 0.459\n","Epoch: 3\n","Train Loss: 0.407, Accuracy: 0.833\n","Validation Loss: 1.757, Accuracy: 0.494\n","Epoch: 4\n","Train Loss: 0.351, Accuracy: 0.853\n","Validation Loss: 1.883, Accuracy: 0.48\n","Epoch: 5\n","Train Loss: 0.31, Accuracy: 0.868\n","Validation Loss: 2.133, Accuracy: 0.484\n","Epoch: 6\n","Train Loss: 0.28, Accuracy: 0.879\n","Validation Loss: 2.186, Accuracy: 0.497\n","Epoch: 7\n","Train Loss: 0.256, Accuracy: 0.888\n","Validation Loss: 2.404, Accuracy: 0.489\n","Epoch: 8\n","Train Loss: 0.236, Accuracy: 0.896\n","Validation Loss: 2.459, Accuracy: 0.471\n","Epoch: 9\n","Train Loss: 0.219, Accuracy: 0.902\n","Validation Loss: 2.515, Accuracy: 0.507\n","Epoch: 10\n","Train Loss: 0.205, Accuracy: 0.908\n","Validation Loss: 2.705, Accuracy: 0.502\n","28min 25s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12659,"status":"ok","timestamp":1715634067473,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"HgGorrmVwhmZ","outputId":"76cddda8-d35e-4393-dc7f-6a518e1adb66"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.282, accuracy: 0.468\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","    EXP_model = EXP_fusion().to(device)\n","    EXP_model.load_state_dict(EXP_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":478902,"status":"ok","timestamp":1715635147370,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"kX-9jUd6y2W3","outputId":"4d0e1c29-fd39-4dd5-bb18-8c98e8a66431"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 1.274, Accuracy: 0.539\n","Validation Loss: 1.278, Accuracy: 0.443\n","Epoch: 2\n","Train Loss: 1.071, Accuracy: 0.602\n","Validation Loss: 1.366, Accuracy: 0.447\n","Epoch: 3\n","Train Loss: 0.96, Accuracy: 0.64\n","Validation Loss: 1.414, Accuracy: 0.452\n","Epoch: 4\n","Train Loss: 0.885, Accuracy: 0.666\n","Validation Loss: 1.453, Accuracy: 0.456\n","Epoch: 5\n","Train Loss: 0.829, Accuracy: 0.685\n","Validation Loss: 1.481, Accuracy: 0.456\n","Epoch: 6\n","Train Loss: 0.786, Accuracy: 0.701\n","Validation Loss: 1.524, Accuracy: 0.456\n","Epoch: 7\n","Train Loss: 0.751, Accuracy: 0.713\n","Validation Loss: 1.546, Accuracy: 0.452\n","Epoch: 8\n","Train Loss: 0.721, Accuracy: 0.724\n","Validation Loss: 1.563, Accuracy: 0.456\n","Epoch: 9\n","Train Loss: 0.695, Accuracy: 0.733\n","Validation Loss: 1.586, Accuracy: 0.451\n","Epoch: 10\n","Train Loss: 0.673, Accuracy: 0.741\n","Validation Loss: 1.604, Accuracy: 0.45\n","7min 57s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4324,"status":"ok","timestamp":1715635487708,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"gYnPVp9U1T-o","outputId":"b151398f-38db-4ca4-f604-94deabe6e771"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.29, accuracy: 0.452\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"OlxUSwAVzKmC"},"source":["##### Effnet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1763576,"status":"ok","timestamp":1715673750263,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"7yztp8PKzFhp","outputId":"b6c96861-d14b-4f00-8f0b-90bc4934e3e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.668, Accuracy: 0.74\n","Validation Loss: 1.387, Accuracy: 0.492\n","Epoch: 2\n","Train Loss: 0.471, Accuracy: 0.805\n","Validation Loss: 1.575, Accuracy: 0.481\n","Epoch: 3\n","Train Loss: 0.373, Accuracy: 0.84\n","Validation Loss: 1.922, Accuracy: 0.473\n","Epoch: 4\n","Train Loss: 0.311, Accuracy: 0.864\n","Validation Loss: 2.178, Accuracy: 0.495\n","Epoch: 5\n","Train Loss: 0.268, Accuracy: 0.881\n","Validation Loss: 2.274, Accuracy: 0.467\n","Epoch: 6\n","Train Loss: 0.236, Accuracy: 0.894\n","Validation Loss: 2.43, Accuracy: 0.495\n","Epoch: 7\n","Train Loss: 0.211, Accuracy: 0.905\n","Validation Loss: 2.607, Accuracy: 0.475\n","Epoch: 8\n","Train Loss: 0.191, Accuracy: 0.913\n","Validation Loss: 2.66, Accuracy: 0.488\n","Epoch: 9\n","Train Loss: 0.175, Accuracy: 0.92\n","Validation Loss: 2.806, Accuracy: 0.477\n","Epoch: 10\n","Train Loss: 0.161, Accuracy: 0.926\n","Validation Loss: 2.994, Accuracy: 0.47\n","29min 23s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14514,"status":"ok","timestamp":1715673934320,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"fO2AH_gYHBXy","outputId":"e107318e-10f7-4d65-8b63-4fbe584132d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.286, accuracy: 0.492\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","    EXP_model = EXP_fusion().to(device)\n","    EXP_model.load_state_dict(EXP_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":456072,"status":"ok","timestamp":1715675232955,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"4_iGBHRhKeVM","outputId":"da26cd7f-bea7-4495-e9cd-45d50c6b30ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 1.417, Accuracy: 0.495\n","Validation Loss: 1.21, Accuracy: 0.416\n","Epoch: 2\n","Train Loss: 1.232, Accuracy: 0.548\n","Validation Loss: 1.213, Accuracy: 0.454\n","Epoch: 3\n","Train Loss: 1.121, Accuracy: 0.585\n","Validation Loss: 1.218, Accuracy: 0.489\n","Epoch: 4\n","Train Loss: 1.043, Accuracy: 0.613\n","Validation Loss: 1.23, Accuracy: 0.493\n","Epoch: 5\n","Train Loss: 0.985, Accuracy: 0.633\n","Validation Loss: 1.229, Accuracy: 0.501\n","Epoch: 6\n","Train Loss: 0.938, Accuracy: 0.649\n","Validation Loss: 1.24, Accuracy: 0.499\n","Epoch: 7\n","Train Loss: 0.9, Accuracy: 0.663\n","Validation Loss: 1.237, Accuracy: 0.506\n","Epoch: 8\n","Train Loss: 0.868, Accuracy: 0.674\n","Validation Loss: 1.252, Accuracy: 0.496\n","Epoch: 9\n","Train Loss: 0.84, Accuracy: 0.683\n","Validation Loss: 1.237, Accuracy: 0.496\n","Epoch: 10\n","Train Loss: 0.816, Accuracy: 0.692\n","Validation Loss: 1.26, Accuracy: 0.499\n","7min 35s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model,model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4770,"status":"ok","timestamp":1715675662169,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"n5Us3nak9BqF","outputId":"88bf4eb9-7df3-4439-fd6e-516dc91fc7fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.309, accuracy: 0.501\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"vfkTbqlzzLyw"},"source":["##### Effnet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1639366,"status":"ok","timestamp":1715678365996,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"y6xR8ij6P9uF","outputId":"d4428b0a-661d-44d3-e420-fab8b2e41977"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.7, Accuracy: 0.729\n","Validation Loss: 1.413, Accuracy: 0.46\n","Epoch: 2\n","Train Loss: 0.529, Accuracy: 0.786\n","Validation Loss: 1.596, Accuracy: 0.445\n","Epoch: 3\n","Train Loss: 0.441, Accuracy: 0.816\n","Validation Loss: 1.822, Accuracy: 0.478\n","Epoch: 4\n","Train Loss: 0.385, Accuracy: 0.837\n","Validation Loss: 1.869, Accuracy: 0.472\n","Epoch: 5\n","Train Loss: 0.345, Accuracy: 0.852\n","Validation Loss: 2.031, Accuracy: 0.466\n","Epoch: 6\n","Train Loss: 0.314, Accuracy: 0.863\n","Validation Loss: 2.083, Accuracy: 0.434\n","Epoch: 7\n","Train Loss: 0.289, Accuracy: 0.873\n","Validation Loss: 2.298, Accuracy: 0.472\n","Epoch: 8\n","Train Loss: 0.268, Accuracy: 0.881\n","Validation Loss: 2.361, Accuracy: 0.459\n","Epoch: 9\n","Train Loss: 0.251, Accuracy: 0.888\n","Validation Loss: 2.384, Accuracy: 0.455\n","Epoch: 10\n","Train Loss: 0.236, Accuracy: 0.894\n","Validation Loss: 2.432, Accuracy: 0.464\n","27min 18s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11459,"status":"ok","timestamp":1715678787437,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"oTivl9yf9oqm","outputId":"dd4d0cfa-270e-4f66-8a3b-088ef0f9702c"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.262, accuracy: 0.478\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","    EXP_model = EXP_fusion().to(device)\n","    EXP_model.load_state_dict(EXP_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":390887,"status":"ok","timestamp":1715676300432,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"k3REomkYQCwJ","outputId":"76ba3d35-98a8-4f2e-f53a-bff6510c1f71"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 1.477, Accuracy: 0.467\n","Validation Loss: 1.214, Accuracy: 0.388\n","Epoch: 2\n","Train Loss: 1.325, Accuracy: 0.505\n","Validation Loss: 1.222, Accuracy: 0.402\n","Epoch: 3\n","Train Loss: 1.232, Accuracy: 0.535\n","Validation Loss: 1.235, Accuracy: 0.412\n","Epoch: 4\n","Train Loss: 1.164, Accuracy: 0.558\n","Validation Loss: 1.25, Accuracy: 0.428\n","Epoch: 5\n","Train Loss: 1.11, Accuracy: 0.577\n","Validation Loss: 1.261, Accuracy: 0.43\n","Epoch: 6\n","Train Loss: 1.065, Accuracy: 0.593\n","Validation Loss: 1.281, Accuracy: 0.429\n","Epoch: 7\n","Train Loss: 1.027, Accuracy: 0.606\n","Validation Loss: 1.278, Accuracy: 0.441\n","Epoch: 8\n","Train Loss: 0.995, Accuracy: 0.618\n","Validation Loss: 1.291, Accuracy: 0.436\n","Epoch: 9\n","Train Loss: 0.967, Accuracy: 0.628\n","Validation Loss: 1.301, Accuracy: 0.439\n","Epoch: 10\n","Train Loss: 0.942, Accuracy: 0.637\n","Validation Loss: 1.315, Accuracy: 0.436\n","6min 29s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5201,"status":"ok","timestamp":1715676632810,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"RwDZL_5KR9wL","outputId":"f7ffdd18-c6dc-4812-db9b-4793a4bde22d"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.275, accuracy: 0.441\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"RBlwdqAWbShk"},"source":["#### Cropped images"]},{"cell_type":"markdown","metadata":{"id":"BAaSqtubbShq"},"source":["##### Effnet + Wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1697457,"status":"ok","timestamp":1715681440253,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"oK2A9RyQbShq","outputId":"a57b4b44-96c0-482d-9893-dffac75ccd20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.742, Accuracy: 0.71\n","Validation Loss: 1.544, Accuracy: 0.483\n","Epoch: 2\n","Train Loss: 0.559, Accuracy: 0.775\n","Validation Loss: 1.756, Accuracy: 0.477\n","Epoch: 3\n","Train Loss: 0.468, Accuracy: 0.807\n","Validation Loss: 1.852, Accuracy: 0.514\n","Epoch: 4\n","Train Loss: 0.409, Accuracy: 0.828\n","Validation Loss: 2.096, Accuracy: 0.512\n","Epoch: 5\n","Train Loss: 0.367, Accuracy: 0.843\n","Validation Loss: 2.13, Accuracy: 0.509\n","Epoch: 6\n","Train Loss: 0.334, Accuracy: 0.855\n","Validation Loss: 2.323, Accuracy: 0.489\n","Epoch: 7\n","Train Loss: 0.308, Accuracy: 0.865\n","Validation Loss: 2.354, Accuracy: 0.506\n","Epoch: 8\n","Train Loss: 0.286, Accuracy: 0.873\n","Validation Loss: 2.506, Accuracy: 0.511\n","Epoch: 9\n","Train Loss: 0.268, Accuracy: 0.88\n","Validation Loss: 2.579, Accuracy: 0.518\n","Epoch: 10\n","Train Loss: 0.252, Accuracy: 0.886\n","Validation Loss: 2.697, Accuracy: 0.503\n","28min 16s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12798,"status":"ok","timestamp":1715681711006,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"0AY7fWH3bShq","outputId":"6f4238e7-31d6-41dd-ba97-0c34ef38d8dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.269, accuracy: 0.518\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","    EXP_model = EXP_fusion().to(device)\n","    EXP_model.load_state_dict(EXP_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":445778,"status":"ok","timestamp":1715679492201,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"PIz06FHpbShq","outputId":"26419a57-3d37-437a-db14-54ef7c037040"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 1.372, Accuracy: 0.495\n","Validation Loss: 1.248, Accuracy: 0.425\n","Epoch: 2\n","Train Loss: 1.168, Accuracy: 0.56\n","Validation Loss: 1.331, Accuracy: 0.452\n","Epoch: 3\n","Train Loss: 1.052, Accuracy: 0.6\n","Validation Loss: 1.379, Accuracy: 0.472\n","Epoch: 4\n","Train Loss: 0.973, Accuracy: 0.628\n","Validation Loss: 1.454, Accuracy: 0.467\n","Epoch: 5\n","Train Loss: 0.915, Accuracy: 0.649\n","Validation Loss: 1.492, Accuracy: 0.465\n","Epoch: 6\n","Train Loss: 0.87, Accuracy: 0.665\n","Validation Loss: 1.535, Accuracy: 0.467\n","Epoch: 7\n","Train Loss: 0.833, Accuracy: 0.678\n","Validation Loss: 1.575, Accuracy: 0.464\n","Epoch: 8\n","Train Loss: 0.803, Accuracy: 0.69\n","Validation Loss: 1.607, Accuracy: 0.463\n","Epoch: 9\n","Train Loss: 0.776, Accuracy: 0.699\n","Validation Loss: 1.635, Accuracy: 0.467\n","Epoch: 10\n","Train Loss: 0.754, Accuracy: 0.707\n","Validation Loss: 1.656, Accuracy: 0.472\n","7min 25s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4777,"status":"ok","timestamp":1715679720083,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"zsxN5ONHbShq","outputId":"ddaef3e8-de2b-4dae-a522-57ba11c6a45e"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.308, accuracy: 0.472\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"7XrIUk5abShr"},"source":["##### Effnet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1706727,"status":"ok","timestamp":1715686017005,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"Vd0X2UGxbShr","outputId":"2f7e9501-301e-4f03-a76f-4a19b18a4145"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.779, Accuracy: 0.695\n","Validation Loss: 1.356, Accuracy: 0.511\n","Epoch: 2\n","Train Loss: 0.586, Accuracy: 0.759\n","Validation Loss: 1.504, Accuracy: 0.512\n","Epoch: 3\n","Train Loss: 0.484, Accuracy: 0.794\n","Validation Loss: 1.683, Accuracy: 0.509\n","Epoch: 4\n","Train Loss: 0.416, Accuracy: 0.818\n","Validation Loss: 1.844, Accuracy: 0.507\n","Epoch: 5\n","Train Loss: 0.366, Accuracy: 0.836\n","Validation Loss: 2.024, Accuracy: 0.501\n","Epoch: 6\n","Train Loss: 0.329, Accuracy: 0.85\n","Validation Loss: 2.264, Accuracy: 0.495\n","Epoch: 7\n","Train Loss: 0.298, Accuracy: 0.862\n","Validation Loss: 2.425, Accuracy: 0.493\n","Epoch: 8\n","Train Loss: 0.273, Accuracy: 0.872\n","Validation Loss: 2.425, Accuracy: 0.5\n","Epoch: 9\n","Train Loss: 0.252, Accuracy: 0.881\n","Validation Loss: 2.808, Accuracy: 0.493\n","Epoch: 10\n","Train Loss: 0.235, Accuracy: 0.889\n","Validation Loss: 2.79, Accuracy: 0.497\n","28min 26s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13035,"status":"ok","timestamp":1715686315263,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"7tCmcYL3bShr","outputId":"daa79001-045e-41a8-c238-cce1315a2003"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.295, accuracy: 0.511\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","    EXP_model = EXP_fusion().to(device)\n","    EXP_model.load_state_dict(EXP_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457959,"status":"ok","timestamp":1715686900983,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"DeWlEc2jbShr","outputId":"b48b0ddb-d43c-4dc5-fa43-37a6fe7c4942"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 1.533, Accuracy: 0.458\n","Validation Loss: 1.177, Accuracy: 0.416\n","Epoch: 2\n","Train Loss: 1.36, Accuracy: 0.506\n","Validation Loss: 1.188, Accuracy: 0.46\n","Epoch: 3\n","Train Loss: 1.251, Accuracy: 0.541\n","Validation Loss: 1.207, Accuracy: 0.484\n","Epoch: 4\n","Train Loss: 1.172, Accuracy: 0.568\n","Validation Loss: 1.227, Accuracy: 0.494\n","Epoch: 5\n","Train Loss: 1.112, Accuracy: 0.588\n","Validation Loss: 1.247, Accuracy: 0.499\n","Epoch: 6\n","Train Loss: 1.064, Accuracy: 0.605\n","Validation Loss: 1.265, Accuracy: 0.502\n","Epoch: 7\n","Train Loss: 1.025, Accuracy: 0.618\n","Validation Loss: 1.282, Accuracy: 0.504\n","Epoch: 8\n","Train Loss: 0.992, Accuracy: 0.63\n","Validation Loss: 1.298, Accuracy: 0.505\n","Epoch: 9\n","Train Loss: 0.964, Accuracy: 0.639\n","Validation Loss: 1.312, Accuracy: 0.506\n","Epoch: 10\n","Train Loss: 0.939, Accuracy: 0.648\n","Validation Loss: 1.326, Accuracy: 0.507\n","7min 37s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model,model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5503,"status":"ok","timestamp":1715687473500,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"3Jdj4ROSbShr","outputId":"b6ccfa94-c870-44ea-d713-590e9a34934f"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.308, accuracy: 0.494\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"9Ki-roz0bShr"},"source":["##### Effnet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1638897,"status":"ok","timestamp":1715689776082,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"JSZM8RPjbShr","outputId":"1704f652-0649-4ff7-baaf-86473a54dbba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.925, Accuracy: 0.644\n","Validation Loss: 1.389, Accuracy: 0.506\n","Epoch: 2\n","Train Loss: 0.745, Accuracy: 0.704\n","Validation Loss: 1.538, Accuracy: 0.492\n","Epoch: 3\n","Train Loss: 0.65, Accuracy: 0.735\n","Validation Loss: 1.683, Accuracy: 0.485\n","Epoch: 4\n","Train Loss: 0.586, Accuracy: 0.757\n","Validation Loss: 1.792, Accuracy: 0.488\n","Epoch: 5\n","Train Loss: 0.539, Accuracy: 0.772\n","Validation Loss: 1.888, Accuracy: 0.486\n","Epoch: 6\n","Train Loss: 0.501, Accuracy: 0.785\n","Validation Loss: 1.986, Accuracy: 0.487\n","Epoch: 7\n","Train Loss: 0.47, Accuracy: 0.796\n","Validation Loss: 2.051, Accuracy: 0.487\n","Epoch: 8\n","Train Loss: 0.443, Accuracy: 0.805\n","Validation Loss: 2.161, Accuracy: 0.487\n","Epoch: 9\n","Train Loss: 0.42, Accuracy: 0.813\n","Validation Loss: 2.263, Accuracy: 0.477\n","Epoch: 10\n","Train Loss: 0.4, Accuracy: 0.82\n","Validation Loss: 2.335, Accuracy: 0.495\n","27min 18s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12036,"status":"ok","timestamp":1715690024446,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"X7yJt29ubShr","outputId":"b3a93753-5010-442a-adc5-4a725e443138"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.281, accuracy: 0.506\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","    EXP_model = EXP_fusion().to(device)\n","    EXP_model.load_state_dict(EXP_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415596,"status":"ok","timestamp":1715690535070,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"uYMTOB5_bShr","outputId":"d995d54f-7c82-4ed8-9e28-be9306de94bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 1.596, Accuracy: 0.408\n","Validation Loss: 1.162, Accuracy: 0.406\n","Epoch: 2\n","Train Loss: 1.466, Accuracy: 0.443\n","Validation Loss: 1.163, Accuracy: 0.441\n","Epoch: 3\n","Train Loss: 1.383, Accuracy: 0.468\n","Validation Loss: 1.18, Accuracy: 0.461\n","Epoch: 4\n","Train Loss: 1.322, Accuracy: 0.489\n","Validation Loss: 1.205, Accuracy: 0.473\n","Epoch: 5\n","Train Loss: 1.272, Accuracy: 0.508\n","Validation Loss: 1.232, Accuracy: 0.478\n","Epoch: 6\n","Train Loss: 1.231, Accuracy: 0.523\n","Validation Loss: 1.257, Accuracy: 0.479\n","Epoch: 7\n","Train Loss: 1.196, Accuracy: 0.536\n","Validation Loss: 1.28, Accuracy: 0.48\n","Epoch: 8\n","Train Loss: 1.166, Accuracy: 0.548\n","Validation Loss: 1.299, Accuracy: 0.48\n","Epoch: 9\n","Train Loss: 1.139, Accuracy: 0.558\n","Validation Loss: 1.316, Accuracy: 0.48\n","Epoch: 10\n","Train Loss: 1.116, Accuracy: 0.567\n","Validation Loss: 1.331, Accuracy: 0.479\n","6min 53s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4617,"status":"ok","timestamp":1715690726425,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"CTdI0AYzbShs","outputId":"22191d28-d2b3-4fcb-f50e-56dda490e935"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.298, accuracy: 0.461\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model, strict=False)\n","    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"Kjx794E5bdTe"},"source":["### Testing"]},{"cell_type":"markdown","metadata":{"id":"nsDxr0QwV30A"},"source":["#### Cropped_aligned images"]},{"cell_type":"markdown","metadata":{"id":"dUFrGk1S__N4"},"source":["##### EffNet + Wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34919,"status":"ok","timestamp":1715635576426,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"PHHY7_sZ2Mwd","outputId":"ef6724e9-e65f-4d00-c715-3056723babd3"},"outputs":[{"name":"stdout","output_type":"stream","text":["EXP_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: f1_score 0.33, accuracy: 0.457\n","34.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('EXP_model')\n","print(visual_feat + ' & ' + auft)\n","EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","EXP_model = EXP_fusion().to(device)\n","EXP_model.load_state_dict(EXP_best_model)\n","test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12836,"status":"ok","timestamp":1715635906241,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"66dMfXYm2Dy_","outputId":"cc418830-04bf-4872-ce3a-0a84dab34f54"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: f1_score 0.394, accuracy: 0.488\n","12 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model, strict=False)\n","val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"be0A7dxPAGJx"},"source":["##### EffNet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33032,"status":"ok","timestamp":1715674133779,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"XbSQ4IBSI_d6","outputId":"0b669b23-713f-462b-a5b3-b96c6b441f31"},"outputs":[{"name":"stdout","output_type":"stream","text":["EXP_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: f1_score 0.316, accuracy: 0.475\n","33 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('EXP_model')\n","print(visual_feat + ' & ' + auft)\n","EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","EXP_model = EXP_fusion().to(device)\n","EXP_model.load_state_dict(EXP_best_model)\n","test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12049,"status":"ok","timestamp":1715675489467,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"8OM1_LawOeZg","outputId":"8c7f2f78-7282-4b65-81dc-17e99598c5ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: f1_score 0.379, accuracy: 0.498\n","11.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model, strict=False)\n","val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"ibOhaO0vAOvH"},"source":["##### EffNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30098,"status":"ok","timestamp":1715678525417,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"8kcSYFJIZ42k","outputId":"5ca3235e-4d5c-4829-abd6-fc89874bcec9"},"outputs":[{"name":"stdout","output_type":"stream","text":["EXP_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: f1_score 0.31, accuracy: 0.447\n","29.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('EXP_model')\n","print(visual_feat + ' & ' + auft)\n","EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","EXP_model = EXP_fusion().to(device)\n","EXP_model.load_state_dict(EXP_best_model)\n","test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10090,"status":"ok","timestamp":1715676472413,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"GwduIUw-_EUU","outputId":"c09762d9-1623-48eb-cd9c-cdf29739416e"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: f1_score 0.327, accuracy: 0.431\n","8.93 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model, strict=False)\n","val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"Yaela_8WeAHK"},"source":["#### Cropped images"]},{"cell_type":"markdown","metadata":{"id":"_Hul_TIheAHL"},"source":["##### EffNet + Wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32429,"status":"ok","timestamp":1715681504982,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"zXLL2_h2ku6k","outputId":"f019b7df-f694-401c-8acb-1a7b96077968"},"outputs":[{"name":"stdout","output_type":"stream","text":["EXP_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: f1_score 0.334, accuracy: 0.472\n","32.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('EXP_model')\n","print(visual_feat + ' & ' + auft)\n","EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","EXP_model = EXP_fusion().to(device)\n","EXP_model.load_state_dict(EXP_best_model)\n","test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11496,"status":"ok","timestamp":1715679643829,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"3zlxjF-DeVr9","outputId":"5852b851-8c84-4c4d-8c48-016e65a01be2"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: f1_score 0.364, accuracy: 0.458\n","10.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model, strict=False)\n","val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"etAjaEyieAHM"},"source":["##### EffNet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33935,"status":"ok","timestamp":1715686085957,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"tMe7Tt4leAHM","outputId":"a33904f8-4e8d-4566-f6bd-4b4a037294cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["EXP_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: f1_score 0.292, accuracy: 0.413\n","32.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('EXP_model')\n","print(visual_feat + ' & ' + auft)\n","EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","EXP_model = EXP_fusion().to(device)\n","EXP_model.load_state_dict(EXP_best_model)\n","test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11889,"status":"ok","timestamp":1715686951032,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"-SCv5VYmeAHN","outputId":"bc698472-067e-44de-9b3b-c172ccd6861f"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: f1_score 0.295, accuracy: 0.374\n","11.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}_loss.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model, strict=False)\n","val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11262,"status":"ok","timestamp":1715686973759,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"LVGLZWu46T3G","outputId":"971249cf-8179-49aa-b3cc-fdfe7eea6eea"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: f1_score 0.342, accuracy: 0.445\n","11.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}_f1s.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model, strict=False)\n","val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"JUZXHSEGeAHN"},"source":["##### EffNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29981,"status":"ok","timestamp":1715689882269,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"1dmpRyxwFRku","outputId":"bd35149f-e48e-4fc9-cc15-1c320b3ffec3"},"outputs":[{"name":"stdout","output_type":"stream","text":["EXP_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: f1_score 0.28, accuracy: 0.401\n","29.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('EXP_model')\n","print(visual_feat + ' & ' + auft)\n","EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n","EXP_model = EXP_fusion().to(device)\n","EXP_model.load_state_dict(EXP_best_model)\n","test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8942,"status":"ok","timestamp":1715690562248,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"HorNLX1CHgev","outputId":"46d4a967-9878-4224-9ee7-8bf61e7fe501"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: f1_score 0.291, accuracy: 0.374\n","9.01 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model, strict=False)\n","val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"]},{"cell_type":"markdown","metadata":{"id":"7zISVpqDy0Io"},"source":["## AU Detection Challenge"]},{"cell_type":"markdown","metadata":{"id":"4t9KlVut-8gN"},"source":["### Loading data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lIzVtIHXjAa"},"outputs":[],"source":["# Cropped_aligned images\n","vis = vis_typ[0]\n","visft = visual_feat[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-s4SOVV4XjAh"},"outputs":[],"source":["# Cropped images\n","vis = vis_typ[1]\n","visft = visual_feat[1]"]},{"cell_type":"markdown","metadata":{"id":"E-e2tiWc-8gO"},"source":["#### Effnet + wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Z8HNxRg-8gO"},"outputs":[],"source":["auft = audio_feat[0]\n","viau = vis_aud[0]"]},{"cell_type":"markdown","metadata":{"id":"rEbORJYi-8gO"},"source":["#### Effnet + vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmDO2-je-8gO"},"outputs":[],"source":["auft = audio_feat[1]\n","viau = vis_aud[1]"]},{"cell_type":"markdown","metadata":{"id":"FWvFn9TS-8gP"},"source":["#### Effnet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZmMqmOj-8gP"},"outputs":[],"source":["auft = audio_feat[2]\n","viau = vis_aud[2]"]},{"cell_type":"markdown","metadata":{"id":"9cSL3k0G-8gP"},"source":["#### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTSmol8-9Zwy"},"outputs":[],"source":["if viau == vis_aud[0]: #Visual+wav2vec2\n","    with open(os.path.join(root,f'models/ABAW6/{vis}/AU/{task[1]}_{typ[0]}_visual.pkl'), 'rb') as f:\n","        data1 = pickle.load(f)\n","    with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[0]}.txt'), 'r') as f:\n","        vidnames = f.read().splitlines()\n","    task1 = task[1]\n","    feature_a = 'audiofeat_wav2vec2'\n","    feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","    filenames = os.listdir(feat_root)[:]\n","    for vname in tqdm(vidnames):\n","            feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","            for imgname, val in feature.items():\n","                if imgname in data1[task1][vname]:\n","                    data1[task1][vname][imgname].update({f'{feature_a}': val})\n","            for img, value in list(data1[task1][vname].items()):\n","                if len(value) < 3:\n","                    data1[task1][vname].pop(img)\n","else:\n","    with open(os.path.join(root,f'models/ABAW6/{vis}/AU/{task[1]}_{typ[0]}_{viau}.pkl'), 'rb') as f:\n","        data1 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2413,"status":"ok","timestamp":1716216230114,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"Mt-McDNz-8gP","outputId":"9df0005d-d0e3-40c4-854c-24d79bb21140"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 236/236 [00:00<00:00, 664.91it/s] \n"]}],"source":["dims = 0\n","iname = []\n","task1 = task[1]\n","with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[0]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","for vname in tqdm(vidnames):\n","    dims += len(data1[task1][vname])\n","    for img in data1[task1][vname].keys():\n","        iname.append(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgDumTlK-8gP"},"outputs":[],"source":["dataset = ABAW_dataset1(data1, iname, dims, task1)\n","train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"KDN9v__h-8gP"},"source":["#### Val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RAqHUP3e-8gP"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/{vis}/AU/{task[1]}_{typ[1]}_{viau}.pkl'), 'rb') as f:\n","    data2 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1716216282682,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"cuLYjlyK-8gP","outputId":"84a0ebcb-f89c-4b10-cc20-565628f679d4"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 59/59 [00:00<00:00, 1230.69it/s]\n"]}],"source":["dims = 0\n","iname = []\n","task1 = task[1]\n","with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[1]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","for vname in tqdm(vidnames):\n","    dims += len(data2[task1][vname])\n","    for img in data2[task1][vname].keys():\n","        iname.append(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYAfwbLp-8gP"},"outputs":[],"source":["dataset = ABAW_dataset1(data2, iname, dims, task1)\n","val_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"OsoLTqkz-8gP"},"source":["#### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuLsvz6J-8gQ"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/{vis}/AU/{task[1]}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n","    data3 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1246,"status":"ok","timestamp":1716216381314,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"sGQmZCVl-8gQ","outputId":"0beb681d-c604-4684-a259-1c18a1e22ec6"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 105/105 [00:00<00:00, 1298.66it/s]\n"]}],"source":["dims = 0\n","iname = []\n","task1 = task[1]\n","with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[2]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","for vname in tqdm(vidnames):\n","    dims += len(data3[task1][vname])\n","    for img in data3[task1][vname].keys():\n","        iname.append(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_x8-0eME-8gQ"},"outputs":[],"source":["dataset = ABAW_dataset1(data3, iname, dims, task1)\n","test_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"LKO9rSzdO9t5"},"source":["### Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOHiQBFK4DEG"},"outputs":[],"source":["class AU_fusion(nn.Module):\n","    def __init__(self, batchsize = batch_size, audio_ft = auft, hidden_size = [512, 128, batch_size]):\n","        super(AU_fusion, self).__init__()\n","        self.batchsize = batchsize\n","        if audio_ft == 'audiofeat_wav2vec2':\n","            self.concat_dim = 2176    #1408+768\n","        elif audio_ft == 'audiofeat_vggish':\n","            self.concat_dim = 1536    #1408+128\n","        elif audio_ft == 'nope':\n","            self.concat_dim = 1408    #visual only\n","        self.hidden_size = hidden_size\n","        self.feat_fc = nn.Conv1d(self.concat_dim, hidden_size[0], 1, padding=0)\n","        self.activ = nn.LeakyReLU(0.1)\n","        self.dropout = nn.Dropout(p=0.3)\n","        self.conv1 = nn.Conv1d(hidden_size[0], hidden_size[1], 1, padding=0)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size[1], nhead=4, dim_feedforward=hidden_size[1], dropout=0.3)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n","        self.head = nn.Sequential(\n","                nn.Linear(hidden_size[1], hidden_size[2]),\n","                nn.BatchNorm1d(hidden_size[2]),\n","                nn.Linear(hidden_size[2], 12))\n","\n","    def forward(self, vis_feat, aud_feat):\n","        if aud_feat == None:\n","            feat = vis_feat\n","        else:\n","            inputs = [vis_feat]\n","            inputs.append(aud_feat)\n","            feat = torch.cat(inputs,dim=1)\n","        feat = torch.transpose(feat,0,1)\n","        feat = self.feat_fc(feat)\n","        feat = self.activ(feat)\n","        out = self.conv1(feat)\n","        out = torch.transpose(out,0,1)\n","        out = self.transformer_encoder(out)\n","        out = self.head(out)\n","\n","        return out, torch.sigmoid(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1199,"status":"ok","timestamp":1716216383094,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"q7xqRYlKLJn0","outputId":"77dbd0a1-b552-494f-9cfa-78617739db79"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["AU_fusion(\n","  (feat_fc): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n","  (activ): LeakyReLU(negative_slope=0.1)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-3): 4 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=128, bias=True)\n","        (dropout): Dropout(p=0.3, inplace=False)\n","        (linear2): Linear(in_features=128, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.3, inplace=False)\n","        (dropout2): Dropout(p=0.3, inplace=False)\n","      )\n","    )\n","  )\n","  (head): Sequential(\n","    (0): Linear(in_features=128, out_features=32, bias=True)\n","    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): Linear(in_features=32, out_features=12, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":33}],"source":["AU_model = AU_fusion().to(device)\n","AU_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Djfaj8fAZVZw"},"outputs":[],"source":["class MLPModel(nn.Module):\n","    def __init__(self, audio_ft = auft, num_classes=12):\n","        super(MLPModel, self).__init__()\n","        if audio_ft == 'audiofeat_wav2vec2':\n","            self.concat_dim = 2176    #1408+768\n","        elif audio_ft == 'audiofeat_vggish':\n","            self.concat_dim = 1536    #1408+128\n","        elif audio_ft == 'nope':\n","            self.concat_dim = 1408    #visual only\n","        self.activ = nn.ReLU()\n","        self.fc1 = nn.Linear(self.concat_dim, 128)\n","        self.fc2 = nn.Linear(128, num_classes)\n","\n","    def forward(self, vis_feat, aud_feat):\n","        if aud_feat == None:\n","            feat = vis_feat\n","        else:\n","            inputs = [vis_feat]\n","            inputs.append(aud_feat)\n","            feat = torch.cat(inputs, dim=1)\n","        feat = self.fc1(feat)\n","        feat = self.activ(feat)\n","        out = self.fc2(feat)\n","        return out, torch.sigmoid(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1716216383094,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"qqfnlKCdKmBf","outputId":"3c0b9f9c-32cd-4bef-db07-d69faf8d06de"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPModel(\n","  (activ): ReLU()\n","  (fc1): Linear(in_features=1536, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=12, bias=True)\n",")"]},"metadata":{},"execution_count":35}],"source":["mlp_model = MLPModel().to(device)\n","mlp_model"]},{"cell_type":"markdown","metadata":{"id":"t8j8cnwsoHuX"},"source":["#### Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lq_YlOzCA-ST"},"outputs":[],"source":["weights = torch.tensor([0.54733899, 0.44180561, 0.56990565, 0.61997328, 0.73956417,0.74692377, 0.72684634, 0.33222808, 0.17383676, 0.20608964, 0.83688068, 0.33890931]).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRmF5dylrBsF"},"outputs":[],"source":["optimizer = optim.AdamW(filter(lambda p: p.requires_grad, AU_model.parameters()), lr=0.00001, betas=(0.9, 0.999), weight_decay=0.00005)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zELaPXWaKuzJ"},"outputs":[],"source":["optimizer = optim.AdamW(filter(lambda p: p.requires_grad, mlp_model.parameters()), lr=0.00001, betas=(0.9, 0.999), weight_decay=0.00005)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)"]},{"cell_type":"markdown","metadata":{"id":"zUmbBhcbPBLy"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6QJIajpA6LE"},"outputs":[],"source":["def train(model, mod_type, train_loader, val_loader, epoch, batch_size, optim, au_feat, weight, vi_au):\n","    model.train(True)\n","    model.eval()\n","    best_loss = float('inf')\n","    f1s_best, accbest = 0, 0\n","    loss_value = []\n","    loss_train = []\n","    loss_val = []\n","    all_preds = []\n","    all_targets = []\n","\n","    for e in range(epoch):\n","        print(f'Epoch: {e+1}')\n","        torch.manual_seed(2809)\n","        iterator = iter(train_loader)\n","        for i in range(len(train_loader)//32):\n","            try:\n","                AU = next(iterator)\n","                if au_feat == 'nope':\n","                    vis_feat, y = AU[visual_feat], AU['label']\n","                    vis_feat, y = vis_feat.to(device), y.to(device)\n","                    aud_feat = None\n","                else:\n","                    vis_feat, aud_feat, y = AU[visual_feat], AU[au_feat], AU['label']\n","                    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n","                model.zero_grad()\n","                pred, au_pred = model(vis_feat, aud_feat)\n","                loss = compute_AU_loss(pred, y, weight)\n","                loss.backward()\n","                optim.step()\n","                loss_value.append(loss.item())\n","                all_preds.extend(au_pred.cpu().tolist())\n","                all_targets.extend(y.cpu().tolist())\n","            except:\n","                break\n","        avg_loss = round(np.mean(loss_value),3)\n","        loss_train.append(avg_loss)\n","        f1_scores, f1_thresh, accuracy, threshold = compute_AU_F1(all_preds, all_targets)\n","        print(f'Train Loss: {avg_loss}, Accuracy of 12 AU classes: {accuracy}')\n","\n","        val_loss, f1s, f1t, acc, f1_threshold = evaluate_model(model, val_loader, au_feat, weight)\n","        loss_val.append(val_loss)\n","\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_AU_{mod_type}_{vi_au}_loss.pth'))\n","            # f1best = f1s\n","            # accbest = acc\n","\n","        if f1s > f1s_best:\n","            #best_loss = val_loss\n","            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_AU_{mod_type}_{vi_au}_f1s.pth'))\n","            f1s_best = f1s\n","            f1t_best = f1t\n","            #accbest = acc\n","\n","        if np.mean(acc) > accbest:\n","            #best_loss = val_loss\n","            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_AU_{mod_type}_{vi_au}_acc.pth'))\n","            #f1best = f1s\n","            accbest = np.mean(acc)\n","\n","        print(f'Validation Loss: {val_loss}, Accuracy of 12 AU classes: {acc}')\n","        scheduler.step(val_loss)\n","    return loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qUHE6yoY-bTy"},"outputs":[],"source":["def evaluate_model(model, data_loader, au_feat, weight):\n","    model.eval()\n","    total_loss = []\n","    all_preds = []\n","    all_targets = []\n","    with torch.no_grad():\n","        iterator = iter(data_loader)\n","        for i in range(len(data_loader)//32):\n","          try:\n","            AU = next(iterator)\n","            if au_feat == 'nope':\n","                vis_feat, y = AU[visual_feat], AU['label']\n","                vis_feat, y = vis_feat.to(device), y.to(device)\n","                aud_feat = None\n","            else:\n","                vis_feat, aud_feat, y = AU[visual_feat], AU[au_feat], AU['label']\n","                vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n","            pred, au_pred = model(vis_feat, aud_feat)\n","            loss = compute_AU_loss(pred, y, weight)\n","            total_loss.append(loss.item())\n","            all_preds.extend(au_pred.cpu().tolist())\n","            all_targets.extend(y.cpu().tolist())\n","          except:\n","            break\n","\n","    f1_scores, f1_thresh, acc, threshold = compute_AU_F1(all_preds, all_targets)\n","    return round(np.mean(total_loss),3), round(f1_scores,3), round(f1_thresh,3), acc, threshold"]},{"cell_type":"markdown","metadata":{"id":"6gRlrEmVaH9R"},"source":["#### Cropped_aligned images"]},{"cell_type":"markdown","metadata":{"id":"Qc0qUsCawNt4"},"source":["##### EffNet + Wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":301078,"status":"ok","timestamp":1715696142515,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"_jy6O05vraF7","outputId":"46080892-3ad4-403c-afb6-7c55fe9454f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.23, Accuracy of 12 AU classes: [0.851, 0.365, 0.73, 0.687, 0.597, 0.597, 0.78, 0.609, 0.034, 0.04, 0.734, 0.088]\n","Validation Loss: 0.193, Accuracy of 12 AU classes: [0.909, 0.734, 0.798, 0.882, 0.791, 0.866, 0.89, 0.925, 0.179, 0.082, 0.781, 0.742]\n","Epoch: 2\n","Train Loss: 0.208, Accuracy of 12 AU classes: [0.866, 0.904, 0.807, 0.749, 0.741, 0.77, 0.819, 0.78, 0.236, 0.253, 0.761, 0.742]\n","Validation Loss: 0.171, Accuracy of 12 AU classes: [0.916, 0.924, 0.87, 0.891, 0.809, 0.878, 0.895, 0.984, 0.651, 0.547, 0.762, 0.878]\n","Epoch: 3\n","Train Loss: 0.195, Accuracy of 12 AU classes: [0.874, 0.916, 0.837, 0.823, 0.752, 0.782, 0.835, 0.473, 0.386, 0.421, 0.775, 0.797]\n","Validation Loss: 0.162, Accuracy of 12 AU classes: [0.918, 0.932, 0.872, 0.894, 0.845, 0.882, 0.896, 0.852, 0.677, 0.745, 0.754, 0.9]\n","Epoch: 4\n","Train Loss: 0.185, Accuracy of 12 AU classes: [0.879, 0.922, 0.854, 0.833, 0.761, 0.791, 0.868, 0.575, 0.479, 0.522, 0.785, 0.824]\n","Validation Loss: 0.159, Accuracy of 12 AU classes: [0.92, 0.934, 0.869, 0.897, 0.844, 0.883, 0.897, 0.884, 0.706, 0.817, 0.768, 0.909]\n","Epoch: 5\n","Train Loss: 0.178, Accuracy of 12 AU classes: [0.883, 0.926, 0.866, 0.841, 0.768, 0.798, 0.873, 0.638, 0.545, 0.591, 0.794, 0.84]\n","Validation Loss: 0.159, Accuracy of 12 AU classes: [0.92, 0.936, 0.86, 0.898, 0.857, 0.884, 0.897, 0.9, 0.729, 0.862, 0.765, 0.911]\n","Epoch: 6\n","Train Loss: 0.173, Accuracy of 12 AU classes: [0.886, 0.928, 0.901, 0.848, 0.773, 0.803, 0.877, 0.682, 0.595, 0.642, 0.801, 0.851]\n","Validation Loss: 0.159, Accuracy of 12 AU classes: [0.919, 0.936, 0.856, 0.899, 0.847, 0.883, 0.886, 0.908, 0.75, 0.9, 0.768, 0.91]\n","Epoch: 7\n","Train Loss: 0.168, Accuracy of 12 AU classes: [0.888, 0.93, 0.905, 0.854, 0.779, 0.808, 0.881, 0.713, 0.636, 0.68, 0.795, 0.859]\n","Validation Loss: 0.16, Accuracy of 12 AU classes: [0.919, 0.936, 0.85, 0.897, 0.852, 0.884, 0.889, 0.915, 0.756, 0.917, 0.764, 0.91]\n","Epoch: 8\n","Train Loss: 0.163, Accuracy of 12 AU classes: [0.89, 0.951, 0.908, 0.859, 0.783, 0.812, 0.884, 0.737, 0.668, 0.709, 0.801, 0.865]\n","Validation Loss: 0.162, Accuracy of 12 AU classes: [0.917, 0.936, 0.844, 0.896, 0.843, 0.884, 0.889, 0.922, 0.756, 0.924, 0.76, 0.91]\n","Epoch: 9\n","Train Loss: 0.16, Accuracy of 12 AU classes: [0.892, 0.951, 0.911, 0.863, 0.788, 0.816, 0.887, 0.756, 0.694, 0.733, 0.807, 0.87]\n","Validation Loss: 0.163, Accuracy of 12 AU classes: [0.914, 0.935, 0.839, 0.897, 0.835, 0.884, 0.89, 0.929, 0.754, 0.93, 0.755, 0.907]\n","Epoch: 10\n","Train Loss: 0.156, Accuracy of 12 AU classes: [0.894, 0.952, 0.914, 0.868, 0.792, 0.82, 0.889, 0.771, 0.715, 0.752, 0.812, 0.874]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.92, 0.932, 0.834, 0.896, 0.824, 0.882, 0.89, 0.934, 0.749, 0.929, 0.749, 0.902]\n","5min 1s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2412,"status":"ok","timestamp":1715696360333,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"5VOG7YkJr_j5","outputId":"d23fbd8f-7016-4be8-aa40-eb8150881e5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.538, f1_threshold: 0.258, accuracy: [0.92, 0.934, 0.869, 0.897, 0.844, 0.883, 0.897, 0.884, 0.706, 0.817, 0.768, 0.909]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n","    AU_model = AU_fusion().to(device)\n","    AU_model.load_state_dict(AU_best_model)\n","    val_loss, f1s, f1t, acc = evaluate_model(AU_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"code","source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 50, 32, optimizer, auft, weights, viau)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gpJ71fHSG3bh","executionInfo":{"status":"ok","timestamp":1716127093254,"user_tz":-180,"elapsed":3492466,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"60d4acf6-95d9-42d5-f6e4-16b4e82cc3e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1\n","Train Loss: 0.28, Accuracy of 12 AU classes: [0.128, 0.085, 0.403, 0.516, 0.541, 0.589, 0.612, 0.03, 0.161, 0.313, 0.629, 0.078]\n","Validation Loss: 0.234, Accuracy of 12 AU classes: [0.168, 0.396, 0.15, 0.864, 0.707, 0.858, 0.872, 0.025, 0.174, 0.373, 0.786, 0.088]\n","Epoch: 2\n","Train Loss: 0.25, Accuracy of 12 AU classes: [0.199, 0.447, 0.63, 0.652, 0.635, 0.525, 0.734, 0.37, 0.284, 0.412, 0.658, 0.208]\n","Validation Loss: 0.205, Accuracy of 12 AU classes: [0.897, 0.902, 0.417, 0.866, 0.784, 0.868, 0.867, 0.988, 0.825, 0.937, 0.727, 0.766]\n","Epoch: 3\n","Train Loss: 0.233, Accuracy of 12 AU classes: [0.711, 0.608, 0.701, 0.699, 0.67, 0.59, 0.774, 0.57, 0.511, 0.598, 0.689, 0.392]\n","Validation Loss: 0.189, Accuracy of 12 AU classes: [0.91, 0.912, 0.648, 0.87, 0.809, 0.87, 0.872, 0.989, 0.826, 0.937, 0.759, 0.849]\n","Epoch: 4\n","Train Loss: 0.222, Accuracy of 12 AU classes: [0.749, 0.69, 0.735, 0.723, 0.69, 0.625, 0.793, 0.67, 0.626, 0.692, 0.707, 0.494]\n","Validation Loss: 0.18, Accuracy of 12 AU classes: [0.909, 0.92, 0.774, 0.875, 0.82, 0.872, 0.874, 0.989, 0.826, 0.937, 0.766, 0.828]\n","Epoch: 5\n","Train Loss: 0.214, Accuracy of 12 AU classes: [0.77, 0.737, 0.756, 0.738, 0.702, 0.648, 0.805, 0.73, 0.694, 0.748, 0.719, 0.555]\n","Validation Loss: 0.174, Accuracy of 12 AU classes: [0.904, 0.916, 0.828, 0.879, 0.825, 0.875, 0.889, 0.988, 0.826, 0.937, 0.754, 0.811]\n","Epoch: 6\n","Train Loss: 0.209, Accuracy of 12 AU classes: [0.784, 0.767, 0.77, 0.748, 0.711, 0.664, 0.813, 0.77, 0.74, 0.785, 0.727, 0.595]\n","Validation Loss: 0.17, Accuracy of 12 AU classes: [0.899, 0.9, 0.849, 0.881, 0.83, 0.879, 0.89, 0.986, 0.826, 0.937, 0.76, 0.804]\n","Epoch: 7\n","Train Loss: 0.204, Accuracy of 12 AU classes: [0.793, 0.788, 0.781, 0.756, 0.718, 0.677, 0.819, 0.798, 0.773, 0.812, 0.733, 0.623]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.896, 0.881, 0.861, 0.883, 0.832, 0.881, 0.89, 0.981, 0.826, 0.937, 0.785, 0.912]\n","Epoch: 8\n","Train Loss: 0.2, Accuracy of 12 AU classes: [0.801, 0.802, 0.789, 0.762, 0.723, 0.686, 0.824, 0.818, 0.797, 0.832, 0.738, 0.645]\n","Validation Loss: 0.164, Accuracy of 12 AU classes: [0.919, 0.865, 0.871, 0.885, 0.832, 0.867, 0.89, 0.974, 0.826, 0.937, 0.788, 0.911]\n","Epoch: 9\n","Train Loss: 0.197, Accuracy of 12 AU classes: [0.806, 0.813, 0.796, 0.767, 0.728, 0.694, 0.828, 0.833, 0.817, 0.847, 0.742, 0.662]\n","Validation Loss: 0.162, Accuracy of 12 AU classes: [0.919, 0.926, 0.876, 0.886, 0.832, 0.871, 0.891, 0.966, 0.826, 0.937, 0.791, 0.909]\n","Epoch: 10\n","Train Loss: 0.195, Accuracy of 12 AU classes: [0.811, 0.821, 0.801, 0.771, 0.731, 0.701, 0.831, 0.845, 0.832, 0.86, 0.746, 0.676]\n","Validation Loss: 0.161, Accuracy of 12 AU classes: [0.919, 0.928, 0.878, 0.887, 0.832, 0.874, 0.892, 0.956, 0.826, 0.937, 0.792, 0.906]\n","Epoch: 11\n","Train Loss: 0.192, Accuracy of 12 AU classes: [0.815, 0.828, 0.806, 0.775, 0.735, 0.763, 0.833, 0.854, 0.844, 0.87, 0.749, 0.688]\n","Validation Loss: 0.16, Accuracy of 12 AU classes: [0.917, 0.924, 0.879, 0.888, 0.831, 0.875, 0.893, 0.95, 0.826, 0.937, 0.793, 0.904]\n","Epoch: 12\n","Train Loss: 0.191, Accuracy of 12 AU classes: [0.818, 0.834, 0.81, 0.778, 0.738, 0.766, 0.835, 0.861, 0.855, 0.878, 0.752, 0.698]\n","Validation Loss: 0.159, Accuracy of 12 AU classes: [0.916, 0.922, 0.88, 0.889, 0.83, 0.877, 0.893, 0.945, 0.826, 0.938, 0.794, 0.901]\n","Epoch: 13\n","Train Loss: 0.189, Accuracy of 12 AU classes: [0.821, 0.838, 0.813, 0.781, 0.74, 0.768, 0.837, 0.868, 0.864, 0.885, 0.739, 0.706]\n","Validation Loss: 0.158, Accuracy of 12 AU classes: [0.916, 0.918, 0.88, 0.89, 0.828, 0.878, 0.894, 0.942, 0.826, 0.938, 0.779, 0.9]\n","Epoch: 14\n","Train Loss: 0.187, Accuracy of 12 AU classes: [0.823, 0.843, 0.816, 0.783, 0.742, 0.77, 0.839, 0.873, 0.871, 0.891, 0.742, 0.714]\n","Validation Loss: 0.157, Accuracy of 12 AU classes: [0.916, 0.915, 0.88, 0.89, 0.828, 0.878, 0.895, 0.94, 0.826, 0.939, 0.781, 0.899]\n","Epoch: 15\n","Train Loss: 0.186, Accuracy of 12 AU classes: [0.826, 0.846, 0.819, 0.785, 0.744, 0.772, 0.84, 0.877, 0.878, 0.896, 0.744, 0.72]\n","Validation Loss: 0.156, Accuracy of 12 AU classes: [0.916, 0.912, 0.881, 0.89, 0.827, 0.878, 0.896, 0.938, 0.826, 0.941, 0.782, 0.898]\n","Epoch: 16\n","Train Loss: 0.184, Accuracy of 12 AU classes: [0.828, 0.849, 0.821, 0.788, 0.746, 0.774, 0.842, 0.881, 0.883, 0.901, 0.746, 0.726]\n","Validation Loss: 0.156, Accuracy of 12 AU classes: [0.915, 0.931, 0.881, 0.891, 0.826, 0.879, 0.896, 0.938, 0.826, 0.942, 0.783, 0.898]\n","Epoch: 17\n","Train Loss: 0.183, Accuracy of 12 AU classes: [0.83, 0.852, 0.823, 0.79, 0.748, 0.775, 0.843, 0.884, 0.888, 0.905, 0.749, 0.731]\n","Validation Loss: 0.155, Accuracy of 12 AU classes: [0.914, 0.93, 0.882, 0.892, 0.825, 0.88, 0.897, 0.937, 0.826, 0.944, 0.784, 0.897]\n","Epoch: 18\n","Train Loss: 0.182, Accuracy of 12 AU classes: [0.832, 0.855, 0.825, 0.791, 0.749, 0.777, 0.844, 0.887, 0.893, 0.908, 0.751, 0.736]\n","Validation Loss: 0.155, Accuracy of 12 AU classes: [0.925, 0.93, 0.883, 0.893, 0.845, 0.88, 0.898, 0.936, 0.827, 0.946, 0.784, 0.897]\n","Epoch: 19\n","Train Loss: 0.181, Accuracy of 12 AU classes: [0.833, 0.857, 0.827, 0.793, 0.751, 0.778, 0.845, 0.889, 0.897, 0.911, 0.752, 0.74]\n","Validation Loss: 0.154, Accuracy of 12 AU classes: [0.925, 0.93, 0.884, 0.894, 0.846, 0.88, 0.899, 0.935, 0.827, 0.948, 0.785, 0.897]\n","Epoch: 20\n","Train Loss: 0.18, Accuracy of 12 AU classes: [0.835, 0.921, 0.829, 0.795, 0.752, 0.779, 0.846, 0.891, 0.901, 0.914, 0.754, 0.744]\n","Validation Loss: 0.154, Accuracy of 12 AU classes: [0.925, 0.931, 0.883, 0.893, 0.846, 0.88, 0.9, 0.935, 0.827, 0.95, 0.787, 0.897]\n","Epoch: 21\n","Train Loss: 0.179, Accuracy of 12 AU classes: [0.836, 0.922, 0.87, 0.796, 0.754, 0.78, 0.847, 0.893, 0.904, 0.916, 0.756, 0.875]\n","Validation Loss: 0.154, Accuracy of 12 AU classes: [0.925, 0.931, 0.883, 0.893, 0.847, 0.88, 0.901, 0.934, 0.827, 0.952, 0.787, 0.897]\n","Epoch: 22\n","Train Loss: 0.178, Accuracy of 12 AU classes: [0.837, 0.923, 0.871, 0.798, 0.755, 0.781, 0.848, 0.895, 0.907, 0.918, 0.757, 0.876]\n","Validation Loss: 0.153, Accuracy of 12 AU classes: [0.926, 0.931, 0.882, 0.893, 0.847, 0.88, 0.902, 0.933, 0.826, 0.953, 0.787, 0.897]\n","Epoch: 23\n","Train Loss: 0.178, Accuracy of 12 AU classes: [0.839, 0.923, 0.872, 0.799, 0.756, 0.782, 0.848, 0.897, 0.91, 0.92, 0.758, 0.877]\n","Validation Loss: 0.153, Accuracy of 12 AU classes: [0.926, 0.93, 0.883, 0.893, 0.847, 0.88, 0.902, 0.933, 0.827, 0.954, 0.788, 0.898]\n","Epoch: 24\n","Train Loss: 0.177, Accuracy of 12 AU classes: [0.84, 0.924, 0.872, 0.8, 0.757, 0.783, 0.849, 0.898, 0.912, 0.922, 0.76, 0.878]\n","Validation Loss: 0.153, Accuracy of 12 AU classes: [0.926, 0.93, 0.882, 0.893, 0.847, 0.881, 0.901, 0.933, 0.827, 0.956, 0.789, 0.897]\n","Epoch: 25\n","Train Loss: 0.176, Accuracy of 12 AU classes: [0.841, 0.925, 0.873, 0.801, 0.758, 0.784, 0.85, 0.9, 0.914, 0.923, 0.761, 0.879]\n","Validation Loss: 0.153, Accuracy of 12 AU classes: [0.926, 0.93, 0.883, 0.893, 0.846, 0.881, 0.902, 0.934, 0.827, 0.958, 0.789, 0.898]\n","Epoch: 26\n","Train Loss: 0.176, Accuracy of 12 AU classes: [0.842, 0.925, 0.874, 0.802, 0.759, 0.785, 0.85, 0.901, 0.916, 0.925, 0.762, 0.88]\n","Validation Loss: 0.152, Accuracy of 12 AU classes: [0.926, 0.93, 0.883, 0.893, 0.846, 0.881, 0.901, 0.934, 0.827, 0.957, 0.789, 0.898]\n","Epoch: 27\n","Train Loss: 0.175, Accuracy of 12 AU classes: [0.843, 0.926, 0.875, 0.804, 0.76, 0.786, 0.851, 0.902, 0.918, 0.926, 0.763, 0.881]\n","Validation Loss: 0.152, Accuracy of 12 AU classes: [0.926, 0.929, 0.883, 0.893, 0.845, 0.881, 0.902, 0.934, 0.827, 0.958, 0.789, 0.898]\n","Epoch: 28\n","Train Loss: 0.174, Accuracy of 12 AU classes: [0.844, 0.926, 0.876, 0.805, 0.761, 0.786, 0.851, 0.903, 0.92, 0.927, 0.764, 0.882]\n","Validation Loss: 0.152, Accuracy of 12 AU classes: [0.926, 0.929, 0.883, 0.893, 0.845, 0.881, 0.902, 0.934, 0.827, 0.959, 0.79, 0.898]\n","Epoch: 29\n","Train Loss: 0.174, Accuracy of 12 AU classes: [0.88, 0.927, 0.876, 0.806, 0.762, 0.787, 0.852, 0.904, 0.922, 0.928, 0.765, 0.882]\n","Validation Loss: 0.152, Accuracy of 12 AU classes: [0.926, 0.929, 0.883, 0.894, 0.845, 0.868, 0.903, 0.934, 0.827, 0.959, 0.79, 0.899]\n","Epoch: 30\n","Train Loss: 0.173, Accuracy of 12 AU classes: [0.881, 0.927, 0.877, 0.807, 0.762, 0.788, 0.852, 0.905, 0.923, 0.929, 0.766, 0.883]\n","Validation Loss: 0.152, Accuracy of 12 AU classes: [0.926, 0.928, 0.883, 0.894, 0.844, 0.869, 0.903, 0.934, 0.827, 0.96, 0.79, 0.899]\n","Epoch: 31\n","Train Loss: 0.173, Accuracy of 12 AU classes: [0.882, 0.928, 0.878, 0.808, 0.763, 0.788, 0.853, 0.906, 0.925, 0.93, 0.767, 0.884]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.926, 0.928, 0.883, 0.895, 0.844, 0.869, 0.902, 0.934, 0.826, 0.96, 0.79, 0.9]\n","Epoch: 32\n","Train Loss: 0.172, Accuracy of 12 AU classes: [0.882, 0.928, 0.878, 0.808, 0.764, 0.789, 0.853, 0.907, 0.926, 0.931, 0.768, 0.884]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.926, 0.928, 0.883, 0.895, 0.843, 0.869, 0.892, 0.935, 0.826, 0.961, 0.79, 0.9]\n","Epoch: 33\n","Train Loss: 0.172, Accuracy of 12 AU classes: [0.883, 0.929, 0.879, 0.809, 0.765, 0.79, 0.854, 0.908, 0.928, 0.932, 0.769, 0.885]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.926, 0.929, 0.883, 0.895, 0.842, 0.869, 0.893, 0.935, 0.826, 0.961, 0.79, 0.901]\n","Epoch: 34\n","Train Loss: 0.171, Accuracy of 12 AU classes: [0.883, 0.929, 0.879, 0.81, 0.765, 0.79, 0.854, 0.909, 0.929, 0.932, 0.77, 0.885]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.925, 0.928, 0.882, 0.896, 0.841, 0.869, 0.894, 0.935, 0.826, 0.961, 0.79, 0.901]\n","Epoch: 35\n","Train Loss: 0.171, Accuracy of 12 AU classes: [0.884, 0.929, 0.88, 0.811, 0.766, 0.791, 0.854, 0.91, 0.93, 0.933, 0.771, 0.886]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.926, 0.929, 0.881, 0.896, 0.84, 0.87, 0.894, 0.935, 0.826, 0.961, 0.789, 0.902]\n","Epoch: 36\n","Train Loss: 0.17, Accuracy of 12 AU classes: [0.884, 0.93, 0.881, 0.812, 0.767, 0.791, 0.855, 0.91, 0.931, 0.934, 0.772, 0.886]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.925, 0.928, 0.881, 0.896, 0.839, 0.87, 0.894, 0.935, 0.826, 0.961, 0.789, 0.902]\n","Epoch: 37\n","Train Loss: 0.17, Accuracy of 12 AU classes: [0.885, 0.93, 0.881, 0.812, 0.767, 0.792, 0.855, 0.911, 0.932, 0.934, 0.772, 0.887]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.925, 0.939, 0.881, 0.896, 0.838, 0.871, 0.895, 0.935, 0.826, 0.961, 0.788, 0.902]\n","Epoch: 38\n","Train Loss: 0.169, Accuracy of 12 AU classes: [0.885, 0.93, 0.882, 0.813, 0.768, 0.792, 0.856, 0.911, 0.933, 0.935, 0.773, 0.887]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.925, 0.939, 0.909, 0.897, 0.837, 0.871, 0.895, 0.935, 0.826, 0.962, 0.789, 0.902]\n","Epoch: 39\n","Train Loss: 0.169, Accuracy of 12 AU classes: [0.885, 0.93, 0.882, 0.814, 0.768, 0.793, 0.856, 0.912, 0.934, 0.935, 0.774, 0.887]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.926, 0.939, 0.908, 0.897, 0.836, 0.872, 0.895, 0.936, 0.826, 0.962, 0.789, 0.902]\n","Epoch: 40\n","Train Loss: 0.168, Accuracy of 12 AU classes: [0.886, 0.931, 0.883, 0.815, 0.769, 0.793, 0.856, 0.913, 0.935, 0.936, 0.774, 0.888]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.926, 0.939, 0.907, 0.897, 0.834, 0.871, 0.895, 0.936, 0.826, 0.963, 0.789, 0.902]\n","Epoch: 41\n","Train Loss: 0.168, Accuracy of 12 AU classes: [0.886, 0.931, 0.883, 0.815, 0.769, 0.794, 0.857, 0.913, 0.936, 0.936, 0.775, 0.888]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.926, 0.939, 0.907, 0.897, 0.833, 0.871, 0.897, 0.937, 0.826, 0.963, 0.789, 0.902]\n","Epoch: 42\n","Train Loss: 0.168, Accuracy of 12 AU classes: [0.887, 0.931, 0.883, 0.816, 0.77, 0.794, 0.857, 0.914, 0.937, 0.937, 0.776, 0.889]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.925, 0.94, 0.907, 0.897, 0.832, 0.871, 0.897, 0.938, 0.826, 0.962, 0.789, 0.902]\n","Epoch: 43\n","Train Loss: 0.167, Accuracy of 12 AU classes: [0.887, 0.931, 0.884, 0.817, 0.77, 0.795, 0.857, 0.914, 0.937, 0.937, 0.776, 0.889]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.926, 0.94, 0.906, 0.897, 0.831, 0.871, 0.898, 0.938, 0.826, 0.962, 0.789, 0.902]\n","Epoch: 44\n","Train Loss: 0.167, Accuracy of 12 AU classes: [0.887, 0.932, 0.884, 0.817, 0.771, 0.795, 0.858, 0.915, 0.938, 0.937, 0.777, 0.889]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.926, 0.94, 0.906, 0.897, 0.83, 0.872, 0.898, 0.939, 0.826, 0.962, 0.79, 0.902]\n","Epoch: 45\n","Train Loss: 0.167, Accuracy of 12 AU classes: [0.888, 0.932, 0.885, 0.818, 0.771, 0.796, 0.858, 0.915, 0.939, 0.938, 0.778, 0.889]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.926, 0.94, 0.905, 0.898, 0.829, 0.872, 0.898, 0.94, 0.826, 0.962, 0.79, 0.902]\n","Epoch: 46\n","Train Loss: 0.166, Accuracy of 12 AU classes: [0.888, 0.932, 0.885, 0.818, 0.772, 0.796, 0.858, 0.916, 0.94, 0.938, 0.778, 0.89]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.925, 0.94, 0.904, 0.897, 0.829, 0.872, 0.898, 0.94, 0.826, 0.962, 0.791, 0.902]\n","Epoch: 47\n","Train Loss: 0.166, Accuracy of 12 AU classes: [0.888, 0.932, 0.885, 0.819, 0.772, 0.796, 0.858, 0.916, 0.94, 0.938, 0.779, 0.89]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.925, 0.94, 0.904, 0.897, 0.828, 0.872, 0.898, 0.941, 0.826, 0.961, 0.79, 0.902]\n","Epoch: 48\n","Train Loss: 0.166, Accuracy of 12 AU classes: [0.889, 0.932, 0.886, 0.82, 0.773, 0.797, 0.859, 0.917, 0.941, 0.939, 0.779, 0.89]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.925, 0.94, 0.904, 0.898, 0.827, 0.872, 0.898, 0.942, 0.826, 0.96, 0.79, 0.903]\n","Epoch: 49\n","Train Loss: 0.165, Accuracy of 12 AU classes: [0.889, 0.933, 0.886, 0.82, 0.773, 0.797, 0.859, 0.917, 0.941, 0.939, 0.78, 0.891]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.925, 0.94, 0.904, 0.898, 0.826, 0.871, 0.898, 0.942, 0.826, 0.958, 0.791, 0.903]\n","Epoch: 50\n","Train Loss: 0.165, Accuracy of 12 AU classes: [0.889, 0.933, 0.887, 0.821, 0.774, 0.798, 0.859, 0.917, 0.942, 0.939, 0.78, 0.891]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.925, 0.94, 0.904, 0.898, 0.825, 0.872, 0.899, 0.943, 0.826, 0.958, 0.79, 0.903]\n","58min 12s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166709,"status":"ok","timestamp":1715712902096,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"53LSD8h9Og5J","outputId":"86620097-d51e-4dc6-bdce-045673f47797"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.289, Accuracy of 12 AU classes: [0.209, 0.052, 0.35, 0.386, 0.6, 0.41, 0.54, 0.175, 0.136, 0.082, 0.663, 0.078]\n","Validation Loss: 0.234, Accuracy of 12 AU classes: [0.773, 0.091, 0.138, 0.85, 0.742, 0.852, 0.874, 0.909, 0.174, 0.063, 0.74, 0.094]\n","Epoch: 2\n","Train Loss: 0.254, Accuracy of 12 AU classes: [0.148, 0.312, 0.597, 0.583, 0.662, 0.54, 0.695, 0.307, 0.302, 0.233, 0.695, 0.311]\n","Validation Loss: 0.203, Accuracy of 12 AU classes: [0.897, 0.9, 0.401, 0.869, 0.77, 0.867, 0.889, 0.959, 0.818, 0.927, 0.734, 0.863]\n","Epoch: 3\n","Train Loss: 0.235, Accuracy of 12 AU classes: [0.64, 0.514, 0.677, 0.652, 0.687, 0.599, 0.747, 0.521, 0.521, 0.474, 0.715, 0.484]\n","Validation Loss: 0.188, Accuracy of 12 AU classes: [0.915, 0.912, 0.681, 0.871, 0.803, 0.869, 0.889, 0.981, 0.826, 0.937, 0.758, 0.891]\n","Epoch: 4\n","Train Loss: 0.224, Accuracy of 12 AU classes: [0.699, 0.619, 0.717, 0.688, 0.701, 0.631, 0.824, 0.631, 0.633, 0.599, 0.727, 0.572]\n","Validation Loss: 0.179, Accuracy of 12 AU classes: [0.908, 0.913, 0.801, 0.875, 0.815, 0.871, 0.89, 0.981, 0.826, 0.937, 0.742, 0.874]\n","Epoch: 5\n","Train Loss: 0.216, Accuracy of 12 AU classes: [0.731, 0.68, 0.741, 0.71, 0.711, 0.653, 0.833, 0.697, 0.7, 0.673, 0.735, 0.621]\n","Validation Loss: 0.173, Accuracy of 12 AU classes: [0.9, 0.901, 0.846, 0.878, 0.824, 0.875, 0.892, 0.977, 0.826, 0.937, 0.757, 0.85]\n","Epoch: 6\n","Train Loss: 0.21, Accuracy of 12 AU classes: [0.752, 0.718, 0.758, 0.725, 0.718, 0.668, 0.839, 0.74, 0.745, 0.723, 0.74, 0.652]\n","Validation Loss: 0.169, Accuracy of 12 AU classes: [0.894, 0.886, 0.862, 0.88, 0.827, 0.878, 0.892, 0.971, 0.826, 0.937, 0.763, 0.839]\n","Epoch: 7\n","Train Loss: 0.205, Accuracy of 12 AU classes: [0.766, 0.744, 0.77, 0.736, 0.724, 0.679, 0.844, 0.77, 0.777, 0.759, 0.745, 0.673]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.92, 0.872, 0.873, 0.882, 0.829, 0.88, 0.893, 0.959, 0.826, 0.937, 0.768, 0.83]\n","Epoch: 8\n","Train Loss: 0.201, Accuracy of 12 AU classes: [0.777, 0.764, 0.78, 0.745, 0.729, 0.688, 0.847, 0.792, 0.801, 0.786, 0.749, 0.689]\n","Validation Loss: 0.164, Accuracy of 12 AU classes: [0.919, 0.926, 0.877, 0.882, 0.831, 0.882, 0.893, 0.948, 0.826, 0.937, 0.789, 0.824]\n","Epoch: 9\n","Train Loss: 0.198, Accuracy of 12 AU classes: [0.785, 0.778, 0.787, 0.752, 0.733, 0.696, 0.85, 0.809, 0.82, 0.806, 0.752, 0.702]\n","Validation Loss: 0.162, Accuracy of 12 AU classes: [0.918, 0.927, 0.88, 0.884, 0.832, 0.883, 0.894, 0.943, 0.826, 0.937, 0.792, 0.911]\n","Epoch: 10\n","Train Loss: 0.195, Accuracy of 12 AU classes: [0.792, 0.79, 0.794, 0.757, 0.736, 0.702, 0.852, 0.822, 0.835, 0.823, 0.755, 0.712]\n","Validation Loss: 0.16, Accuracy of 12 AU classes: [0.917, 0.925, 0.881, 0.885, 0.834, 0.871, 0.895, 0.939, 0.826, 0.937, 0.793, 0.908]\n","2min 47s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":604,"status":"ok","timestamp":1715713350797,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"GepgHdzWehYr","outputId":"eb6dd7f7-c1cc-48de-9873-f7cdc49d0637"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.518, f1_threshold: 0.258, accuracy: [0.917, 0.925, 0.881, 0.885, 0.834, 0.871, 0.895, 0.939, 0.826, 0.937, 0.793, 0.908]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n","    mlp_model = MLPModel(num_classes = 12).to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, f1s, f1t, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"markdown","metadata":{"id":"n7c4vN-bwT2d"},"source":["##### EffNet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":295331,"status":"ok","timestamp":1715698371118,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"_lSjgizIwT2d","outputId":"84ca4e8c-a0f1-474b-e202-2c4398f68db9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.227, Accuracy of 12 AU classes: [0.801, 0.603, 0.795, 0.737, 0.702, 0.707, 0.846, 0.077, 0.328, 0.031, 0.725, 0.306]\n","Validation Loss: 0.188, Accuracy of 12 AU classes: [0.911, 0.806, 0.82, 0.899, 0.838, 0.889, 0.899, 0.119, 0.175, 0.063, 0.797, 0.752]\n","Epoch: 2\n","Train Loss: 0.206, Accuracy of 12 AU classes: [0.843, 0.742, 0.838, 0.777, 0.737, 0.788, 0.858, 0.349, 0.188, 0.175, 0.753, 0.557]\n","Validation Loss: 0.168, Accuracy of 12 AU classes: [0.901, 0.878, 0.873, 0.89, 0.849, 0.889, 0.895, 0.785, 0.357, 0.305, 0.795, 0.829]\n","Epoch: 3\n","Train Loss: 0.193, Accuracy of 12 AU classes: [0.86, 0.804, 0.855, 0.797, 0.753, 0.796, 0.864, 0.52, 0.359, 0.361, 0.768, 0.659]\n","Validation Loss: 0.158, Accuracy of 12 AU classes: [0.915, 0.93, 0.873, 0.895, 0.865, 0.885, 0.905, 0.871, 0.642, 0.696, 0.818, 0.866]\n","Epoch: 4\n","Train Loss: 0.185, Accuracy of 12 AU classes: [0.869, 0.838, 0.865, 0.81, 0.762, 0.802, 0.869, 0.614, 0.478, 0.487, 0.779, 0.716]\n","Validation Loss: 0.154, Accuracy of 12 AU classes: [0.919, 0.933, 0.9, 0.899, 0.867, 0.887, 0.91, 0.877, 0.726, 0.818, 0.821, 0.89]\n","Epoch: 5\n","Train Loss: 0.178, Accuracy of 12 AU classes: [0.875, 0.858, 0.873, 0.82, 0.77, 0.806, 0.873, 0.672, 0.559, 0.571, 0.787, 0.752]\n","Validation Loss: 0.152, Accuracy of 12 AU classes: [0.922, 0.932, 0.895, 0.901, 0.863, 0.887, 0.909, 0.882, 0.762, 0.854, 0.821, 0.899]\n","Epoch: 6\n","Train Loss: 0.173, Accuracy of 12 AU classes: [0.879, 0.872, 0.878, 0.828, 0.775, 0.81, 0.876, 0.711, 0.618, 0.63, 0.793, 0.777]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.923, 0.93, 0.891, 0.902, 0.859, 0.887, 0.91, 0.88, 0.78, 0.868, 0.821, 0.9]\n","Epoch: 7\n","Train Loss: 0.169, Accuracy of 12 AU classes: [0.883, 0.932, 0.883, 0.834, 0.78, 0.813, 0.878, 0.739, 0.662, 0.674, 0.798, 0.794]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.923, 0.927, 0.885, 0.903, 0.874, 0.886, 0.91, 0.885, 0.795, 0.866, 0.82, 0.9]\n","Epoch: 8\n","Train Loss: 0.165, Accuracy of 12 AU classes: [0.885, 0.934, 0.887, 0.84, 0.785, 0.816, 0.881, 0.76, 0.696, 0.706, 0.803, 0.808]\n","Validation Loss: 0.149, Accuracy of 12 AU classes: [0.923, 0.926, 0.883, 0.905, 0.873, 0.885, 0.909, 0.885, 0.792, 0.859, 0.82, 0.898]\n","Epoch: 9\n","Train Loss: 0.162, Accuracy of 12 AU classes: [0.888, 0.936, 0.891, 0.844, 0.789, 0.819, 0.883, 0.776, 0.723, 0.732, 0.807, 0.818]\n","Validation Loss: 0.149, Accuracy of 12 AU classes: [0.923, 0.921, 0.88, 0.892, 0.871, 0.883, 0.908, 0.891, 0.792, 0.857, 0.817, 0.9]\n","Epoch: 10\n","Train Loss: 0.159, Accuracy of 12 AU classes: [0.89, 0.938, 0.894, 0.865, 0.792, 0.822, 0.885, 0.789, 0.744, 0.753, 0.811, 0.827]\n","Validation Loss: 0.149, Accuracy of 12 AU classes: [0.92, 0.915, 0.878, 0.894, 0.87, 0.879, 0.908, 0.896, 0.787, 0.858, 0.814, 0.897]\n","4min 55s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1885,"status":"ok","timestamp":1715698510252,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"AkdBU_glwT2d","outputId":"a6e37e43-493b-481d-bf2c-b756ad893f1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.572, f1_threshold: 0.25, accuracy: [0.92, 0.915, 0.878, 0.894, 0.87, 0.879, 0.908, 0.896, 0.787, 0.858, 0.814, 0.897]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n","    AU_model = AU_fusion().to(device)\n","    AU_model.load_state_dict(AU_best_model)\n","    val_loss, f1s, f1t, acc = evaluate_model(AU_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":168270,"status":"ok","timestamp":1715715166866,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"FXEbsa3sOqGa","outputId":"407bc4f7-6c6f-4f31-aa7d-e4fd26caf1b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.289, Accuracy of 12 AU classes: [0.159, 0.052, 0.191, 0.467, 0.57, 0.405, 0.561, 0.194, 0.152, 0.041, 0.629, 0.079]\n","Validation Loss: 0.236, Accuracy of 12 AU classes: [0.508, 0.09, 0.673, 0.89, 0.814, 0.855, 0.89, 0.979, 0.174, 0.237, 0.746, 0.089]\n","Epoch: 2\n","Train Loss: 0.255, Accuracy of 12 AU classes: [0.128, 0.345, 0.496, 0.63, 0.651, 0.537, 0.713, 0.346, 0.285, 0.193, 0.667, 0.413]\n","Validation Loss: 0.201, Accuracy of 12 AU classes: [0.898, 0.911, 0.883, 0.883, 0.819, 0.854, 0.892, 0.989, 0.826, 0.937, 0.724, 0.91]\n","Epoch: 3\n","Train Loss: 0.237, Accuracy of 12 AU classes: [0.628, 0.546, 0.616, 0.683, 0.678, 0.596, 0.76, 0.555, 0.513, 0.452, 0.694, 0.582]\n","Validation Loss: 0.185, Accuracy of 12 AU classes: [0.919, 0.918, 0.879, 0.88, 0.823, 0.847, 0.89, 0.989, 0.826, 0.937, 0.76, 0.892]\n","Epoch: 4\n","Train Loss: 0.226, Accuracy of 12 AU classes: [0.691, 0.646, 0.673, 0.709, 0.694, 0.628, 0.783, 0.659, 0.627, 0.582, 0.709, 0.66]\n","Validation Loss: 0.176, Accuracy of 12 AU classes: [0.919, 0.925, 0.706, 0.882, 0.83, 0.856, 0.889, 0.989, 0.826, 0.937, 0.747, 0.838]\n","Epoch: 5\n","Train Loss: 0.218, Accuracy of 12 AU classes: [0.726, 0.704, 0.706, 0.725, 0.704, 0.649, 0.797, 0.721, 0.695, 0.66, 0.719, 0.7]\n","Validation Loss: 0.17, Accuracy of 12 AU classes: [0.916, 0.932, 0.809, 0.884, 0.835, 0.864, 0.889, 0.989, 0.826, 0.937, 0.761, 0.818]\n","Epoch: 6\n","Train Loss: 0.212, Accuracy of 12 AU classes: [0.748, 0.739, 0.729, 0.735, 0.711, 0.664, 0.807, 0.763, 0.741, 0.713, 0.726, 0.721]\n","Validation Loss: 0.165, Accuracy of 12 AU classes: [0.916, 0.927, 0.831, 0.887, 0.84, 0.871, 0.89, 0.989, 0.826, 0.937, 0.77, 0.814]\n","Epoch: 7\n","Train Loss: 0.207, Accuracy of 12 AU classes: [0.763, 0.763, 0.745, 0.743, 0.717, 0.675, 0.814, 0.792, 0.774, 0.75, 0.732, 0.735]\n","Validation Loss: 0.162, Accuracy of 12 AU classes: [0.916, 0.918, 0.842, 0.888, 0.843, 0.874, 0.901, 0.986, 0.826, 0.937, 0.773, 0.914]\n","Epoch: 8\n","Train Loss: 0.204, Accuracy of 12 AU classes: [0.774, 0.78, 0.757, 0.75, 0.722, 0.684, 0.819, 0.813, 0.798, 0.777, 0.737, 0.745]\n","Validation Loss: 0.16, Accuracy of 12 AU classes: [0.915, 0.91, 0.849, 0.89, 0.847, 0.877, 0.901, 0.978, 0.826, 0.937, 0.775, 0.911]\n","Epoch: 9\n","Train Loss: 0.201, Accuracy of 12 AU classes: [0.783, 0.793, 0.767, 0.755, 0.726, 0.691, 0.823, 0.829, 0.817, 0.799, 0.741, 0.752]\n","Validation Loss: 0.158, Accuracy of 12 AU classes: [0.915, 0.904, 0.852, 0.89, 0.849, 0.88, 0.891, 0.97, 0.826, 0.937, 0.778, 0.911]\n","Epoch: 10\n","Train Loss: 0.198, Accuracy of 12 AU classes: [0.79, 0.803, 0.775, 0.759, 0.729, 0.698, 0.827, 0.841, 0.832, 0.816, 0.727, 0.758]\n","Validation Loss: 0.157, Accuracy of 12 AU classes: [0.916, 0.9, 0.854, 0.892, 0.85, 0.883, 0.892, 0.961, 0.826, 0.937, 0.767, 0.909]\n","2min 47s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2800,"status":"ok","timestamp":1715715320635,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"1w6_jhQvmq4m","outputId":"7984edc9-5d2d-4a63-b7c0-3f72f513d82b"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.532, f1_threshold: 0.217, accuracy: [0.916, 0.9, 0.854, 0.892, 0.85, 0.883, 0.892, 0.961, 0.826, 0.937, 0.767, 0.909]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n","    mlp_model = MLPModel(num_classes = 12).to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, f1s, f1t, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"markdown","metadata":{"id":"1laG6DbGwUoR"},"source":["##### EffNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":291495,"status":"ok","timestamp":1715699539675,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"-9ehojtewUoR","outputId":"8516586f-5a46-45a4-a10c-a116d6054856"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.229, Accuracy of 12 AU classes: [0.848, 0.751, 0.81, 0.809, 0.694, 0.692, 0.818, 0.882, 0.479, 0.353, 0.749, 0.735]\n","Validation Loss: 0.192, Accuracy of 12 AU classes: [0.9, 0.735, 0.88, 0.882, 0.852, 0.877, 0.9, 0.92, 0.173, 0.741, 0.766, 0.915]\n","Epoch: 2\n","Train Loss: 0.208, Accuracy of 12 AU classes: [0.87, 0.813, 0.844, 0.822, 0.732, 0.737, 0.845, 0.913, 0.716, 0.145, 0.771, 0.561]\n","Validation Loss: 0.17, Accuracy of 12 AU classes: [0.906, 0.644, 0.859, 0.893, 0.858, 0.879, 0.904, 0.976, 0.188, 0.826, 0.796, 0.904]\n","Epoch: 3\n","Train Loss: 0.195, Accuracy of 12 AU classes: [0.879, 0.856, 0.861, 0.831, 0.749, 0.755, 0.855, 0.85, 0.192, 0.301, 0.783, 0.669]\n","Validation Loss: 0.159, Accuracy of 12 AU classes: [0.906, 0.93, 0.904, 0.895, 0.864, 0.881, 0.896, 0.923, 0.433, 0.863, 0.789, 0.92]\n","Epoch: 4\n","Train Loss: 0.185, Accuracy of 12 AU classes: [0.885, 0.879, 0.872, 0.838, 0.761, 0.767, 0.879, 0.872, 0.331, 0.408, 0.791, 0.726]\n","Validation Loss: 0.155, Accuracy of 12 AU classes: [0.91, 0.935, 0.896, 0.894, 0.855, 0.881, 0.904, 0.938, 0.651, 0.758, 0.778, 0.922]\n","Epoch: 5\n","Train Loss: 0.178, Accuracy of 12 AU classes: [0.889, 0.893, 0.88, 0.844, 0.769, 0.81, 0.882, 0.888, 0.436, 0.487, 0.785, 0.762]\n","Validation Loss: 0.154, Accuracy of 12 AU classes: [0.913, 0.939, 0.89, 0.88, 0.867, 0.88, 0.903, 0.948, 0.713, 0.804, 0.765, 0.92]\n","Epoch: 6\n","Train Loss: 0.173, Accuracy of 12 AU classes: [0.891, 0.903, 0.886, 0.849, 0.776, 0.814, 0.885, 0.9, 0.514, 0.548, 0.792, 0.785]\n","Validation Loss: 0.154, Accuracy of 12 AU classes: [0.914, 0.939, 0.888, 0.9, 0.863, 0.88, 0.901, 0.957, 0.765, 0.841, 0.763, 0.919]\n","Epoch: 7\n","Train Loss: 0.168, Accuracy of 12 AU classes: [0.894, 0.91, 0.89, 0.853, 0.782, 0.817, 0.887, 0.908, 0.573, 0.595, 0.797, 0.802]\n","Validation Loss: 0.154, Accuracy of 12 AU classes: [0.928, 0.939, 0.892, 0.879, 0.856, 0.877, 0.899, 0.964, 0.791, 0.889, 0.76, 0.918]\n","Epoch: 8\n","Train Loss: 0.164, Accuracy of 12 AU classes: [0.896, 0.916, 0.894, 0.857, 0.787, 0.82, 0.889, 0.915, 0.618, 0.633, 0.802, 0.9]\n","Validation Loss: 0.154, Accuracy of 12 AU classes: [0.913, 0.939, 0.892, 0.878, 0.852, 0.883, 0.898, 0.972, 0.805, 0.92, 0.766, 0.917]\n","Epoch: 9\n","Train Loss: 0.16, Accuracy of 12 AU classes: [0.897, 0.92, 0.898, 0.861, 0.792, 0.823, 0.891, 0.92, 0.653, 0.662, 0.806, 0.903]\n","Validation Loss: 0.154, Accuracy of 12 AU classes: [0.912, 0.938, 0.891, 0.878, 0.848, 0.88, 0.895, 0.977, 0.813, 0.926, 0.768, 0.917]\n","Epoch: 10\n","Train Loss: 0.157, Accuracy of 12 AU classes: [0.899, 0.923, 0.901, 0.864, 0.796, 0.826, 0.893, 0.925, 0.681, 0.687, 0.81, 0.905]\n","Validation Loss: 0.155, Accuracy of 12 AU classes: [0.927, 0.939, 0.86, 0.877, 0.846, 0.877, 0.893, 0.98, 0.817, 0.927, 0.768, 0.916]\n","4min 51s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2186,"status":"ok","timestamp":1715699949635,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"VtADdvikwUoR","outputId":"e52f51ce-0c51-4463-d77e-d8a652b5ce75"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.578, f1_threshold: 0.25, accuracy: [0.913, 0.939, 0.89, 0.88, 0.867, 0.88, 0.903, 0.948, 0.713, 0.804, 0.765, 0.92]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n","    AU_model = AU_fusion().to(device)\n","    AU_model.load_state_dict(AU_best_model)\n","    val_loss, f1s, f1t, acc = evaluate_model(AU_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":162184,"status":"ok","timestamp":1715716039337,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"RjeT7BQZOriE","outputId":"f511ed6d-5a96-45ab-cc2f-b066d9cc949a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.306, Accuracy of 12 AU classes: [0.241, 0.074, 0.173, 0.487, 0.553, 0.549, 0.668, 0.448, 0.041, 0.028, 0.689, 0.076]\n","Validation Loss: 0.248, Accuracy of 12 AU classes: [0.119, 0.178, 0.366, 0.891, 0.815, 0.865, 0.892, 0.768, 0.234, 0.091, 0.607, 0.088]\n","Epoch: 2\n","Train Loss: 0.268, Accuracy of 12 AU classes: [0.376, 0.155, 0.462, 0.647, 0.643, 0.66, 0.767, 0.343, 0.232, 0.122, 0.702, 0.082]\n","Validation Loss: 0.201, Accuracy of 12 AU classes: [0.905, 0.602, 0.888, 0.886, 0.818, 0.865, 0.898, 0.22, 0.177, 0.661, 0.742, 0.103]\n","Epoch: 3\n","Train Loss: 0.246, Accuracy of 12 AU classes: [0.547, 0.383, 0.598, 0.604, 0.673, 0.697, 0.803, 0.308, 0.213, 0.359, 0.717, 0.215]\n","Validation Loss: 0.184, Accuracy of 12 AU classes: [0.922, 0.89, 0.883, 0.889, 0.818, 0.87, 0.894, 0.976, 0.763, 0.884, 0.758, 0.693]\n","Epoch: 4\n","Train Loss: 0.233, Accuracy of 12 AU classes: [0.629, 0.521, 0.661, 0.649, 0.689, 0.716, 0.82, 0.473, 0.398, 0.509, 0.726, 0.36]\n","Validation Loss: 0.175, Accuracy of 12 AU classes: [0.923, 0.925, 0.614, 0.89, 0.796, 0.872, 0.892, 0.989, 0.827, 0.937, 0.747, 0.853]\n","Epoch: 5\n","Train Loss: 0.223, Accuracy of 12 AU classes: [0.675, 0.606, 0.697, 0.677, 0.699, 0.727, 0.831, 0.572, 0.512, 0.602, 0.732, 0.455]\n","Validation Loss: 0.17, Accuracy of 12 AU classes: [0.919, 0.929, 0.784, 0.891, 0.808, 0.875, 0.892, 0.989, 0.827, 0.937, 0.76, 0.858]\n","Epoch: 6\n","Train Loss: 0.217, Accuracy of 12 AU classes: [0.705, 0.66, 0.721, 0.696, 0.707, 0.736, 0.838, 0.639, 0.589, 0.663, 0.737, 0.518]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.917, 0.917, 0.828, 0.892, 0.817, 0.879, 0.892, 0.989, 0.827, 0.937, 0.768, 0.845]\n","Epoch: 7\n","Train Loss: 0.211, Accuracy of 12 AU classes: [0.725, 0.697, 0.737, 0.71, 0.713, 0.742, 0.843, 0.686, 0.643, 0.708, 0.741, 0.561]\n","Validation Loss: 0.163, Accuracy of 12 AU classes: [0.913, 0.904, 0.847, 0.892, 0.823, 0.883, 0.902, 0.988, 0.827, 0.937, 0.77, 0.839]\n","Epoch: 8\n","Train Loss: 0.207, Accuracy of 12 AU classes: [0.741, 0.724, 0.75, 0.72, 0.718, 0.747, 0.847, 0.721, 0.684, 0.741, 0.744, 0.593]\n","Validation Loss: 0.161, Accuracy of 12 AU classes: [0.913, 0.893, 0.855, 0.893, 0.827, 0.885, 0.903, 0.988, 0.827, 0.937, 0.752, 0.835]\n","Epoch: 9\n","Train Loss: 0.204, Accuracy of 12 AU classes: [0.753, 0.744, 0.76, 0.729, 0.722, 0.751, 0.85, 0.748, 0.716, 0.766, 0.747, 0.617]\n","Validation Loss: 0.159, Accuracy of 12 AU classes: [0.912, 0.933, 0.86, 0.894, 0.829, 0.888, 0.903, 0.982, 0.827, 0.937, 0.76, 0.915]\n","Epoch: 10\n","Train Loss: 0.201, Accuracy of 12 AU classes: [0.762, 0.76, 0.768, 0.735, 0.725, 0.755, 0.852, 0.768, 0.741, 0.787, 0.75, 0.636]\n","Validation Loss: 0.158, Accuracy of 12 AU classes: [0.913, 0.934, 0.862, 0.895, 0.829, 0.888, 0.903, 0.966, 0.827, 0.937, 0.766, 0.914]\n","2min 42s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 20, 32, optimizer, auft, weights, viau)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmh18erI1XYa","executionInfo":{"status":"ok","timestamp":1716122620847,"user_tz":-180,"elapsed":557746,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"5f68b92a-a797-4e2e-a7f2-0875e45bda8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1\n","Train Loss: 0.187, Accuracy of 12 AU classes: [0.864, 0.903, 0.835, 0.79, 0.742, 0.739, 0.874, 0.97, 0.97, 0.971, 0.745, 0.838]\n","Validation Loss: 0.17, Accuracy of 12 AU classes: [0.913, 0.917, 0.747, 0.889, 0.816, 0.876, 0.893, 0.989, 0.827, 0.937, 0.755, 0.89]\n","Epoch: 2\n","Train Loss: 0.185, Accuracy of 12 AU classes: [0.859, 0.911, 0.835, 0.791, 0.744, 0.741, 0.874, 0.969, 0.97, 0.972, 0.749, 0.832]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.907, 0.918, 0.822, 0.891, 0.825, 0.881, 0.893, 0.988, 0.827, 0.937, 0.765, 0.876]\n","Epoch: 3\n","Train Loss: 0.183, Accuracy of 12 AU classes: [0.856, 0.912, 0.836, 0.791, 0.745, 0.743, 0.874, 0.968, 0.97, 0.972, 0.751, 0.826]\n","Validation Loss: 0.162, Accuracy of 12 AU classes: [0.907, 0.912, 0.84, 0.891, 0.83, 0.884, 0.903, 0.986, 0.827, 0.937, 0.77, 0.868]\n","Epoch: 4\n","Train Loss: 0.182, Accuracy of 12 AU classes: [0.854, 0.911, 0.836, 0.791, 0.747, 0.745, 0.874, 0.966, 0.97, 0.972, 0.753, 0.822]\n","Validation Loss: 0.16, Accuracy of 12 AU classes: [0.908, 0.903, 0.852, 0.892, 0.832, 0.886, 0.903, 0.975, 0.827, 0.937, 0.773, 0.862]\n","Epoch: 5\n","Train Loss: 0.181, Accuracy of 12 AU classes: [0.853, 0.909, 0.837, 0.792, 0.748, 0.748, 0.874, 0.963, 0.97, 0.972, 0.755, 0.819]\n","Validation Loss: 0.159, Accuracy of 12 AU classes: [0.909, 0.896, 0.856, 0.893, 0.833, 0.889, 0.903, 0.963, 0.827, 0.937, 0.758, 0.861]\n","Epoch: 6\n","Train Loss: 0.18, Accuracy of 12 AU classes: [0.852, 0.908, 0.875, 0.793, 0.75, 0.75, 0.874, 0.961, 0.97, 0.972, 0.757, 0.817]\n","Validation Loss: 0.157, Accuracy of 12 AU classes: [0.909, 0.933, 0.86, 0.894, 0.833, 0.89, 0.904, 0.951, 0.827, 0.938, 0.764, 0.862]\n","Epoch: 7\n","Train Loss: 0.179, Accuracy of 12 AU classes: [0.851, 0.907, 0.875, 0.793, 0.751, 0.751, 0.874, 0.958, 0.97, 0.972, 0.758, 0.815]\n","Validation Loss: 0.156, Accuracy of 12 AU classes: [0.909, 0.934, 0.862, 0.895, 0.834, 0.89, 0.904, 0.945, 0.827, 0.938, 0.77, 0.862]\n","Epoch: 8\n","Train Loss: 0.178, Accuracy of 12 AU classes: [0.851, 0.906, 0.874, 0.794, 0.752, 0.753, 0.874, 0.956, 0.97, 0.972, 0.759, 0.814]\n","Validation Loss: 0.155, Accuracy of 12 AU classes: [0.91, 0.934, 0.864, 0.896, 0.835, 0.89, 0.904, 0.941, 0.827, 0.939, 0.772, 0.863]\n","Epoch: 9\n","Train Loss: 0.178, Accuracy of 12 AU classes: [0.851, 0.905, 0.874, 0.794, 0.753, 0.755, 0.874, 0.954, 0.97, 0.972, 0.761, 0.813]\n","Validation Loss: 0.154, Accuracy of 12 AU classes: [0.911, 0.934, 0.865, 0.897, 0.836, 0.89, 0.905, 0.939, 0.827, 0.94, 0.774, 0.864]\n","Epoch: 10\n","Train Loss: 0.177, Accuracy of 12 AU classes: [0.851, 0.904, 0.874, 0.795, 0.754, 0.756, 0.875, 0.953, 0.97, 0.971, 0.762, 0.813]\n","Validation Loss: 0.154, Accuracy of 12 AU classes: [0.911, 0.934, 0.866, 0.897, 0.836, 0.89, 0.906, 0.937, 0.827, 0.942, 0.775, 0.865]\n","Epoch: 11\n","Train Loss: 0.176, Accuracy of 12 AU classes: [0.851, 0.904, 0.874, 0.795, 0.755, 0.757, 0.875, 0.951, 0.97, 0.971, 0.763, 0.812]\n","Validation Loss: 0.153, Accuracy of 12 AU classes: [0.911, 0.933, 0.867, 0.897, 0.836, 0.89, 0.906, 0.936, 0.827, 0.946, 0.776, 0.865]\n","Epoch: 12\n","Train Loss: 0.176, Accuracy of 12 AU classes: [0.851, 0.946, 0.874, 0.796, 0.756, 0.758, 0.875, 0.95, 0.97, 0.971, 0.764, 0.812]\n","Validation Loss: 0.153, Accuracy of 12 AU classes: [0.927, 0.933, 0.866, 0.898, 0.836, 0.89, 0.895, 0.935, 0.827, 0.95, 0.777, 0.866]\n","Epoch: 13\n","Train Loss: 0.175, Accuracy of 12 AU classes: [0.851, 0.946, 0.874, 0.796, 0.757, 0.759, 0.875, 0.948, 0.97, 0.971, 0.765, 0.812]\n","Validation Loss: 0.152, Accuracy of 12 AU classes: [0.927, 0.933, 0.867, 0.897, 0.835, 0.89, 0.895, 0.934, 0.827, 0.951, 0.779, 0.867]\n","Epoch: 14\n","Train Loss: 0.175, Accuracy of 12 AU classes: [0.851, 0.946, 0.874, 0.797, 0.757, 0.76, 0.875, 0.947, 0.97, 0.97, 0.765, 0.812]\n","Validation Loss: 0.152, Accuracy of 12 AU classes: [0.927, 0.932, 0.867, 0.898, 0.833, 0.89, 0.896, 0.934, 0.827, 0.953, 0.78, 0.867]\n","Epoch: 15\n","Train Loss: 0.174, Accuracy of 12 AU classes: [0.851, 0.945, 0.874, 0.797, 0.758, 0.761, 0.875, 0.946, 0.97, 0.97, 0.766, 0.811]\n","Validation Loss: 0.152, Accuracy of 12 AU classes: [0.928, 0.933, 0.868, 0.898, 0.852, 0.89, 0.896, 0.933, 0.827, 0.954, 0.781, 0.867]\n","Epoch: 16\n","Train Loss: 0.174, Accuracy of 12 AU classes: [0.851, 0.945, 0.874, 0.798, 0.759, 0.762, 0.876, 0.945, 0.97, 0.97, 0.767, 0.909]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.929, 0.933, 0.868, 0.898, 0.853, 0.889, 0.897, 0.933, 0.827, 0.955, 0.782, 0.868]\n","Epoch: 17\n","Train Loss: 0.173, Accuracy of 12 AU classes: [0.851, 0.945, 0.875, 0.799, 0.759, 0.763, 0.876, 0.944, 0.97, 0.969, 0.768, 0.908]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.93, 0.932, 0.868, 0.898, 0.853, 0.889, 0.899, 0.932, 0.827, 0.957, 0.783, 0.869]\n","Epoch: 18\n","Train Loss: 0.173, Accuracy of 12 AU classes: [0.852, 0.945, 0.875, 0.799, 0.76, 0.764, 0.876, 0.944, 0.97, 0.969, 0.768, 0.908]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.93, 0.932, 0.869, 0.897, 0.853, 0.878, 0.9, 0.933, 0.827, 0.957, 0.783, 0.869]\n","Epoch: 19\n","Train Loss: 0.173, Accuracy of 12 AU classes: [0.852, 0.944, 0.875, 0.8, 0.76, 0.764, 0.876, 0.943, 0.97, 0.968, 0.769, 0.907]\n","Validation Loss: 0.151, Accuracy of 12 AU classes: [0.93, 0.932, 0.869, 0.898, 0.853, 0.879, 0.9, 0.933, 0.827, 0.957, 0.784, 0.87]\n","Epoch: 20\n","Train Loss: 0.172, Accuracy of 12 AU classes: [0.884, 0.944, 0.875, 0.8, 0.761, 0.765, 0.876, 0.942, 0.97, 0.968, 0.77, 0.907]\n","Validation Loss: 0.15, Accuracy of 12 AU classes: [0.93, 0.932, 0.869, 0.898, 0.853, 0.879, 0.901, 0.933, 0.827, 0.958, 0.785, 0.87]\n","9min 17s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":925,"status":"ok","timestamp":1716122862920,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"JD5X7Z7bu4sZ","outputId":"754d07f7-9a03-43a0-9607-5b2cc239650b"},"outputs":[{"output_type":"stream","name":"stdout","text":["best metric: f1_score 0.527, f1_threshold: 0.225, accuracy: [0.913, 0.934, 0.862, 0.895, 0.829, 0.888, 0.903, 0.966, 0.827, 0.937, 0.766, 0.914]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_mlp_{viau}.pth'))\n","    mlp_model = MLPModel(num_classes = 12).to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, f1s, f1t, acc, thresh = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"markdown","metadata":{"id":"6z2X3UCtFQQC"},"source":["#### Cropped images"]},{"cell_type":"markdown","metadata":{"id":"bje0cpc1FQQL"},"source":["##### EffNet + Wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269669,"status":"ok","timestamp":1715707400500,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"9NFf5x3kFQQL","outputId":"0f6e850e-2f2e-486d-dbb1-901b27b672cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.248, Accuracy of 12 AU classes: [0.464, 0.616, 0.696, 0.494, 0.59, 0.726, 0.822, 0.821, 0.398, 0.027, 0.698, 0.474]\n","Validation Loss: 0.201, Accuracy of 12 AU classes: [0.84, 0.875, 0.523, 0.893, 0.783, 0.88, 0.895, 0.906, 0.176, 0.55, 0.755, 0.465]\n","Epoch: 2\n","Train Loss: 0.224, Accuracy of 12 AU classes: [0.863, 0.773, 0.788, 0.641, 0.668, 0.754, 0.838, 0.881, 0.228, 0.089, 0.721, 0.633]\n","Validation Loss: 0.177, Accuracy of 12 AU classes: [0.913, 0.916, 0.754, 0.884, 0.826, 0.877, 0.898, 0.955, 0.176, 0.333, 0.743, 0.734]\n","Epoch: 3\n","Train Loss: 0.21, Accuracy of 12 AU classes: [0.872, 0.828, 0.82, 0.698, 0.701, 0.767, 0.847, 0.769, 0.135, 0.3, 0.735, 0.705]\n","Validation Loss: 0.168, Accuracy of 12 AU classes: [0.91, 0.919, 0.768, 0.888, 0.833, 0.881, 0.892, 0.978, 0.219, 0.754, 0.755, 0.867]\n","Epoch: 4\n","Train Loss: 0.2, Accuracy of 12 AU classes: [0.877, 0.944, 0.838, 0.808, 0.72, 0.776, 0.853, 0.812, 0.274, 0.437, 0.746, 0.889]\n","Validation Loss: 0.164, Accuracy of 12 AU classes: [0.908, 0.92, 0.801, 0.892, 0.847, 0.883, 0.89, 0.936, 0.376, 0.804, 0.757, 0.888]\n","Epoch: 5\n","Train Loss: 0.193, Accuracy of 12 AU classes: [0.881, 0.946, 0.85, 0.817, 0.733, 0.783, 0.858, 0.839, 0.383, 0.527, 0.755, 0.895]\n","Validation Loss: 0.163, Accuracy of 12 AU classes: [0.92, 0.923, 0.814, 0.894, 0.846, 0.884, 0.88, 0.946, 0.505, 0.846, 0.759, 0.896]\n","Epoch: 6\n","Train Loss: 0.187, Accuracy of 12 AU classes: [0.884, 0.948, 0.858, 0.824, 0.743, 0.789, 0.862, 0.858, 0.465, 0.59, 0.763, 0.899]\n","Validation Loss: 0.163, Accuracy of 12 AU classes: [0.921, 0.926, 0.862, 0.892, 0.838, 0.884, 0.879, 0.955, 0.564, 0.877, 0.762, 0.9]\n","Epoch: 7\n","Train Loss: 0.182, Accuracy of 12 AU classes: [0.887, 0.949, 0.865, 0.83, 0.751, 0.794, 0.865, 0.872, 0.527, 0.636, 0.77, 0.902]\n","Validation Loss: 0.164, Accuracy of 12 AU classes: [0.922, 0.928, 0.855, 0.892, 0.845, 0.884, 0.88, 0.959, 0.602, 0.881, 0.76, 0.903]\n","Epoch: 8\n","Train Loss: 0.177, Accuracy of 12 AU classes: [0.889, 0.949, 0.871, 0.835, 0.759, 0.799, 0.869, 0.882, 0.575, 0.672, 0.777, 0.904]\n","Validation Loss: 0.165, Accuracy of 12 AU classes: [0.923, 0.927, 0.852, 0.891, 0.842, 0.883, 0.877, 0.963, 0.634, 0.871, 0.761, 0.904]\n","Epoch: 9\n","Train Loss: 0.173, Accuracy of 12 AU classes: [0.891, 0.95, 0.876, 0.84, 0.765, 0.803, 0.872, 0.89, 0.614, 0.699, 0.783, 0.906]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.923, 0.928, 0.867, 0.889, 0.837, 0.88, 0.878, 0.966, 0.658, 0.856, 0.76, 0.905]\n","Epoch: 10\n","Train Loss: 0.169, Accuracy of 12 AU classes: [0.893, 0.951, 0.88, 0.845, 0.771, 0.807, 0.875, 0.897, 0.646, 0.722, 0.788, 0.908]\n","Validation Loss: 0.168, Accuracy of 12 AU classes: [0.918, 0.928, 0.865, 0.888, 0.831, 0.886, 0.876, 0.9, 0.663, 0.939, 0.757, 0.906]\n","4min 29s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2799,"status":"ok","timestamp":1715708111271,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"dD7JJRE2FQQL","outputId":"932130ed-f958-43fb-f86b-0a72eb3ceca9"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.552, f1_threshold: 0.258, accuracy: [0.92, 0.923, 0.814, 0.894, 0.846, 0.884, 0.88, 0.946, 0.505, 0.846, 0.759, 0.896]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n","    AU_model = AU_fusion().to(device)\n","    AU_model.load_state_dict(AU_best_model)\n","    val_loss, f1s, f1t, acc = evaluate_model(AU_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":171578,"status":"ok","timestamp":1715714109264,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"ahlzskb3FQQM","outputId":"8885bc71-7739-4f8f-b45e-34e33b81052b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.287, Accuracy of 12 AU classes: [0.126, 0.109, 0.306, 0.452, 0.535, 0.368, 0.505, 0.146, 0.073, 0.277, 0.63, 0.078]\n","Validation Loss: 0.24, Accuracy of 12 AU classes: [0.12, 0.577, 0.138, 0.878, 0.683, 0.843, 0.855, 0.933, 0.513, 0.089, 0.645, 0.089]\n","Epoch: 2\n","Train Loss: 0.256, Accuracy of 12 AU classes: [0.182, 0.47, 0.557, 0.607, 0.62, 0.491, 0.664, 0.294, 0.466, 0.362, 0.637, 0.24]\n","Validation Loss: 0.215, Accuracy of 12 AU classes: [0.51, 0.867, 0.409, 0.865, 0.811, 0.851, 0.882, 0.972, 0.821, 0.933, 0.723, 0.837]\n","Epoch: 3\n","Train Loss: 0.241, Accuracy of 12 AU classes: [0.244, 0.622, 0.644, 0.662, 0.652, 0.557, 0.717, 0.516, 0.633, 0.564, 0.655, 0.431]\n","Validation Loss: 0.201, Accuracy of 12 AU classes: [0.651, 0.881, 0.655, 0.891, 0.814, 0.853, 0.879, 0.986, 0.824, 0.936, 0.736, 0.89]\n","Epoch: 4\n","Train Loss: 0.23, Accuracy of 12 AU classes: [0.734, 0.699, 0.688, 0.69, 0.67, 0.594, 0.744, 0.629, 0.717, 0.666, 0.67, 0.536]\n","Validation Loss: 0.192, Accuracy of 12 AU classes: [0.721, 0.892, 0.754, 0.868, 0.819, 0.855, 0.88, 0.986, 0.824, 0.936, 0.719, 0.896]\n","Epoch: 5\n","Train Loss: 0.223, Accuracy of 12 AU classes: [0.759, 0.746, 0.716, 0.707, 0.682, 0.618, 0.761, 0.697, 0.767, 0.727, 0.68, 0.599]\n","Validation Loss: 0.186, Accuracy of 12 AU classes: [0.905, 0.904, 0.808, 0.871, 0.823, 0.874, 0.881, 0.985, 0.824, 0.936, 0.74, 0.89]\n","Epoch: 6\n","Train Loss: 0.218, Accuracy of 12 AU classes: [0.775, 0.776, 0.735, 0.72, 0.691, 0.636, 0.772, 0.742, 0.801, 0.768, 0.689, 0.641]\n","Validation Loss: 0.182, Accuracy of 12 AU classes: [0.908, 0.908, 0.833, 0.874, 0.83, 0.881, 0.882, 0.982, 0.824, 0.936, 0.749, 0.881]\n","Epoch: 7\n","Train Loss: 0.214, Accuracy of 12 AU classes: [0.787, 0.796, 0.749, 0.729, 0.698, 0.649, 0.781, 0.774, 0.825, 0.797, 0.695, 0.669]\n","Validation Loss: 0.178, Accuracy of 12 AU classes: [0.909, 0.911, 0.844, 0.875, 0.832, 0.885, 0.885, 0.973, 0.824, 0.936, 0.755, 0.874]\n","Epoch: 8\n","Train Loss: 0.21, Accuracy of 12 AU classes: [0.796, 0.812, 0.76, 0.737, 0.704, 0.659, 0.788, 0.797, 0.843, 0.819, 0.7, 0.69]\n","Validation Loss: 0.175, Accuracy of 12 AU classes: [0.912, 0.911, 0.85, 0.876, 0.834, 0.887, 0.886, 0.961, 0.824, 0.936, 0.76, 0.862]\n","Epoch: 9\n","Train Loss: 0.207, Accuracy of 12 AU classes: [0.803, 0.823, 0.769, 0.742, 0.709, 0.668, 0.793, 0.815, 0.857, 0.836, 0.704, 0.706]\n","Validation Loss: 0.172, Accuracy of 12 AU classes: [0.914, 0.909, 0.852, 0.877, 0.835, 0.887, 0.887, 0.945, 0.824, 0.936, 0.762, 0.856]\n","Epoch: 10\n","Train Loss: 0.205, Accuracy of 12 AU classes: [0.809, 0.832, 0.776, 0.747, 0.713, 0.675, 0.797, 0.828, 0.868, 0.849, 0.708, 0.718]\n","Validation Loss: 0.17, Accuracy of 12 AU classes: [0.914, 0.908, 0.856, 0.878, 0.837, 0.888, 0.888, 0.933, 0.824, 0.937, 0.764, 0.852]\n","2min 50s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":767,"status":"ok","timestamp":1715714441434,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"MwS9hLYpFQQM","outputId":"9c4b9e8c-5aa5-4d1a-ae59-fec82ef4d85a"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.502, f1_threshold: 0.258, accuracy: [0.914, 0.908, 0.856, 0.878, 0.837, 0.888, 0.888, 0.933, 0.824, 0.937, 0.764, 0.852]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n","    mlp_model = MLPModel(num_classes = 12).to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, f1s, f1t, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"markdown","metadata":{"id":"_dTKws0fFQQM"},"source":["##### EffNet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267213,"status":"ok","timestamp":1715709060627,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"aUn2vMUNFQQM","outputId":"e97326b5-e4e5-47be-e80d-b03275f5ad4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.237, Accuracy of 12 AU classes: [0.831, 0.388, 0.805, 0.657, 0.687, 0.737, 0.774, 0.873, 0.539, 0.495, 0.702, 0.372]\n","Validation Loss: 0.19, Accuracy of 12 AU classes: [0.897, 0.593, 0.896, 0.875, 0.819, 0.889, 0.884, 0.989, 0.176, 0.342, 0.781, 0.812]\n","Epoch: 2\n","Train Loss: 0.216, Accuracy of 12 AU classes: [0.861, 0.625, 0.844, 0.726, 0.708, 0.757, 0.84, 0.904, 0.324, 0.277, 0.726, 0.63]\n","Validation Loss: 0.171, Accuracy of 12 AU classes: [0.915, 0.884, 0.819, 0.89, 0.845, 0.883, 0.895, 0.971, 0.446, 0.642, 0.795, 0.909]\n","Epoch: 3\n","Train Loss: 0.205, Accuracy of 12 AU classes: [0.873, 0.891, 0.858, 0.754, 0.718, 0.768, 0.846, 0.805, 0.51, 0.491, 0.739, 0.72]\n","Validation Loss: 0.164, Accuracy of 12 AU classes: [0.912, 0.915, 0.889, 0.877, 0.83, 0.877, 0.897, 0.987, 0.673, 0.941, 0.78, 0.911]\n","Epoch: 4\n","Train Loss: 0.197, Accuracy of 12 AU classes: [0.88, 0.906, 0.866, 0.771, 0.725, 0.775, 0.851, 0.838, 0.611, 0.323, 0.727, 0.767]\n","Validation Loss: 0.161, Accuracy of 12 AU classes: [0.917, 0.915, 0.875, 0.885, 0.842, 0.88, 0.896, 0.953, 0.72, 0.626, 0.784, 0.908]\n","Epoch: 5\n","Train Loss: 0.191, Accuracy of 12 AU classes: [0.884, 0.915, 0.872, 0.783, 0.731, 0.781, 0.855, 0.86, 0.674, 0.431, 0.736, 0.795]\n","Validation Loss: 0.16, Accuracy of 12 AU classes: [0.931, 0.919, 0.862, 0.886, 0.833, 0.881, 0.896, 0.961, 0.74, 0.799, 0.782, 0.811]\n","Epoch: 6\n","Train Loss: 0.186, Accuracy of 12 AU classes: [0.887, 0.922, 0.877, 0.792, 0.736, 0.785, 0.858, 0.875, 0.717, 0.508, 0.744, 0.814]\n","Validation Loss: 0.16, Accuracy of 12 AU classes: [0.932, 0.92, 0.872, 0.887, 0.85, 0.881, 0.898, 0.968, 0.743, 0.87, 0.782, 0.817]\n","Epoch: 7\n","Train Loss: 0.182, Accuracy of 12 AU classes: [0.889, 0.926, 0.881, 0.831, 0.741, 0.789, 0.861, 0.887, 0.748, 0.565, 0.75, 0.828]\n","Validation Loss: 0.16, Accuracy of 12 AU classes: [0.932, 0.92, 0.85, 0.888, 0.85, 0.888, 0.899, 0.974, 0.75, 0.896, 0.78, 0.824]\n","Epoch: 8\n","Train Loss: 0.178, Accuracy of 12 AU classes: [0.891, 0.93, 0.884, 0.835, 0.745, 0.793, 0.864, 0.896, 0.771, 0.609, 0.756, 0.838]\n","Validation Loss: 0.16, Accuracy of 12 AU classes: [0.932, 0.92, 0.846, 0.901, 0.85, 0.889, 0.898, 0.978, 0.755, 0.904, 0.779, 0.83]\n","Epoch: 9\n","Train Loss: 0.175, Accuracy of 12 AU classes: [0.893, 0.933, 0.887, 0.839, 0.749, 0.796, 0.866, 0.903, 0.79, 0.644, 0.761, 0.846]\n","Validation Loss: 0.16, Accuracy of 12 AU classes: [0.931, 0.921, 0.845, 0.901, 0.848, 0.89, 0.889, 0.98, 0.764, 0.914, 0.775, 0.836]\n","Epoch: 10\n","Train Loss: 0.172, Accuracy of 12 AU classes: [0.894, 0.935, 0.889, 0.842, 0.753, 0.799, 0.868, 0.909, 0.805, 0.671, 0.766, 0.852]\n","Validation Loss: 0.16, Accuracy of 12 AU classes: [0.931, 0.921, 0.843, 0.899, 0.845, 0.89, 0.889, 0.982, 0.766, 0.918, 0.777, 0.842]\n","4min 27s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3437,"status":"ok","timestamp":1715709894448,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"5MpKsUFxFQQM","outputId":"015e9108-0245-4566-ef8a-55181aa30e1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.57, f1_threshold: 0.258, accuracy: [0.931, 0.919, 0.862, 0.886, 0.833, 0.881, 0.896, 0.961, 0.74, 0.799, 0.782, 0.811]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n","    AU_model = AU_fusion().to(device)\n","    AU_model.load_state_dict(AU_best_model)\n","    val_loss, f1s, f1t, acc = evaluate_model(AU_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":170558,"status":"ok","timestamp":1715717054665,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"vMxiLbHLFQQM","outputId":"0e610ba3-3cf1-4dd0-c003-dd31e5f06994"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.299, Accuracy of 12 AU classes: [0.14, 0.205, 0.18, 0.493, 0.472, 0.39, 0.539, 0.043, 0.031, 0.045, 0.63, 0.12]\n","Validation Loss: 0.245, Accuracy of 12 AU classes: [0.284, 0.091, 0.51, 0.888, 0.7, 0.848, 0.886, 0.151, 0.176, 0.21, 0.736, 0.721]\n","Epoch: 2\n","Train Loss: 0.266, Accuracy of 12 AU classes: [0.126, 0.336, 0.5, 0.633, 0.59, 0.511, 0.686, 0.226, 0.099, 0.189, 0.63, 0.191]\n","Validation Loss: 0.212, Accuracy of 12 AU classes: [0.131, 0.908, 0.139, 0.879, 0.797, 0.841, 0.881, 0.988, 0.557, 0.829, 0.737, 0.788]\n","Epoch: 3\n","Train Loss: 0.248, Accuracy of 12 AU classes: [0.148, 0.541, 0.611, 0.679, 0.632, 0.569, 0.731, 0.474, 0.341, 0.444, 0.661, 0.419]\n","Validation Loss: 0.196, Accuracy of 12 AU classes: [0.472, 0.909, 0.397, 0.878, 0.818, 0.844, 0.878, 0.989, 0.721, 0.936, 0.746, 0.895]\n","Epoch: 4\n","Train Loss: 0.237, Accuracy of 12 AU classes: [0.684, 0.643, 0.664, 0.703, 0.656, 0.601, 0.754, 0.599, 0.488, 0.576, 0.676, 0.533]\n","Validation Loss: 0.187, Accuracy of 12 AU classes: [0.694, 0.91, 0.693, 0.878, 0.827, 0.869, 0.878, 0.989, 0.798, 0.936, 0.712, 0.893]\n","Epoch: 5\n","Train Loss: 0.229, Accuracy of 12 AU classes: [0.721, 0.705, 0.696, 0.717, 0.671, 0.623, 0.769, 0.673, 0.583, 0.656, 0.686, 0.597]\n","Validation Loss: 0.181, Accuracy of 12 AU classes: [0.769, 0.92, 0.797, 0.878, 0.83, 0.878, 0.88, 0.989, 0.824, 0.936, 0.745, 0.89]\n","Epoch: 6\n","Train Loss: 0.224, Accuracy of 12 AU classes: [0.745, 0.746, 0.718, 0.727, 0.682, 0.639, 0.778, 0.723, 0.647, 0.708, 0.693, 0.638]\n","Validation Loss: 0.176, Accuracy of 12 AU classes: [0.813, 0.923, 0.83, 0.879, 0.832, 0.884, 0.882, 0.989, 0.824, 0.936, 0.759, 0.885]\n","Epoch: 7\n","Train Loss: 0.219, Accuracy of 12 AU classes: [0.762, 0.774, 0.734, 0.735, 0.69, 0.651, 0.786, 0.758, 0.693, 0.746, 0.699, 0.666]\n","Validation Loss: 0.173, Accuracy of 12 AU classes: [0.836, 0.922, 0.846, 0.88, 0.832, 0.888, 0.884, 0.989, 0.824, 0.936, 0.773, 0.88]\n","Epoch: 8\n","Train Loss: 0.215, Accuracy of 12 AU classes: [0.775, 0.794, 0.747, 0.74, 0.697, 0.661, 0.792, 0.785, 0.727, 0.774, 0.703, 0.687]\n","Validation Loss: 0.17, Accuracy of 12 AU classes: [0.852, 0.922, 0.852, 0.882, 0.832, 0.888, 0.884, 0.987, 0.824, 0.936, 0.778, 0.878]\n","Epoch: 9\n","Train Loss: 0.212, Accuracy of 12 AU classes: [0.784, 0.809, 0.757, 0.745, 0.702, 0.668, 0.796, 0.805, 0.754, 0.796, 0.686, 0.703]\n","Validation Loss: 0.168, Accuracy of 12 AU classes: [0.919, 0.922, 0.855, 0.897, 0.832, 0.889, 0.885, 0.983, 0.824, 0.936, 0.78, 0.874]\n","Epoch: 10\n","Train Loss: 0.21, Accuracy of 12 AU classes: [0.792, 0.821, 0.765, 0.749, 0.707, 0.675, 0.8, 0.82, 0.776, 0.814, 0.69, 0.715]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.921, 0.922, 0.856, 0.883, 0.831, 0.89, 0.886, 0.973, 0.824, 0.936, 0.783, 0.873]\n","2min 49s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3436,"status":"ok","timestamp":1715717170015,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"agq8c0aRFQQM","outputId":"6f49c990-1571-4325-9679-07e180693fd6"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.524, f1_threshold: 0.233, accuracy: [0.921, 0.922, 0.856, 0.883, 0.831, 0.89, 0.886, 0.973, 0.824, 0.936, 0.783, 0.873]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n","    mlp_model = MLPModel(num_classes = 12).to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, f1s, f1t, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"markdown","metadata":{"id":"dnnmn1mlFQQN"},"source":["##### EffNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":258860,"status":"ok","timestamp":1715710506699,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"tssAKXl5FQQN","outputId":"95d3ed80-af39-44cc-d1dd-5db82455135f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.237, Accuracy of 12 AU classes: [0.747, 0.821, 0.8, 0.698, 0.657, 0.705, 0.8, 0.751, 0.048, 0.053, 0.686, 0.834]\n","Validation Loss: 0.197, Accuracy of 12 AU classes: [0.903, 0.921, 0.852, 0.891, 0.852, 0.891, 0.888, 0.728, 0.187, 0.759, 0.791, 0.696]\n","Epoch: 2\n","Train Loss: 0.217, Accuracy of 12 AU classes: [0.797, 0.885, 0.842, 0.748, 0.703, 0.728, 0.826, 0.604, 0.251, 0.261, 0.717, 0.865]\n","Validation Loss: 0.177, Accuracy of 12 AU classes: [0.893, 0.903, 0.862, 0.88, 0.859, 0.893, 0.902, 0.956, 0.381, 0.728, 0.804, 0.794]\n","Epoch: 3\n","Train Loss: 0.205, Accuracy of 12 AU classes: [0.88, 0.906, 0.859, 0.769, 0.723, 0.74, 0.837, 0.363, 0.411, 0.402, 0.734, 0.876]\n","Validation Loss: 0.17, Accuracy of 12 AU classes: [0.899, 0.934, 0.89, 0.888, 0.859, 0.893, 0.887, 0.812, 0.567, 0.776, 0.789, 0.817]\n","Epoch: 4\n","Train Loss: 0.197, Accuracy of 12 AU classes: [0.883, 0.917, 0.868, 0.781, 0.736, 0.748, 0.844, 0.483, 0.512, 0.493, 0.745, 0.882]\n","Validation Loss: 0.167, Accuracy of 12 AU classes: [0.898, 0.938, 0.879, 0.888, 0.856, 0.893, 0.892, 0.849, 0.668, 0.813, 0.781, 0.901]\n","Epoch: 5\n","Train Loss: 0.191, Accuracy of 12 AU classes: [0.884, 0.924, 0.875, 0.79, 0.746, 0.755, 0.848, 0.56, 0.583, 0.556, 0.753, 0.885]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.897, 0.937, 0.888, 0.881, 0.853, 0.892, 0.892, 0.87, 0.734, 0.845, 0.782, 0.9]\n","Epoch: 6\n","Train Loss: 0.187, Accuracy of 12 AU classes: [0.886, 0.928, 0.88, 0.797, 0.753, 0.76, 0.852, 0.615, 0.634, 0.604, 0.76, 0.887]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.898, 0.906, 0.886, 0.877, 0.866, 0.889, 0.895, 0.884, 0.788, 0.959, 0.785, 0.862]\n","Epoch: 7\n","Train Loss: 0.183, Accuracy of 12 AU classes: [0.887, 0.932, 0.885, 0.803, 0.759, 0.765, 0.855, 0.655, 0.673, 0.643, 0.765, 0.889]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.903, 0.913, 0.882, 0.889, 0.866, 0.889, 0.896, 0.894, 0.804, 0.968, 0.786, 0.871]\n","Epoch: 8\n","Train Loss: 0.179, Accuracy of 12 AU classes: [0.888, 0.934, 0.888, 0.808, 0.764, 0.769, 0.857, 0.686, 0.704, 0.674, 0.77, 0.89]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.904, 0.916, 0.879, 0.886, 0.865, 0.888, 0.896, 0.905, 0.803, 0.972, 0.763, 0.875]\n","Epoch: 9\n","Train Loss: 0.176, Accuracy of 12 AU classes: [0.889, 0.936, 0.891, 0.812, 0.769, 0.772, 0.86, 0.71, 0.728, 0.701, 0.774, 0.891]\n","Validation Loss: 0.167, Accuracy of 12 AU classes: [0.906, 0.921, 0.873, 0.88, 0.866, 0.887, 0.894, 0.913, 0.805, 0.973, 0.764, 0.876]\n","Epoch: 10\n","Train Loss: 0.173, Accuracy of 12 AU classes: [0.89, 0.938, 0.894, 0.816, 0.773, 0.776, 0.862, 0.73, 0.747, 0.723, 0.778, 0.892]\n","Validation Loss: 0.168, Accuracy of 12 AU classes: [0.908, 0.926, 0.87, 0.876, 0.866, 0.886, 0.893, 0.923, 0.806, 0.975, 0.77, 0.877]\n","4min 18s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2070,"status":"ok","timestamp":1715710700881,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"LRxpiLQiFQQN","outputId":"e6ddf404-a6a8-4a4a-8fad-a98d89a57533"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.542, f1_threshold: 0.25, accuracy: [0.897, 0.937, 0.888, 0.881, 0.853, 0.892, 0.892, 0.87, 0.734, 0.845, 0.782, 0.9]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n","    AU_model = AU_fusion().to(device)\n","    AU_model.load_state_dict(AU_best_model)\n","    val_loss, f1s, f1t, acc = evaluate_model(AU_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163360,"status":"ok","timestamp":1715717663056,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"eYH9Pzv7FQQN","outputId":"12f29d1d-6291-4952-a712-03b02f634e6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1\n","Train Loss: 0.313, Accuracy of 12 AU classes: [0.281, 0.339, 0.204, 0.492, 0.504, 0.518, 0.513, 0.568, 0.064, 0.039, 0.627, 0.076]\n","Validation Loss: 0.259, Accuracy of 12 AU classes: [0.119, 0.286, 0.662, 0.891, 0.806, 0.843, 0.889, 0.877, 0.282, 0.072, 0.711, 0.089]\n","Epoch: 2\n","Train Loss: 0.276, Accuracy of 12 AU classes: [0.395, 0.226, 0.296, 0.503, 0.6, 0.498, 0.673, 0.436, 0.05, 0.253, 0.627, 0.086]\n","Validation Loss: 0.212, Accuracy of 12 AU classes: [0.882, 0.749, 0.833, 0.867, 0.818, 0.847, 0.878, 0.731, 0.194, 0.068, 0.721, 0.094]\n","Epoch: 3\n","Train Loss: 0.256, Accuracy of 12 AU classes: [0.558, 0.451, 0.466, 0.587, 0.635, 0.561, 0.636, 0.411, 0.266, 0.235, 0.635, 0.229]\n","Validation Loss: 0.195, Accuracy of 12 AU classes: [0.474, 0.903, 0.193, 0.872, 0.825, 0.849, 0.88, 0.908, 0.769, 0.898, 0.668, 0.616]\n","Epoch: 4\n","Train Loss: 0.244, Accuracy of 12 AU classes: [0.637, 0.574, 0.554, 0.631, 0.532, 0.596, 0.68, 0.544, 0.44, 0.416, 0.648, 0.369]\n","Validation Loss: 0.185, Accuracy of 12 AU classes: [0.699, 0.915, 0.52, 0.875, 0.832, 0.874, 0.882, 0.951, 0.825, 0.936, 0.71, 0.774]\n","Epoch: 5\n","Train Loss: 0.235, Accuracy of 12 AU classes: [0.683, 0.648, 0.608, 0.659, 0.557, 0.619, 0.707, 0.626, 0.545, 0.527, 0.658, 0.463]\n","Validation Loss: 0.18, Accuracy of 12 AU classes: [0.781, 0.922, 0.726, 0.892, 0.837, 0.881, 0.885, 0.964, 0.825, 0.937, 0.734, 0.808]\n","Epoch: 6\n","Train Loss: 0.229, Accuracy of 12 AU classes: [0.712, 0.697, 0.644, 0.677, 0.575, 0.635, 0.784, 0.681, 0.616, 0.601, 0.667, 0.527]\n","Validation Loss: 0.175, Accuracy of 12 AU classes: [0.838, 0.923, 0.809, 0.892, 0.84, 0.886, 0.886, 0.96, 0.825, 0.937, 0.753, 0.827]\n","Epoch: 7\n","Train Loss: 0.224, Accuracy of 12 AU classes: [0.733, 0.73, 0.671, 0.691, 0.589, 0.648, 0.792, 0.72, 0.667, 0.654, 0.674, 0.573]\n","Validation Loss: 0.172, Accuracy of 12 AU classes: [0.868, 0.924, 0.837, 0.893, 0.842, 0.889, 0.886, 0.952, 0.825, 0.937, 0.761, 0.845]\n","Epoch: 8\n","Train Loss: 0.22, Accuracy of 12 AU classes: [0.748, 0.754, 0.691, 0.701, 0.6, 0.658, 0.799, 0.749, 0.704, 0.693, 0.679, 0.606]\n","Validation Loss: 0.17, Accuracy of 12 AU classes: [0.879, 0.927, 0.848, 0.893, 0.843, 0.89, 0.887, 0.945, 0.825, 0.937, 0.767, 0.856]\n","Epoch: 9\n","Train Loss: 0.217, Accuracy of 12 AU classes: [0.76, 0.772, 0.707, 0.709, 0.609, 0.666, 0.804, 0.772, 0.734, 0.724, 0.684, 0.632]\n","Validation Loss: 0.168, Accuracy of 12 AU classes: [0.883, 0.931, 0.855, 0.894, 0.844, 0.882, 0.887, 0.939, 0.825, 0.937, 0.771, 0.859]\n","Epoch: 10\n","Train Loss: 0.214, Accuracy of 12 AU classes: [0.769, 0.787, 0.719, 0.716, 0.617, 0.672, 0.808, 0.789, 0.757, 0.749, 0.687, 0.652]\n","Validation Loss: 0.166, Accuracy of 12 AU classes: [0.923, 0.93, 0.858, 0.863, 0.844, 0.885, 0.888, 0.934, 0.825, 0.937, 0.772, 0.861]\n","2min 44s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1223,"status":"ok","timestamp":1715717994958,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"UN5UKQCcFQQN","outputId":"ec30a961-17cd-4991-e0d4-02e8cfe99423"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: f1_score 0.513, f1_threshold: 0.217, accuracy: [0.923, 0.93, 0.858, 0.863, 0.844, 0.885, 0.888, 0.934, 0.825, 0.937, 0.772, 0.861]\n"]}],"source":["try:\n","    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n","    mlp_model = MLPModel(num_classes = 12).to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, f1s, f1t, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n","    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"markdown","metadata":{"id":"8iIGwebQcq1G"},"source":["### Testing"]},{"cell_type":"markdown","metadata":{"id":"d32EYknyadOP"},"source":["#### Cropped_aligned images"]},{"cell_type":"markdown","metadata":{"id":"eENa-EAHacgS"},"source":["##### EffNet + Wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14944,"status":"ok","timestamp":1715865940670,"user":{"displayName":"Uyên Võ","userId":"08500545526524026138"},"user_tz":-180},"id":"xnqbW-EZkWbG","outputId":"cc87c2a0-a747-4a55-adc5-f08cfa03579c"},"outputs":[{"name":"stdout","output_type":"stream","text":["AU_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: f1_score: 0.449, f1_threshold: [0.30000000000000004, 0.30000000000000004, 0.2, 0.30000000000000004, 0.5, 0.30000000000000004, 0.6, 0.1, 0.1, 0.2, 0.6, 0.1], accuracy: [0.844, 0.978, 0.846, 0.875, 0.79, 0.814, 0.916, 0.859, 0.569, 0.961, 0.778, 0.819]\n","14.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('AU_model')\n","print(visual_feat + ' & ' + auft)\n","AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_fusion_{viau}.pth'))\n","AU_model = AU_fusion().to(device)\n","AU_model.load_state_dict(AU_best_model)\n","test_loss, f1s, f1t, acc, threshold = evaluate_model(AU_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {threshold}, accuracy: {acc}')"]},{"cell_type":"code","source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n","mlp_model = MLPModel(num_classes = 12).to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, f1s, f1t, acc, threshold = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {threshold}, accuracy: {acc}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cgvjr5Ah5lcU","executionInfo":{"status":"ok","timestamp":1716127097663,"user_tz":-180,"elapsed":2670,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"55082795-19f7-4d9e-e0b6-5b552e4aa6aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: f1_score: 0.463, f1_threshold: [0.30000000000000004, 0.2, 0.2, 0.30000000000000004, 0.5, 0.30000000000000004, 0.6, 0.1, 0.1, 0.1, 0.6, 0.30000000000000004], accuracy: [0.89, 0.961, 0.862, 0.881, 0.779, 0.828, 0.922, 0.936, 0.98, 0.667, 0.784, 0.936]\n","1.58 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}]},{"cell_type":"markdown","metadata":{"id":"u0-X9uZiali4"},"source":["##### EffNet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2746,"status":"ok","timestamp":1715698396597,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"0W27nWyqlld7","outputId":"803003c5-0ae8-4203-e55c-be03f4bb7c7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["AU_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: f1_score: 0.439, f1_threshold: 0.3, accuracy: [0.823, 0.965, 0.809, 0.816, 0.812, 0.814, 0.898, 0.901, 0.98, 0.783, 0.753, 0.944]\n","2.72 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('AU_model')\n","print(visual_feat + ' & ' + auft)\n","AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n","AU_model = AU_fusion().to(device)\n","AU_model.load_state_dict(AU_best_model)\n","test_loss, f1s, f1t, acc = evaluate_model(AU_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":877,"status":"ok","timestamp":1715715167724,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"UbPwiOjOmxD5","outputId":"3c012821-923c-4752-b9ee-079bb46ca91b"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: f1_score: 0.423, f1_threshold: 0.267, accuracy: [0.866, 0.935, 0.801, 0.754, 0.782, 0.806, 0.923, 0.955, 0.98, 0.982, 0.78, 0.855]\n","1.53 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n","mlp_model = MLPModel(num_classes = 12).to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, f1s, f1t, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"markdown","metadata":{"id":"iOd79RahaoMZ"},"source":["##### EffNet"]},{"cell_type":"code","source":["%%timeit -n 1 -r 1\n","print('AU_model')\n","print(visual_feat + ' & ' + auft)\n","AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_fusion_{viau}.pth'))\n","AU_model = AU_fusion().to(device)\n","AU_model.load_state_dict(AU_best_model)\n","test_loss, f1s, f1t, acc, threshold = evaluate_model(AU_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {threshold}, accuracy: {acc}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YrDXkMYkCqj-","executionInfo":{"status":"ok","timestamp":1715974421812,"user_tz":-180,"elapsed":3670,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"b774c152-4c73-4060-8876-0b32e2b38232"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AU_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: f1_score: 0.455, f1_threshold: [0.30000000000000004, 0.30000000000000004, 0.2, 0.2, 0.6, 0.30000000000000004, 0.6, 0.1, 0.1, 0.2, 0.5, 0.2], accuracy: [0.882, 0.97, 0.833, 0.792, 0.834, 0.799, 0.916, 0.832, 0.404, 0.892, 0.75, 0.914]\n","2.71 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3584,"status":"ok","timestamp":1715699543240,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"zI35wCBbw6fy","outputId":"6dc629a0-6b43-4191-c989-28a3565e6ce8"},"outputs":[{"name":"stdout","output_type":"stream","text":["AU_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: f1_score: 0.455, f1_threshold: 0.3, accuracy: [0.882, 0.97, 0.833, 0.792, 0.834, 0.799, 0.916, 0.832, 0.404, 0.892, 0.75, 0.914]\n","2.52 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('AU_model')\n","print(visual_feat + ' & ' + auft)\n","AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n","AU_model = AU_fusion().to(device)\n","AU_model.load_state_dict(AU_best_model)\n","test_loss, f1s, f1t, acc = evaluate_model(AU_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"code","source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n","mlp_model = MLPModel(num_classes = 12).to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, f1s, f1t, acc, threshold = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {threshold}, accuracy: {acc}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d1-DGeZ91usG","executionInfo":{"status":"ok","timestamp":1716122625444,"user_tz":-180,"elapsed":2975,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"645dece5-240d-4f96-8619-e63c80b61f5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MLP_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: f1_score: 0.443, f1_threshold: [0.30000000000000004, 0.2, 0.2, 0.30000000000000004, 0.5, 0.30000000000000004, 0.6, 0.1, 0.1, 0.1, 0.5, 0.2], accuracy: [0.903, 0.97, 0.805, 0.815, 0.812, 0.819, 0.924, 0.94, 0.98, 0.768, 0.779, 0.93]\n","1.39 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}]},{"cell_type":"markdown","metadata":{"id":"xzFpMJckaiVt"},"source":["#### Cropped images"]},{"cell_type":"markdown","metadata":{"id":"yrtmaqyKavcX"},"source":["##### EffNet + Wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4747,"status":"ok","timestamp":1715707587623,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"eZbp9lzOaiV1","outputId":"d0131d3d-ab78-41f4-cdf8-f3b7545c499e"},"outputs":[{"name":"stdout","output_type":"stream","text":["AU_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: f1_score: 0.443, f1_threshold: 0.3, accuracy: [0.83, 0.955, 0.813, 0.771, 0.806, 0.812, 0.928, 0.964, 0.952, 0.74, 0.763, 0.905]\n","2.94 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('AU_model')\n","print(visual_feat + ' & ' + auft)\n","AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n","AU_model = AU_fusion().to(device)\n","AU_model.load_state_dict(AU_best_model)\n","test_loss, f1s, f1t, acc = evaluate_model(AU_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2390,"status":"ok","timestamp":1715714113346,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"uMh5aG1wKf9q","outputId":"93af7751-e759-49c5-8a1f-8234cb814ebd"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: f1_score: 0.435, f1_threshold: 0.308, accuracy: [0.884, 0.982, 0.828, 0.801, 0.799, 0.838, 0.927, 0.953, 0.979, 0.96, 0.765, 0.948]\n","1.82 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, f1s, f1t, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"markdown","metadata":{"id":"4h9G2Bwnazby"},"source":["##### EffNet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3776,"status":"ok","timestamp":1715709067746,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"J_m5CwxaaiV1","outputId":"2ca8933f-45d9-440a-99fd-27ac089b5962"},"outputs":[{"name":"stdout","output_type":"stream","text":["AU_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: f1_score: 0.445, f1_threshold: 0.333, accuracy: [0.879, 0.976, 0.802, 0.808, 0.812, 0.825, 0.922, 0.959, 0.898, 0.863, 0.773, 0.793]\n","2.82 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('AU_model')\n","print(visual_feat + ' & ' + auft)\n","AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n","AU_model = AU_fusion().to(device)\n","AU_model.load_state_dict(AU_best_model)\n","test_loss, f1s, f1t, acc = evaluate_model(AU_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1941,"status":"ok","timestamp":1715717056595,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"wco2KZWbPcLH","outputId":"0a6dea80-3294-4e9a-c985-9219b274d63b"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: f1_score: 0.43, f1_threshold: 0.275, accuracy: [0.903, 0.981, 0.751, 0.755, 0.816, 0.813, 0.925, 0.958, 0.98, 0.985, 0.767, 0.78]\n","1.61 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n","mlp_model = MLPModel(num_classes = 12).to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, f1s, f1t, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"markdown","metadata":{"id":"kh-C9EeQa4mF"},"source":["##### EffNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4157,"status":"ok","timestamp":1715710510849,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"YZgv-tm_aiV1","outputId":"f52014d5-551a-43c8-a85e-1daf7f341951"},"outputs":[{"name":"stdout","output_type":"stream","text":["AU_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: f1_score: 0.439, f1_threshold: 0.317, accuracy: [0.891, 0.987, 0.836, 0.781, 0.803, 0.827, 0.92, 0.764, 0.754, 0.715, 0.777, 0.851]\n","2.64 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('AU_model')\n","print(visual_feat + ' & ' + auft)\n","AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n","AU_model = AU_fusion().to(device)\n","AU_model.load_state_dict(AU_best_model)\n","test_loss, f1s, f1t, acc = evaluate_model(AU_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2183,"status":"ok","timestamp":1715717665223,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"0o2OkvxWVP-a","outputId":"a848c3f8-8930-4be7-85cf-77ff23bd86dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: f1_score: 0.427, f1_threshold: 0.275, accuracy: [0.905, 0.978, 0.781, 0.788, 0.815, 0.817, 0.914, 0.95, 0.98, 0.984, 0.754, 0.793]\n","1.39 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n","mlp_model = MLPModel(num_classes = 12).to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, f1s, f1t, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n","print(f'Test set: f1_score: {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"]},{"cell_type":"markdown","metadata":{"id":"ZWSTa4TEzNLi"},"source":["## VA Estimation Challenge"]},{"cell_type":"markdown","metadata":{"id":"0D1rOOtUPGJS"},"source":["### Loading data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PpOnAdBZHEf"},"outputs":[],"source":["# Cropped_aligned images\n","vis = vis_typ[0]\n","visft = visual_feat[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1E65uF_ZHEl"},"outputs":[],"source":["# Cropped images\n","vis = vis_typ[1]\n","visft = visual_feat[1]"]},{"cell_type":"markdown","metadata":{"id":"6loHg0Oaml6s"},"source":["#### Effnet + wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qaoMmvejsdR"},"outputs":[],"source":["auft = audio_feat[0]\n","viau = vis_aud[0]"]},{"cell_type":"markdown","metadata":{"id":"grJntlFqms8l"},"source":["#### Effnet + vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EhVH9IjHmruz"},"outputs":[],"source":["auft = audio_feat[1]\n","viau = vis_aud[1]"]},{"cell_type":"markdown","metadata":{"id":"0SLN9gglm0XA"},"source":["#### Effnet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4LyVKQQm23-"},"outputs":[],"source":["auft = audio_feat[2]\n","viau = vis_aud[2]"]},{"cell_type":"markdown","metadata":{"id":"WvvlnRz-ZRXj"},"source":["#### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140220,"status":"ok","timestamp":1716119996434,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"ROpV5jKu6Y8N","outputId":"d3fa547c-5f3d-4d8d-92b8-d362200b097f"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:10<00:00, 26.43it/s]\n"]}],"source":["if auft == audio_feat[0]:\n","    with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[0]}_visual.pkl'), 'rb') as f:\n","        data1 = pickle.load(f)\n","    with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[0]}.txt'), 'r') as f:\n","        vidnames = f.read().splitlines()\n","    task1 = task[2]\n","    feature_a = 'audiofeat_wav2vec2'\n","    feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","    filenames = os.listdir(feat_root)[:]\n","    for vname in tqdm(vidnames):\n","            feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","            for imgname, val in feature.items():\n","                if imgname in data1[task1][vname]:\n","                    data1[task1][vname][imgname].update({f'{feature_a}': val})\n","            for img, value in list(data1[task1][vname].items()):\n","                if len(value) < 3:\n","                    data1[task1][vname].pop(img)\n","elif auft == audio_feat[1]:\n","    with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[0]}_visual.pkl'), 'rb') as f:\n","        data1 = pickle.load(f)\n","    with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[0]}.txt'), 'r') as f:\n","        vidnames = f.read().splitlines()\n","    task1 = task[2]\n","    feature_a = 'audiofeat_vggish'\n","    feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","    filenames = os.listdir(feat_root)[:]\n","    for vname in tqdm(vidnames):\n","            feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","            for imgname, val in feature.items():\n","                if imgname in data1[task1][vname]:\n","                    data1[task1][vname][imgname].update({f'{feature_a}': val})\n","            for img, value in list(data1[task1][vname].items()):\n","                if len(value) < 3:\n","                    data1[task1][vname].pop(img)\n","else:\n","    with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[0]}_{viau}.pkl'), 'rb') as f:\n","        data1 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1716119996434,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"RCJoUSEEZRXk","outputId":"160ae36f-e786-4803-a432-8a3b7da0b535"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:00<00:00, 1249.04it/s]\n"]}],"source":["dims = 0\n","iname = []\n","task1 = task[2]\n","with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[0]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","for vname in tqdm(vidnames):\n","    dims += len(data1[task1][vname])\n","    for img in data1[task1][vname].keys():\n","        iname.append(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z24CCEkwZRXk"},"outputs":[],"source":["dataset = ABAW_dataset1(data1, iname, dims, task1)\n","train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"3MwWOEO-ZRXk"},"source":["#### Val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJuunI3sZRXk"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[1]}_{viau}.pkl'), 'rb') as f:\n","    data2 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1715765833022,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"5riL0y7UZRXk","outputId":"c70ea33b-ba33-42f6-81fc-34e00034faaa"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 71/71 [00:00<00:00, 934.23it/s]\n"]}],"source":["dims = 0\n","iname = []\n","task1 = task[2]\n","with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[1]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","for vname in tqdm(vidnames):\n","    dims += len(data2[task1][vname])\n","    for img in data2[task1][vname].keys():\n","        iname.append(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xRI-XOs9ZRXk"},"outputs":[],"source":["dataset = ABAW_dataset1(data2, iname, dims, task1)\n","val_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"IM-R-dPNZRXk"},"source":["#### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiWBsFEXZRXk"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n","    data3 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":389,"status":"ok","timestamp":1716119618467,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"kUNvkmldZRXk","outputId":"4ff33eea-4f89-460a-ff48-652c8b5fabe8"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 76/76 [00:00<00:00, 1159.12it/s]\n"]}],"source":["dims = 0\n","iname = []\n","task1 = task[2]\n","with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[2]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","for vname in tqdm(vidnames):\n","    dims += len(data3[task1][vname])\n","    for img in data3[task1][vname].keys():\n","        iname.append(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAm67bXaZRXk"},"outputs":[],"source":["dataset = ABAW_dataset1(data3, iname, dims, task1)\n","test_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)"]},{"cell_type":"markdown","metadata":{"id":"uo7YJPxtPMXQ"},"source":["### Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVrhnVSdrkn9"},"outputs":[],"source":["class VA_fusion(nn.Module):\n","    def __init__(self, batchsize = batch_size, audio_ft = auft, hidden_size = [512, 128, batch_size]):\n","        super(VA_fusion, self).__init__()\n","        self.batchsize = batchsize\n","        if audio_ft == 'audiofeat_wav2vec2':\n","            self.concat_dim = 2176    #1408+768\n","        elif audio_ft == 'audiofeat_vggish':\n","            self.concat_dim = 1536    #1408+128\n","        elif audio_ft == 'nope':\n","            self.concat_dim = 1408    #visual only\n","        self.hidden_size = hidden_size\n","        self.feat_fc = nn.Conv1d(self.concat_dim, hidden_size[0], 1, padding=0)\n","        self.activ = nn.LeakyReLU(0.1)\n","        self.dropout = nn.Dropout(p=0.3)\n","        self.conv1 = nn.Conv1d(hidden_size[0], hidden_size[1], 1, padding=0)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size[1], nhead=4, dim_feedforward=hidden_size[1], dropout=0.3)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n","        self.vhead = nn.Sequential(\n","                nn.Linear(hidden_size[1], hidden_size[2]),\n","                nn.BatchNorm1d(hidden_size[2]),\n","                nn.Linear(hidden_size[2], 1),\n","                )\n","        self.ahead = nn.Sequential(\n","                nn.Linear(hidden_size[1], hidden_size[2]),\n","                nn.BatchNorm1d(hidden_size[2]),\n","                nn.Linear(hidden_size[2], 1),\n","                )\n","\n","    def forward(self, vis_feat, aud_feat):\n","        if aud_feat == None:\n","            feat = vis_feat\n","        else:\n","            inputs = [vis_feat]\n","            inputs.append(aud_feat)\n","            feat = torch.cat(inputs,dim=1)\n","        feat = torch.transpose(feat,0,1)\n","        feat = self.feat_fc(feat)\n","        feat = self.activ(feat)\n","        out = self.conv1(feat)\n","        out = torch.transpose(out,0,1)\n","        out = self.transformer_encoder(out)\n","        vout = self.vhead(out)\n","        aout = self.ahead(out)\n","\n","        return vout, aout, torch.tanh(vout), torch.tanh(aout)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":452,"status":"ok","timestamp":1715765882261,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"rGReEGQ_PQb7","outputId":"7a84c7e8-428d-4c4a-903b-8e8ab1b8841c"},"outputs":[{"data":{"text/plain":["VA_fusion(\n","  (feat_fc): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n","  (activ): LeakyReLU(negative_slope=0.1)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-3): 4 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=128, bias=True)\n","        (dropout): Dropout(p=0.3, inplace=False)\n","        (linear2): Linear(in_features=128, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.3, inplace=False)\n","        (dropout2): Dropout(p=0.3, inplace=False)\n","      )\n","    )\n","  )\n","  (vhead): Sequential(\n","    (0): Linear(in_features=128, out_features=32, bias=True)\n","    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): Linear(in_features=32, out_features=1, bias=True)\n","  )\n","  (ahead): Sequential(\n","    (0): Linear(in_features=128, out_features=32, bias=True)\n","    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): Linear(in_features=32, out_features=1, bias=True)\n","  )\n",")"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["VA_model = VA_fusion().to(device)\n","VA_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KE3lHnVrxBMl"},"outputs":[],"source":["class MLPModel(nn.Module):\n","    def __init__(self, audio_ft = auft, num_classes=1):\n","        super(MLPModel, self).__init__()\n","        if audio_ft == 'audiofeat_wav2vec2':\n","            self.concat_dim = 2176    #1408+768\n","        elif audio_ft == 'audiofeat_vggish':\n","            self.concat_dim = 1536    #1408+128\n","        elif audio_ft == 'nope':\n","            self.concat_dim = 1408    #visual only\n","        self.activ = nn.ReLU()\n","        self.fc1 = nn.Linear(self.concat_dim, 128)\n","        self.fc2 = nn.Linear(128, num_classes)\n","\n","    def forward(self, vis_feat, aud_feat):\n","        if aud_feat == None:\n","            feat = vis_feat\n","        else:\n","            inputs = [vis_feat]\n","            inputs.append(aud_feat)\n","            feat = torch.cat(inputs, dim=1)\n","        feat = self.fc1(feat)\n","        feat = self.activ(feat)\n","        vout = self.fc2(feat)\n","        aout = self.fc2(feat)\n","\n","        return vout, aout, torch.tanh(vout), torch.tanh(aout)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716111771406,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"gM0fnofbXuz5","outputId":"d36a8100-57c9-4de8-f4dc-238e60799395"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPModel(\n","  (activ): ReLU()\n","  (fc1): Linear(in_features=2176, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":63}],"source":["mlp_model = MLPModel().to(device)\n","mlp_model"]},{"cell_type":"markdown","metadata":{"id":"HtGySUkSxXcn"},"source":["#### Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERu2L4x_NKGK"},"outputs":[],"source":["optimizer = optim.AdamW(filter(lambda p: p.requires_grad, VA_model.parameters()), lr=0.00005, betas=(0.9, 0.999), weight_decay=0.00001)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"doLSNbv2yXN7"},"outputs":[],"source":["optimizer = optim.AdamW(filter(lambda p: p.requires_grad, mlp_model.parameters()), lr=0.00005, betas=(0.9, 0.999), weight_decay=0.00001)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)"]},{"cell_type":"markdown","metadata":{"id":"SVgDHgMqPOti"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMrTqTBANKGK"},"outputs":[],"source":["def train(model, mod_type, train_loader, val_loader, epoch, batch_size, optim, scheduler, au_feat, vis_aud):\n","\n","    model.train(True)\n","    model.eval()\n","    best_loss, best_mse = float('inf'), float('inf')\n","    loss_value = []\n","    loss_train = []\n","    loss_val = []\n","    loss_mse = []\n","    cc1best, cc2best = 0, 0\n","\n","    for e in range(epoch):\n","        print(f'Training Epoch: {e+1}')\n","        torch.manual_seed(2809)\n","        iterator = iter(train_loader)\n","        for i in range(len(train_loader)//32):\n","            try:\n","                VA = next(iterator)\n","                if au_feat == 'nope':\n","                    vis_feat, y = VA[visual_feat], VA['label']\n","                    vis_feat, y = vis_feat.to(device), y.to(device)\n","                    aud_feat = None\n","                else:\n","                    vis_feat, aud_feat, y = VA[visual_feat], VA[au_feat], VA['label']\n","                    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n","                model.zero_grad()\n","                Vpred, Apred, v_pred, a_pred = model(vis_feat, aud_feat)\n","                mse_loss, ccc_loss = compute_VA_loss(Vpred, Apred, y)\n","                ccc_loss.backward()\n","                optim.step()\n","                loss_value.append(ccc_loss.item())\n","                loss_mse.append(mse_loss.item())\n","                preds = torch.cat((v_pred, a_pred), dim=1)\n","            except:\n","                break\n","\n","        avg_loss = round(np.mean(loss_value),3)\n","        loss_train.append(avg_loss)\n","        print(f'Train Loss: {avg_loss}, mse: {round(np.mean(loss_mse),3)}')\n","\n","        val_loss, mse, ccc1, ccc2 = evaluate_model(model, val_loader, au_feat)\n","        loss_val.append(val_loss)\n","\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            #best_mse = mse\n","            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_VA_{mod_type}_{vis_aud}_loss.pth'))\n","            # cc1best = ccc1\n","            # cc2best = ccc2\n","\n","        if mse < best_mse:\n","            best_mse = mse\n","            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_VA_{mod_type}_{vis_aud}_mse.pth'))\n","\n","        if ccc1 > cc1best:\n","            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_VA_{mod_type}_{vis_aud}_c1.pth'))\n","            cc1best = ccc1\n","\n","        if ccc2 > cc2best:\n","            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_VA_{mod_type}_{vis_aud}_c2.pth'))\n","            cc2best = ccc2\n","\n","        print(f'Validation Loss: {val_loss}, mse: {mse}')\n","\n","        scheduler.step(val_loss)\n","    return loss_train, loss_val, best_loss, best_mse, cc1best, cc2best"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJLovdcDNKGK"},"outputs":[],"source":["def evaluate_model(model, data_loader, au_feat):\n","    model.eval()\n","    total_loss = []\n","    all_targets = []\n","    all_preds = []\n","    mse = []\n","    with torch.no_grad():\n","        iterator = iter(data_loader)\n","        for i in range(len(data_loader)//32):\n","            VA = next(iterator)\n","            if au_feat == 'nope':\n","                vis_feat, y = VA[visual_feat], VA['label']\n","                vis_feat, y = vis_feat.to(device), y.to(device)\n","                aud_feat = None\n","            else:\n","                vis_feat, aud_feat, y = VA[visual_feat], VA[au_feat], VA['label']\n","                vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n","            Vpred, Apred, v_pred, a_pred = model(vis_feat, aud_feat)\n","            mse_loss, ccc_loss = compute_VA_loss(Vpred, Apred, y)\n","            total_loss.append(ccc_loss.item())\n","            mse.append(mse_loss.item())\n","            preds = torch.cat((v_pred, a_pred), dim=1)\n","            all_preds.extend(preds.cpu().tolist())\n","            all_targets.extend(y.cpu().tolist())\n","\n","    ccc1, ccc2 = compute_VA_CCC(all_preds, all_targets)\n","    return round(np.mean(total_loss),3), round(np.mean(mse),3), round(ccc1,3), round(ccc2,3)"]},{"cell_type":"markdown","metadata":{"id":"wxfN5hGJjXuJ"},"source":["#### Cropped_aligned images"]},{"cell_type":"markdown","metadata":{"id":"zYUtuFWpp0GA"},"source":["##### Effnet + Wav2vec"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197382,"status":"ok","timestamp":1715719480448,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"cAeX1IDPNKGK","outputId":"4f2b3016-42ea-49e6-e9e9-540446df6ccb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 0.624, mse: 0.491\n","Validation Loss: 1.251, mse: 0.253\n","Training Epoch: 2\n","Train Loss: 0.579, mse: 0.489\n","Validation Loss: 1.265, mse: 0.25\n","Training Epoch: 3\n","Train Loss: 0.556, mse: 0.486\n","Validation Loss: 1.256, mse: 0.252\n","Training Epoch: 4\n","Train Loss: 0.539, mse: 0.486\n","Validation Loss: 1.301, mse: 0.261\n","Training Epoch: 5\n","Train Loss: 0.525, mse: 0.485\n","Validation Loss: 1.33, mse: 0.264\n","Training Epoch: 6\n","Train Loss: 0.514, mse: 0.485\n","Validation Loss: 1.315, mse: 0.249\n","Training Epoch: 7\n","Train Loss: 0.504, mse: 0.484\n","Validation Loss: 1.394, mse: 0.243\n","Training Epoch: 8\n","Train Loss: 0.494, mse: 0.484\n","Validation Loss: 1.259, mse: 0.237\n","Training Epoch: 9\n","Train Loss: 0.487, mse: 0.484\n","Validation Loss: 1.275, mse: 0.222\n","Training Epoch: 10\n","Train Loss: 0.48, mse: 0.483\n","Validation Loss: 1.262, mse: 0.243\n","3min 16s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2001,"status":"ok","timestamp":1715720858943,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"u0ueJmX8NKGL","outputId":"c18705ef-d20d-41c0-89e1-ea9bc1053c4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.352, CCC_Arousal: 0.48\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","    VA_model = VA_fusion().to(device)\n","    VA_model.load_state_dict(VA_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81430,"status":"ok","timestamp":1715720991436,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"rbuTIrd171GR","outputId":"51d0608c-bf9b-4f48-92cd-28e37f09bff5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 1.234, mse: 0.47\n","Validation Loss: 1.221, mse: 0.288\n","Training Epoch: 2\n","Train Loss: 1.179, mse: 0.474\n","Validation Loss: 1.185, mse: 0.274\n","Training Epoch: 3\n","Train Loss: 1.154, mse: 0.474\n","Validation Loss: 1.194, mse: 0.269\n","Training Epoch: 4\n","Train Loss: 1.135, mse: 0.473\n","Validation Loss: 1.175, mse: 0.264\n","Training Epoch: 5\n","Train Loss: 1.121, mse: 0.474\n","Validation Loss: 1.186, mse: 0.268\n","Training Epoch: 6\n","Train Loss: 1.112, mse: 0.475\n","Validation Loss: 1.198, mse: 0.276\n","Training Epoch: 7\n","Train Loss: 1.104, mse: 0.475\n","Validation Loss: 1.193, mse: 0.283\n","Training Epoch: 8\n","Train Loss: 1.096, mse: 0.476\n","Validation Loss: 1.174, mse: 0.264\n","Training Epoch: 9\n","Train Loss: 1.089, mse: 0.476\n","Validation Loss: 1.191, mse: 0.276\n","Training Epoch: 10\n","Train Loss: 1.082, mse: 0.476\n","Validation Loss: 1.181, mse: 0.262\n","1min 20s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1210,"status":"ok","timestamp":1715721393171,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"169Uv6e79hXa","outputId":"3896beea-f4d2-49ed-a2ba-d9ef143acf11"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.501, CCC_Arousal: 0.532\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"u6cSDQxWp4Rj"},"source":["##### Effnet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":190538,"status":"ok","timestamp":1715766072794,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"k9DEokaBXdtM","outputId":"e0de2a06-e40f-4502-97d7-d91d508130be"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 0.81, mse: 0.483\n","Validation Loss: 1.33, mse: 0.34\n","Training Epoch: 2\n","Train Loss: 0.71, mse: 0.484\n","Validation Loss: 1.322, mse: 0.307\n","Training Epoch: 3\n","Train Loss: 0.657, mse: 0.484\n","Validation Loss: 1.336, mse: 0.3\n","Training Epoch: 4\n","Train Loss: 0.617, mse: 0.484\n","Validation Loss: 1.334, mse: 0.293\n","Training Epoch: 5\n","Train Loss: 0.586, mse: 0.483\n","Validation Loss: 1.349, mse: 0.288\n","Training Epoch: 6\n","Train Loss: 0.559, mse: 0.483\n","Validation Loss: 1.355, mse: 0.283\n","Training Epoch: 7\n","Train Loss: 0.535, mse: 0.482\n","Validation Loss: 1.382, mse: 0.282\n","Training Epoch: 8\n","Train Loss: 0.513, mse: 0.482\n","Validation Loss: 1.407, mse: 0.275\n","Training Epoch: 9\n","Train Loss: 0.493, mse: 0.482\n","Validation Loss: 1.414, mse: 0.275\n","Training Epoch: 10\n","Train Loss: 0.475, mse: 0.481\n","Validation Loss: 1.426, mse: 0.272\n","3min 9s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2297,"status":"ok","timestamp":1715766212695,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"AnNup6gCz1PR","outputId":"86118e40-b8dd-4bb8-c356-e7b18cc6cfac"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.241, CCC_Arousal: 0.485\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","    VA_model = VA_fusion().to(device)\n","    VA_model.load_state_dict(VA_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77962,"status":"ok","timestamp":1715758867307,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"bkT7r4OGMV9q","outputId":"f1802095-5f5f-40db-89ac-1e784664a7cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 1.218, mse: 0.461\n","Validation Loss: 1.176, mse: 0.29\n","Training Epoch: 2\n","Train Loss: 1.178, mse: 0.468\n","Validation Loss: 1.183, mse: 0.308\n","Training Epoch: 3\n","Train Loss: 1.157, mse: 0.469\n","Validation Loss: 1.195, mse: 0.315\n","Training Epoch: 4\n","Train Loss: 1.144, mse: 0.469\n","Validation Loss: 1.174, mse: 0.308\n","Training Epoch: 5\n","Train Loss: 1.135, mse: 0.471\n","Validation Loss: 1.216, mse: 0.335\n","Training Epoch: 6\n","Train Loss: 1.126, mse: 0.471\n","Validation Loss: 1.234, mse: 0.341\n","Training Epoch: 7\n","Train Loss: 1.12, mse: 0.472\n","Validation Loss: 1.232, mse: 0.341\n","Training Epoch: 8\n","Train Loss: 1.113, mse: 0.473\n","Validation Loss: 1.212, mse: 0.352\n","Training Epoch: 9\n","Train Loss: 1.107, mse: 0.474\n","Validation Loss: 1.218, mse: 0.352\n","Training Epoch: 10\n","Train Loss: 1.102, mse: 0.475\n","Validation Loss: 1.222, mse: 0.37\n","1min 17s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2681,"status":"ok","timestamp":1715759078221,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"iFO_Den8MY0o","outputId":"6e97551b-72b0-451c-f23a-2d27c86703e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.416, CCC_Arousal: 0.422\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"y7u9hUAvqCkN"},"source":["##### Effnet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":181628,"status":"ok","timestamp":1715759848189,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"6AFlCmXezBv8","outputId":"80e7ed00-8a79-4d9b-ad9e-23722548c6ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 0.84, mse: 0.483\n","Validation Loss: 1.38, mse: 0.282\n","Training Epoch: 2\n","Train Loss: 0.75, mse: 0.486\n","Validation Loss: 1.404, mse: 0.265\n","Training Epoch: 3\n","Train Loss: 0.705, mse: 0.487\n","Validation Loss: 1.369, mse: 0.279\n","Training Epoch: 4\n","Train Loss: 0.673, mse: 0.487\n","Validation Loss: 1.408, mse: 0.293\n","Training Epoch: 5\n","Train Loss: 0.65, mse: 0.486\n","Validation Loss: 1.445, mse: 0.31\n","Training Epoch: 6\n","Train Loss: 0.631, mse: 0.485\n","Validation Loss: 1.462, mse: 0.307\n","Training Epoch: 7\n","Train Loss: 0.615, mse: 0.485\n","Validation Loss: 1.399, mse: 0.326\n","Training Epoch: 8\n","Train Loss: 0.601, mse: 0.485\n","Validation Loss: 1.404, mse: 0.303\n","Training Epoch: 9\n","Train Loss: 0.59, mse: 0.485\n","Validation Loss: 1.417, mse: 0.321\n","Training Epoch: 10\n","Train Loss: 0.579, mse: 0.485\n","Validation Loss: 1.384, mse: 0.293\n","3min 1s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2780,"status":"ok","timestamp":1715760341021,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"S8qfvZ1yz1wS","outputId":"06f3e79b-1ca5-4993-c172-2e77ac004302"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.244, CCC_Arousal: 0.375\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","    VA_model = VA_fusion().to(device)\n","    VA_model.load_state_dict(VA_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70326,"status":"ok","timestamp":1715760201919,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"HpEjTc_6OGoR","outputId":"3958b486-f607-4f5f-abe8-aa941df2fd26"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 1.281, mse: 0.467\n","Validation Loss: 1.254, mse: 0.284\n","Training Epoch: 2\n","Train Loss: 1.236, mse: 0.466\n","Validation Loss: 1.245, mse: 0.299\n","Training Epoch: 3\n","Train Loss: 1.214, mse: 0.468\n","Validation Loss: 1.24, mse: 0.313\n","Training Epoch: 4\n","Train Loss: 1.197, mse: 0.467\n","Validation Loss: 1.236, mse: 0.317\n","Training Epoch: 5\n","Train Loss: 1.184, mse: 0.469\n","Validation Loss: 1.231, mse: 0.303\n","Training Epoch: 6\n","Train Loss: 1.175, mse: 0.469\n","Validation Loss: 1.253, mse: 0.308\n","Training Epoch: 7\n","Train Loss: 1.169, mse: 0.469\n","Validation Loss: 1.249, mse: 0.306\n","Training Epoch: 8\n","Train Loss: 1.161, mse: 0.471\n","Validation Loss: 1.245, mse: 0.324\n","Training Epoch: 9\n","Train Loss: 1.155, mse: 0.472\n","Validation Loss: 1.243, mse: 0.325\n","Training Epoch: 10\n","Train Loss: 1.15, mse: 0.473\n","Validation Loss: 1.254, mse: 0.317\n","1min 9s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":792,"status":"ok","timestamp":1715760344815,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"ONbeewCHOI15","outputId":"0ae6bd15-5496-4bc6-ed33-49fca7252bab"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.422, CCC_Arousal: 0.427\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"daK4JRW_jjjz"},"source":["#### Cropped images"]},{"cell_type":"markdown","metadata":{"id":"NQG-B8Bujjjz"},"source":["##### Effnet + Wav2vec"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":214823,"status":"ok","timestamp":1715761718029,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"lYvHRCrZjjj0","outputId":"43e91ee3-e73e-4559-fc1d-e33a973fa025"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 0.903, mse: 0.504\n","Validation Loss: 1.389, mse: 0.254\n","Training Epoch: 2\n","Train Loss: 0.793, mse: 0.494\n","Validation Loss: 1.377, mse: 0.243\n","Training Epoch: 3\n","Train Loss: 0.74, mse: 0.491\n","Validation Loss: 1.356, mse: 0.244\n","Training Epoch: 4\n","Train Loss: 0.704, mse: 0.49\n","Validation Loss: 1.324, mse: 0.232\n","Training Epoch: 5\n","Train Loss: 0.678, mse: 0.489\n","Validation Loss: 1.32, mse: 0.206\n","Training Epoch: 6\n","Train Loss: 0.657, mse: 0.488\n","Validation Loss: 1.384, mse: 0.236\n","Training Epoch: 7\n","Train Loss: 0.639, mse: 0.488\n","Validation Loss: 1.384, mse: 0.236\n","Training Epoch: 8\n","Train Loss: 0.624, mse: 0.488\n","Validation Loss: 1.329, mse: 0.22\n","Training Epoch: 9\n","Train Loss: 0.611, mse: 0.487\n","Validation Loss: 1.31, mse: 0.253\n","Training Epoch: 10\n","Train Loss: 0.599, mse: 0.487\n","Validation Loss: 1.295, mse: 0.228\n","3min 35s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2777,"status":"ok","timestamp":1715761955385,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"L7IVnP_sjjj0","outputId":"c106a45d-9a5e-4655-aaeb-e8e1e6050786"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.329, CCC_Arousal: 0.39\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","    VA_model = VA_fusion().to(device)\n","    VA_model.load_state_dict(VA_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93937,"status":"ok","timestamp":1715762085551,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"l5zzFa3GUUMz","outputId":"8baa7981-fad1-4a8a-ee5a-c7f45c46bea1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 1.279, mse: 0.464\n","Validation Loss: 1.261, mse: 0.315\n","Training Epoch: 2\n","Train Loss: 1.231, mse: 0.47\n","Validation Loss: 1.242, mse: 0.302\n","Training Epoch: 3\n","Train Loss: 1.209, mse: 0.472\n","Validation Loss: 1.202, mse: 0.277\n","Training Epoch: 4\n","Train Loss: 1.192, mse: 0.473\n","Validation Loss: 1.218, mse: 0.275\n","Training Epoch: 5\n","Train Loss: 1.179, mse: 0.473\n","Validation Loss: 1.178, mse: 0.264\n","Training Epoch: 6\n","Train Loss: 1.166, mse: 0.474\n","Validation Loss: 1.139, mse: 0.254\n","Training Epoch: 7\n","Train Loss: 1.158, mse: 0.475\n","Validation Loss: 1.163, mse: 0.262\n","Training Epoch: 8\n","Train Loss: 1.15, mse: 0.476\n","Validation Loss: 1.147, mse: 0.263\n","Training Epoch: 9\n","Train Loss: 1.144, mse: 0.477\n","Validation Loss: 1.153, mse: 0.265\n","Training Epoch: 10\n","Train Loss: 1.137, mse: 0.477\n","Validation Loss: 1.161, mse: 0.257\n","1min 33s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2880,"status":"ok","timestamp":1715762298156,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"lP3ZYh89UXE1","outputId":"4b41b6d1-f6c0-47f8-c557-8c0ab7155626"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.501, CCC_Arousal: 0.523\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"4TELrToCUaxP"},"source":["##### Effnet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":187592,"status":"ok","timestamp":1715764085797,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"3tIb6S0cUaxP","outputId":"5dffcb03-d2c8-4087-c91b-070ba3a2f526"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 0.904, mse: 0.484\n","Validation Loss: 1.319, mse: 0.281\n","Training Epoch: 2\n","Train Loss: 0.808, mse: 0.483\n","Validation Loss: 1.338, mse: 0.283\n","Training Epoch: 3\n","Train Loss: 0.753, mse: 0.482\n","Validation Loss: 1.357, mse: 0.28\n","Training Epoch: 4\n","Train Loss: 0.712, mse: 0.482\n","Validation Loss: 1.367, mse: 0.275\n","Training Epoch: 5\n","Train Loss: 0.678, mse: 0.481\n","Validation Loss: 1.375, mse: 0.271\n","Training Epoch: 6\n","Train Loss: 0.649, mse: 0.481\n","Validation Loss: 1.383, mse: 0.269\n","Training Epoch: 7\n","Train Loss: 0.622, mse: 0.481\n","Validation Loss: 1.4, mse: 0.269\n","Training Epoch: 8\n","Train Loss: 0.599, mse: 0.48\n","Validation Loss: 1.417, mse: 0.269\n","Training Epoch: 9\n","Train Loss: 0.577, mse: 0.48\n","Validation Loss: 1.434, mse: 0.273\n","Training Epoch: 10\n","Train Loss: 0.556, mse: 0.48\n","Validation Loss: 1.445, mse: 0.272\n","3min 7s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1885,"status":"ok","timestamp":1715764223661,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"kVjp84klUaxP","outputId":"b4cb27b1-7708-4ea8-c817-82ef5684f580"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.215, CCC_Arousal: 0.456\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","    VA_model = VA_fusion().to(device)\n","    VA_model.load_state_dict(VA_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78883,"status":"ok","timestamp":1715764328883,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"eFf8pMDtUaxQ","outputId":"a37c0dd7-7bbb-4778-969b-022a7ff7698a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 1.286, mse: 0.45\n","Validation Loss: 1.298, mse: 0.352\n","Training Epoch: 2\n","Train Loss: 1.241, mse: 0.458\n","Validation Loss: 1.312, mse: 0.349\n","Training Epoch: 3\n","Train Loss: 1.221, mse: 0.462\n","Validation Loss: 1.321, mse: 0.349\n","Training Epoch: 4\n","Train Loss: 1.207, mse: 0.464\n","Validation Loss: 1.328, mse: 0.347\n","Training Epoch: 5\n","Train Loss: 1.197, mse: 0.465\n","Validation Loss: 1.333, mse: 0.346\n","Training Epoch: 6\n","Train Loss: 1.189, mse: 0.466\n","Validation Loss: 1.338, mse: 0.345\n","Training Epoch: 7\n","Train Loss: 1.182, mse: 0.467\n","Validation Loss: 1.343, mse: 0.345\n","Training Epoch: 8\n","Train Loss: 1.176, mse: 0.468\n","Validation Loss: 1.346, mse: 0.345\n","Training Epoch: 9\n","Train Loss: 1.171, mse: 0.468\n","Validation Loss: 1.35, mse: 0.345\n","Training Epoch: 10\n","Train Loss: 1.166, mse: 0.469\n","Validation Loss: 1.354, mse: 0.345\n","1min 18s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1154,"status":"ok","timestamp":1715764457707,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"z3DhVcYTUaxQ","outputId":"23b132f6-c71b-4d93-dc5f-5a9c97604a3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.34, CCC_Arousal: 0.337\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"CrKjh40GUfo8"},"source":["##### Effnet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176313,"status":"ok","timestamp":1715764998424,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"usoBNxvqUfpD","outputId":"e4b1f127-4f2e-4de0-854a-7aeb644d174d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 0.961, mse: 0.488\n","Validation Loss: 1.517, mse: 0.328\n","Training Epoch: 2\n","Train Loss: 0.859, mse: 0.488\n","Validation Loss: 1.522, mse: 0.31\n","Training Epoch: 3\n","Train Loss: 0.805, mse: 0.487\n","Validation Loss: 1.546, mse: 0.298\n","Training Epoch: 4\n","Train Loss: 0.766, mse: 0.487\n","Validation Loss: 1.558, mse: 0.288\n","Training Epoch: 5\n","Train Loss: 0.735, mse: 0.487\n","Validation Loss: 1.572, mse: 0.282\n","Training Epoch: 6\n","Train Loss: 0.708, mse: 0.487\n","Validation Loss: 1.584, mse: 0.276\n","Training Epoch: 7\n","Train Loss: 0.685, mse: 0.487\n","Validation Loss: 1.599, mse: 0.275\n","Training Epoch: 8\n","Train Loss: 0.664, mse: 0.487\n","Validation Loss: 1.604, mse: 0.273\n","Training Epoch: 9\n","Train Loss: 0.644, mse: 0.487\n","Validation Loss: 1.619, mse: 0.273\n","Training Epoch: 10\n","Train Loss: 0.626, mse: 0.486\n","Validation Loss: 1.632, mse: 0.274\n","2min 56s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2840,"status":"ok","timestamp":1715765231475,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"MyDBMb7nUfpE","outputId":"44795669-65a3-4f01-819f-2932f712011d"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.141, CCC_Arousal: 0.324\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","    VA_model = VA_fusion().to(device)\n","    VA_model.load_state_dict(VA_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70238,"status":"ok","timestamp":1715765336284,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"F-972eOPUfpF","outputId":"20c1d000-1a12-4540-f6ac-e8a0e15a8b89"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Epoch: 1\n","Train Loss: 1.357, mse: 0.469\n","Validation Loss: 1.34, mse: 0.352\n","Training Epoch: 2\n","Train Loss: 1.315, mse: 0.473\n","Validation Loss: 1.34, mse: 0.336\n","Training Epoch: 3\n","Train Loss: 1.296, mse: 0.474\n","Validation Loss: 1.338, mse: 0.332\n","Training Epoch: 4\n","Train Loss: 1.283, mse: 0.475\n","Validation Loss: 1.333, mse: 0.329\n","Training Epoch: 5\n","Train Loss: 1.274, mse: 0.476\n","Validation Loss: 1.328, mse: 0.326\n","Training Epoch: 6\n","Train Loss: 1.266, mse: 0.477\n","Validation Loss: 1.323, mse: 0.323\n","Training Epoch: 7\n","Train Loss: 1.26, mse: 0.477\n","Validation Loss: 1.318, mse: 0.32\n","Training Epoch: 8\n","Train Loss: 1.254, mse: 0.478\n","Validation Loss: 1.314, mse: 0.318\n","Training Epoch: 9\n","Train Loss: 1.248, mse: 0.478\n","Validation Loss: 1.311, mse: 0.316\n","Training Epoch: 10\n","Train Loss: 1.244, mse: 0.478\n","Validation Loss: 1.309, mse: 0.314\n","1min 9s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","loss_train, loss_val, best_loss, best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1134,"status":"ok","timestamp":1715765452763,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"EEKYHuAsUfpF","outputId":"f6dfaedf-5937-44ba-c882-31696ea7cab2"},"outputs":[{"name":"stdout","output_type":"stream","text":["best metric: CCC_Valence 0.376, CCC_Arousal: 0.366\n"]}],"source":["try:\n","    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n","except:\n","    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","    mlp_model = MLPModel().to(device)\n","    mlp_model.load_state_dict(mlp_best_model)\n","    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n","    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"ew3FhCIJpw8_"},"source":["### Testing"]},{"cell_type":"markdown","metadata":{"id":"S-X_JD7xjqWi"},"source":["#### Cropped_aligned images"]},{"cell_type":"markdown","metadata":{"id":"46JKsCSt6tN6"},"source":["##### EffNet + Wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3055,"status":"ok","timestamp":1715719889371,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"Smw7BhxO38SB","outputId":"34fa10b0-4588-458f-a901-5691c7408344"},"outputs":[{"name":"stdout","output_type":"stream","text":["visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: CCC_Valence 0.657, CCC_Arousal: 0.465\n","1.85 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('VA_model')\n","print(visual_feat + ' & ' + auft)\n","VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","VA_model = VA_fusion().to(device)\n","VA_model.load_state_dict(VA_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3AhxNg6vk5t","executionInfo":{"status":"ok","timestamp":1716103688594,"user_tz":-180,"elapsed":1646,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"c6b7ebc1-f019-432a-d7ba-6f5a9a0f1e96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: CCC_Valence 0.752, CCC_Arousal: 0.542\n","1.51 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2410,"status":"ok","timestamp":1715721168818,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"p4RGqEaD9l-f","outputId":"be0d2ada-d7c2-41e6-9636-7a29c7699261"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: CCC_Valence 0.752, CCC_Arousal: 0.542\n","843 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"xD1f1Y8uIwkd"},"source":["##### EffNet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2164,"status":"ok","timestamp":1715766074923,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"8aPM0VNamnWQ","outputId":"bee896f5-36f1-47cb-d08d-7f472d19019d"},"outputs":[{"name":"stdout","output_type":"stream","text":["VA_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: CCC_Valence 0.496, CCC_Arousal: 0.36\n","1.77 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('VA_model')\n","print(visual_feat + ' & ' + auft)\n","VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","VA_model = VA_fusion().to(device)\n","VA_model.load_state_dict(VA_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1139,"status":"ok","timestamp":1715758920750,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"o3KDbxHHM1Np","outputId":"c622e390-5476-44c0-9acf-a2d20a3a814b"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: CCC_Valence 0.704, CCC_Arousal: 0.511\n","904 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"Fy_rcZd9QpC_"},"source":["##### EffNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3464,"status":"ok","timestamp":1715759952355,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"_XV5ZqxwQwuM","outputId":"24e1f8db-12fd-48bc-9764-d959987bc38e"},"outputs":[{"name":"stdout","output_type":"stream","text":["VA_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: CCC_Valence 0.514, CCC_Arousal: 0.429\n","1.73 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('VA_model')\n","print(visual_feat + ' & ' + auft)\n","VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","VA_model = VA_fusion().to(device)\n","VA_model.load_state_dict(VA_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1386,"status":"ok","timestamp":1715760389282,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"HXCljqCpSbQj","outputId":"4f6e22a9-bd31-49a4-b6ec-b9bea7874ce8"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: CCC_Valence 0.684, CCC_Arousal: 0.525\n","773 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"U2jn87NejvSJ"},"source":["#### Cropped images"]},{"cell_type":"markdown","metadata":{"id":"slohgnaM7LoB"},"source":["##### EffNet + Wav2vec2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3883,"status":"ok","timestamp":1715762179625,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"9b442pBYjvSP","outputId":"02239908-7465-4773-e655-82bca2db5601"},"outputs":[{"name":"stdout","output_type":"stream","text":["VA_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: CCC_Valence: 0.643, CCC_Arousal: 0.421\n","2.43 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('VA_model')\n","print(visual_feat + ' & ' + auft)\n","VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","VA_model = VA_fusion().to(device)\n","VA_model.load_state_dict(VA_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n","print(f'Test set: CCC_Valence: {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":975,"status":"ok","timestamp":1715762095087,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"6hKOk_ldY2zI","outputId":"795bbfcf-7d5b-4bdf-ddaa-7349a95cc7dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n","Test set: CCC_Valence 0.761, CCC_Arousal: 0.464\n","1.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"awNLFEWmT3C2"},"source":["##### EffNet + Vggish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2205,"status":"ok","timestamp":1715764087988,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"6bPgGGQaT3C3","outputId":"b91085c1-d150-4769-8970-02e133e32de3"},"outputs":[{"name":"stdout","output_type":"stream","text":["VA_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: CCC_Valence 0.509, CCC_Arousal: 0.321\n","1.83 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('VA_model')\n","print(visual_feat + ' & ' + auft)\n","VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","VA_model = VA_fusion().to(device)\n","VA_model.load_state_dict(VA_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1715764332193,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"drZ6gnkshV3h","outputId":"bf009bf4-0082-4268-fc72-14dba26c059f"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & audiofeat_vggish\n","Test set: CCC_Valence 0.699, CCC_Arousal: 0.427\n","779 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"1lC8Qb7UT62p"},"source":["##### EffNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1688,"status":"ok","timestamp":1715765000099,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"k2f1ZNNPT62q","outputId":"fbd339d2-846e-4f7e-e357-e6c3b72240e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["VA_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: CCC_Valence 0.383, CCC_Arousal: 0.301\n","1.68 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('VA_model')\n","print(visual_feat + ' & ' + auft)\n","VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n","VA_model = VA_fusion().to(device)\n","VA_model.load_state_dict(VA_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1715765360038,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"xv6AcHMBlUls","outputId":"f3ae0286-e144-4f84-ab8c-fa423552ccb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP_model\n","visualfeat_enet_b2_8_best & nope\n","Test set: CCC_Valence 0.632, CCC_Arousal: 0.503\n","745 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}],"source":["%%timeit -n 1 -r 1\n","print('MLP_model')\n","print(visual_feat + ' & ' + auft)\n","mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model)\n","test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n","print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"]},{"cell_type":"markdown","metadata":{"id":"pmzvE5czs3hX"},"source":["# Smooth validation prediction (EffNet + Wav2vec2 + MLP)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m9Nyajzm5_ww"},"outputs":[],"source":["def smooth_prediction(img, predict):\n","    cur_ind = 0\n","    preds_proba = []\n","    if img:\n","        for i in range(img[-1]):\n","            if img[cur_ind] - 1 == i:\n","                preds_proba.append(predict[cur_ind])\n","                cur_ind += 1\n","            else:\n","                if cur_ind == 0:\n","                    preds_proba.append(predict[cur_ind])\n","                else:\n","                    w = (i - img[cur_ind - 1] + 1) / (img[cur_ind] - img[cur_ind - 1])\n","                    pred = w * predict[cur_ind - 1] + (1 - w) * predict[cur_ind]\n","                    preds_proba.append(pred)\n","        return np.array([p.cpu().detach().numpy() for p in preds_proba])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8y-D0QuO9ubK"},"outputs":[],"source":["def slide_window(preds_proba, i, delta, typ):\n","    i1 = max(i - delta, 0)\n","    if typ == 'mean':\n","        proba = np.mean(preds_proba[i1:i+delta+1], axis=0)\n","    elif typ == 'median':\n","        proba = np.median(preds_proba[i1:i+delta+1], axis=0)\n","    else:\n","        proba = np.mean(preds_proba[i1:i+delta+1:int(typ)], axis=0)\n","    return np.argmax(proba), proba"]},{"cell_type":"markdown","metadata":{"id":"sd8sIrCrwZIT"},"source":["## EXPR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGEXpsL6IsME"},"outputs":[],"source":["tsk = task[0]\n","vis = vis_typ[0]\n","viau = vis_aud[0]\n","auft = audio_feat[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOXjdlJRYS76"},"outputs":[],"source":["class MLPModel(nn.Module):\n","    def __init__(self, audio_ft = auft, num_classes=8):\n","        super(MLPModel, self).__init__()\n","        if audio_ft == 'audiofeat_wav2vec2':\n","            self.concat_dim = 2176    #1408+768\n","        elif audio_ft == 'audiofeat_vggish':\n","            self.concat_dim = 1536    #1408+128\n","        elif audio_ft == 'nope':\n","            self.concat_dim = 1408    #visual only\n","        self.activ = nn.ReLU()\n","        self.fc1 = nn.Linear(self.concat_dim, 128)\n","        self.fc2 = nn.Linear(128, num_classes)\n","\n","    def forward(self, vis_feat, aud_feat):\n","        inputs = [vis_feat]\n","        inputs.append(aud_feat)\n","        feat = torch.cat(inputs, dim=0)\n","        feat = self.fc1(feat)\n","        feat = self.activ(feat)\n","        out = self.fc2(feat)\n","        return out, torch.softmax(out, dim=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3384,"status":"ok","timestamp":1716057185155,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"rz30ISAQwj-v","outputId":"0c7e378f-d0b0-438c-a8a6-4d29d013dfde"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":25}],"source":["mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34D1o-JyEPeb"},"outputs":[],"source":["anno_path = os.path.join(root,f'data/Annotations/{tsk}/{split[1]}')\n","with open(os.path.join(root, f'data/Annotations/{tsk}/{typ[2]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{tsk}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n","    data = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113687,"status":"ok","timestamp":1716057339664,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"XnxZ2o1TD9mB","outputId":"03c1417f-4e1d-4ced-cf18-cb85d9f97520"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 70/70 [01:53<00:00,  1.62s/it]\n"]}],"source":["test_vid = {}\n","for vname in tqdm(vidnames):\n","        img, predict, label = [], [], []\n","        for imgname, val in sorted(data[tsk][vname].items()):\n","            vis_feat = torch.tensor(val[visual_feat]).to(device)\n","            if auft == 'nope':\n","                aud_feat = None\n","            else:\n","                aud_feat = torch.tensor(val[auft]).to(device)\n","            if tsk == task[2]:\n","                _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor([vpred, apred])\n","            else:\n","                _, pred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor(pred)\n","            ind = int(imgname.split('/')[1][:-4])\n","            img.append(ind)\n","            predict.append(preds)\n","            label.append(data[tsk][vname][imgname]['label'])\n","        test_vid[vname] = (img, predict, label)"]},{"cell_type":"code","source":["hyperparams=[(isMean,delta) for delta in [0, 15, 30, 60, 100, 200] for isMean in [1,0] if not (isMean==0 and delta==0)]\n","total_true=[]\n","total_preds=[[] for _ in range(len(hyperparams))]\n","timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n","for videoname,(img, predict, label) in test_vid.items():\n","    for i,ind in enumerate(img):\n","        total_true.append(label[i].cpu().numpy())\n","    preds_proba = smooth_prediction(img, predict)\n","    for hInd,(isMean,delta) in enumerate(hyperparams):\n","        preds=[]\n","        start = time.time()\n","        for i in range(len(preds_proba)):\n","            i1=max(i-delta,0)\n","            if isMean:\n","                best_ind, proba = slide_window(preds_proba, i, delta, 'mean')\n","            else:\n","                best_ind, proba = slide_window(preds_proba, i, delta, 'median')\n","            preds.append(best_ind)\n","        for i,ind in enumerate(img):\n","            if label[i]>=0:\n","                total_preds[hInd].append(preds[ind-1])\n","        end = time.time()\n","        timing_results[(isMean, delta)] += end - start\n","total_true=np.array(total_true)"],"metadata":{"id":"AWQOiHvVogjP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for hInd, (isMean, delta) in enumerate(hyperparams):\n","    preds = np.array(total_preds[hInd])\n","    accuracy = round((preds == total_true).mean(), 3)\n","    f1 = round(f1_score(y_true=total_true, y_pred=preds, average='macro'), 3)\n","    mean_or_median = 'mean' if isMean else 'median'\n","    time_taken = round(timing_results[(isMean, delta)],3)\n","    print(f'{mean_or_median}; delta: {delta}; Acc: {accuracy}; F1: {f1}; Time: {time_taken}s')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6AqaXWC7-tL3","executionInfo":{"status":"ok","timestamp":1716057538917,"user_tz":-180,"elapsed":1298,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"1d11cfa1-0b6c-4a7a-d3a5-1c2a99dccb6c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mean; delta: 0; Acc: 0.488; F1: 0.394; Time: 9.554s\n","mean; delta: 15; Acc: 0.524; F1: 0.429; Time: 9.97s\n","median; delta: 15; Acc: 0.523; F1: 0.43; Time: 20.216s\n","mean; delta: 30; Acc: 0.533; F1: 0.44; Time: 9.814s\n","median; delta: 30; Acc: 0.533; F1: 0.442; Time: 20.944s\n","mean; delta: 60; Acc: 0.537; F1: 0.443; Time: 9.286s\n","median; delta: 60; Acc: 0.538; F1: 0.449; Time: 22.854s\n","mean; delta: 100; Acc: 0.539; F1: 0.448; Time: 9.671s\n","median; delta: 100; Acc: 0.541; F1: 0.454; Time: 25.641s\n","mean; delta: 200; Acc: 0.538; F1: 0.437; Time: 11.106s\n","median; delta: 200; Acc: 0.544; F1: 0.457; Time: 31.662s\n"]}]},{"cell_type":"markdown","metadata":{"id":"0_2hrWvEYRTB"},"source":["## AU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xm8amJJlYRTC"},"outputs":[],"source":["tsk = task[1]\n","vis = vis_typ[0]\n","viau = vis_aud[0]\n","auft = audio_feat[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-KTrFVuYRTD"},"outputs":[],"source":["class MLPModel(nn.Module):\n","    def __init__(self, audio_ft = auft, num_classes=12):\n","        super(MLPModel, self).__init__()\n","        if audio_ft == 'audiofeat_wav2vec2':\n","            self.concat_dim = 2176    #1408+768\n","        elif audio_ft == 'audiofeat_vggish':\n","            self.concat_dim = 1536    #1408+128\n","        elif audio_ft == 'nope':\n","            self.concat_dim = 1408    #visual only\n","        self.activ = nn.ReLU()\n","        self.fc1 = nn.Linear(self.concat_dim, 128)\n","        self.fc2 = nn.Linear(128, num_classes)\n","\n","    def forward(self, vis_feat, aud_feat):\n","        if aud_feat == None:\n","            feat = vis_feat\n","        else:\n","            inputs = [vis_feat]\n","            inputs.append(aud_feat)\n","            feat = torch.cat(inputs, dim=0)\n","        feat = self.fc1(feat)\n","        feat = self.activ(feat)\n","        out = self.fc2(feat)\n","        return out, torch.sigmoid(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716129195483,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"zo2EVwxwYRTD","outputId":"f0ee2b6d-7920-4c72-ddc9-7d14f57bb941"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":25}],"source":["mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzwwmlgfYRTE"},"outputs":[],"source":["anno_path = os.path.join(root,f'data/Annotations/{tsk}/{split[1]}')\n","with open(os.path.join(root, f'data/Annotations/{tsk}/{typ[2]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","with open(os.path.join(root,f'models/ABAW6/{vis}/AU/{tsk}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n","    data = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76s7Sk_dYRTE","outputId":"67c2513e-cf42-4c83-e1a3-10b9bb050069","executionInfo":{"status":"ok","timestamp":1716129475908,"user_tz":-180,"elapsed":226365,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 105/105 [03:46<00:00,  2.16s/it]\n"]}],"source":["test_vid = {}\n","for vname in tqdm(vidnames):\n","        img, predict, label = [], [], []\n","        for imgname, val in sorted(data[tsk][vname].items()):\n","            vis_feat = torch.tensor(val[visual_feat]).to(device)\n","            if auft == 'nope':\n","                aud_feat = None\n","            else:\n","                aud_feat = torch.tensor(val[auft]).to(device)\n","            if tsk == task[2]:\n","                _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor([vpred, apred])\n","            else:\n","                _, pred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor(pred)\n","            ind = int(imgname.split('/')[1][:-4])\n","            img.append(ind)\n","            predict.append(preds)\n","            label.append(data[tsk][vname][imgname]['label'])\n","        test_vid[vname] = (img, predict, label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8ZjiiVbHzuK"},"outputs":[],"source":["thresholds = np.array([0.30000000000000004, 0.2, 0.2, 0.30000000000000004, 0.5, 0.30000000000000004, 0.6, 0.1, 0.1, 0.1, 0.6, 0.30000000000000004])\n","#np.array([0.2, 0.2, 0.2, 0.30000000000000004, 0.6, 0.30000000000000004, 0.6, 0.1, 0.1, 0.1, 0.6, 0.2])\n","#np.array([0.2, 0.2, 0.2, 0.4, 0.5, 0.4, 0.6, 0.1, 0.1, 0.1, 0.6, 0.2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fj6YmjcFEbZA"},"outputs":[],"source":["hyperparams=[(isMean,delta) for delta in [0, 15, 30, 60, 100, 200] for isMean in [1,0] if not (isMean==0 and delta==0)]\n","total_true=[]\n","total_preds=[[] for _ in range(len(hyperparams))]\n","timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n","for videoname,(img, predict, label) in test_vid.items():\n","    for i,ind in enumerate(img):\n","        total_true.append(label[i].cpu().numpy())\n","    preds_proba = smooth_prediction(img, predict)\n","    for hInd,(isMean,delta) in enumerate(hyperparams):\n","        preds=[]\n","        start = time.time()\n","        for i in range(len(preds_proba)):\n","            i1=max(i-delta,0)\n","            if isMean:\n","                _, proba = slide_window(preds_proba, i, delta, 'mean')\n","            else:\n","                _, proba = slide_window(preds_proba, i, delta, 'median')\n","            aus = (proba>=thresholds)*1\n","            preds.append(aus)\n","        for i,ind in enumerate(img):\n","            if label[i][0]>=-1 and label[i][1]>=-1:\n","                total_preds[hInd].append(preds[ind-1])\n","        end = time.time()\n","        timing_results[(isMean, delta)] += end - start\n","total_true=np.array(total_true)"]},{"cell_type":"code","source":["for hInd, (isMean, delta) in enumerate(hyperparams):\n","    preds = np.array(total_preds[hInd])\n","    accuracy = round((preds == total_true).mean(), 3)\n","    f1 = round(np.mean([f1_score(y_true=total_true[:,i],y_pred=preds[:,i]) for i in range(preds.shape[1])]), 3)\n","    mean_or_median = 'mean' if isMean else 'median'\n","    time_taken = round(timing_results[(isMean, delta)],3)\n","    print(f'{mean_or_median}; delta: {delta}; Acc: {accuracy}; F1: {f1}; Time: {time_taken}s')"],"metadata":{"id":"rKawFEtvM775","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716129802014,"user_tz":-180,"elapsed":23065,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"f2a26a18-4332-4aaf-ab75-6822e3ba90aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mean; delta: 0; Acc: 0.855; F1: 0.487; Time: 15.418s\n","mean; delta: 15; Acc: 0.86; F1: 0.496; Time: 15.937s\n","median; delta: 15; Acc: 0.863; F1: 0.495; Time: 29.683s\n","mean; delta: 30; Acc: 0.858; F1: 0.491; Time: 16.427s\n","median; delta: 30; Acc: 0.863; F1: 0.491; Time: 33.438s\n","mean; delta: 60; Acc: 0.854; F1: 0.478; Time: 16.618s\n","median; delta: 60; Acc: 0.862; F1: 0.477; Time: 35.013s\n","mean; delta: 100; Acc: 0.85; F1: 0.465; Time: 17.563s\n","median; delta: 100; Acc: 0.859; F1: 0.459; Time: 39.341s\n","mean; delta: 200; Acc: 0.842; F1: 0.438; Time: 19.513s\n","median; delta: 200; Acc: 0.852; F1: 0.429; Time: 50.336s\n"]}]},{"cell_type":"markdown","metadata":{"id":"KmE8Fo_YSyzW"},"source":["## VA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7bOkc2A5S36T"},"outputs":[],"source":["tsk = task[2]\n","vis = vis_typ[0]\n","viau = vis_aud[0]\n","auft = audio_feat[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N62Dut3yS36U"},"outputs":[],"source":["class MLPModel(nn.Module):\n","    def __init__(self, audio_ft = auft, num_classes=1):\n","        super(MLPModel, self).__init__()\n","        if audio_ft == 'audiofeat_wav2vec2':\n","            self.concat_dim = 2176    #1408+768\n","        elif audio_ft == 'audiofeat_vggish':\n","            self.concat_dim = 1536    #1408+128\n","        elif audio_ft == 'nope':\n","            self.concat_dim = 1408    #visual only\n","        self.activ = nn.ReLU()\n","        self.fc1 = nn.Linear(self.concat_dim, 128)\n","        self.fc2 = nn.Linear(128, num_classes)\n","\n","    def forward(self, vis_feat, aud_feat):\n","        if aud_feat == None:\n","            feat = vis_feat\n","        else:\n","            inputs = [vis_feat]\n","            inputs.append(aud_feat)\n","            feat = torch.cat(inputs, dim=1)\n","        feat = self.fc1(feat)\n","        feat = self.activ(feat)\n","        vout = self.fc2(feat)\n","        aout = self.fc2(feat)\n","\n","        return vout, aout, torch.tanh(vout), torch.tanh(aout)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":695,"status":"ok","timestamp":1716119556267,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"aPIQMmCGS36U","outputId":"bc0419ad-b1e4-4ddd-e3eb-ef596fae9531"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":57}],"source":["mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n","mlp_model = MLPModel().to(device)\n","mlp_model.load_state_dict(mlp_best_model)"]},{"cell_type":"code","source":["img, predict, label = [], [], []\n","vid = {}\n","iterator = iter(test_loader)\n","for i in range(len(test_loader)//32):\n","    VA = next(iterator)\n","    images = VA['frame']\n","    vis_feat, aud_feat, y = VA[visual_feat], VA[auft], VA['label']\n","    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n","    _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n","    preds = torch.cat((vpred, apred), dim=1)\n","    img.extend(images)\n","    predict.extend(preds)\n","    label.extend(y)"],"metadata":{"id":"OycdBS1N38ii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index, pred, lab = [], [], []\n","test_vid = {}\n","for i, val in enumerate(img):\n","    ind = int(val.split('/')[1][:-4])\n","    vname = val.split('/')[0]\n","    if i == 0:\n","        prename = vname\n","        index.append(ind)\n","        pred.append(predict[i])\n","        lab.append(label[i])\n","    else:\n","        if vname == prename:\n","            index.append(ind)\n","            pred.append(predict[i])\n","            lab.append(label[i])\n","        else:\n","            combined = list(zip(index, pred, lab))\n","            combined_sorted = sorted(combined, key=lambda x: x[0])\n","            index_list_sorted, pred_list_sorted, lab_list_sorted = zip(*combined_sorted)\n","            test_vid[prename] = (list(index_list_sorted), list(pred_list_sorted), list(lab_list_sorted))\n","            prename = vname\n","            index, pred, lab = [], [], []\n","            index.append(ind)\n","            pred.append(predict[i])\n","            lab.append(label[i])"],"metadata":{"id":"fIkPBo71D0Lm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QwJa1N2S36V"},"outputs":[],"source":["anno_path = os.path.join(root,f'data/Annotations/{tsk}/{split[1]}')\n","with open(os.path.join(root, f'data/Annotations/{tsk}/{typ[2]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{tsk}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n","    data = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hI9qPRmqS36W","outputId":"77062ade-3157-4ee3-f2fb-9bd5e165bb3a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716103993778,"user_tz":-180,"elapsed":218253,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 76/76 [03:38<00:00,  2.87s/it]\n"]}],"source":["test_vid = {}\n","for vname in tqdm(vidnames):\n","        img, predict, label = [], [], []\n","        for imgname, val in sorted(data[tsk][vname].items()):\n","            vis_feat = torch.tensor(val[visual_feat]).to(device)\n","            if auft == 'nope':\n","                aud_feat = None\n","            else:\n","                aud_feat = torch.tensor(val[auft]).to(device)\n","            if tsk == task[2]:\n","                _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor([vpred, apred])\n","            else:\n","                _, pred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor(pred)\n","            ind = int(imgname.split('/')[1][:-4])\n","            img.append(ind)\n","            predict.append(preds)\n","            label.append(data[tsk][vname][imgname]['label'])\n","        test_vid[vname] = (img, predict, label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69oAXZnpax2r"},"outputs":[],"source":["hyperparams=[(isMean,delta) for delta in [0, 15, 30, 60, 100, 200] for isMean in [1,0] if not (isMean==0 and delta==0)]\n","total_true=[]\n","total_preds=[[] for _ in range(len(hyperparams))]\n","timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n","for videoname,(img, predict, label) in test_vid.items():\n","    for i,ind in enumerate(img):\n","        total_true.append(label[i].cpu().numpy())\n","    preds_proba = smooth_prediction(img, predict)\n","    for hInd,(isMean,delta) in enumerate(hyperparams):\n","        preds=[]\n","        start = time.time()\n","        for i in range(len(preds_proba)):\n","            i1=max(i-delta,0)\n","            if isMean:\n","                best_ind, proba = slide_window(preds_proba, i, delta, 'mean')\n","            else:\n","                best_ind, proba = slide_window(preds_proba, i, delta, 'median')\n","            preds.append(proba)\n","        for i, ind in enumerate(img):\n","            if label[i][0]>=-1 and label[i][1]>=-1:\n","                total_preds[hInd].append(preds[ind-1])\n","        end = time.time()\n","        timing_results[(isMean, delta)] += end - start\n","total_true=np.array(total_true)"]},{"cell_type":"code","source":["for hInd, (isMean, delta) in enumerate(hyperparams):\n","    preds = np.array(total_preds[hInd])\n","    ccc1, ccc2 = compute_VA_CCC(preds, total_true)\n","    mean_or_median = 'mean' if isMean else 'median'\n","    time_taken = round(timing_results[(isMean, delta)],3)\n","    print(f'{mean_or_median}; delta: {delta}; CCCV: {ccc1:.3f}; CCCA: {ccc2:.3f}; Time: {time_taken}s')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9UZONFvO0_T","executionInfo":{"status":"ok","timestamp":1716111791731,"user_tz":-180,"elapsed":16,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"f32268b7-9982-4a94-e712-a0775d5fae17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mean; delta: 0; CCCV: 0.755; CCCA: 0.538; Time: 2.09s\n","mean; delta: 15; CCCV: 0.781; CCCA: 0.574; Time: 1.054s\n","median; delta: 15; CCCV: 0.780; CCCA: 0.566; Time: 1.343s\n","mean; delta: 30; CCCV: 0.788; CCCA: 0.582; Time: 1.022s\n","median; delta: 30; CCCV: 0.790; CCCA: 0.573; Time: 1.333s\n","mean; delta: 60; CCCV: 0.789; CCCA: 0.576; Time: 1.084s\n","median; delta: 60; CCCV: 0.793; CCCA: 0.569; Time: 1.389s\n","mean; delta: 100; CCCV: 0.783; CCCA: 0.559; Time: 1.034s\n","median; delta: 100; CCCV: 0.788; CCCA: 0.556; Time: 1.374s\n","mean; delta: 200; CCCV: 0.770; CCCA: 0.519; Time: 1.141s\n","median; delta: 200; CCCV: 0.771; CCCA: 0.499; Time: 1.45s\n"]}]},{"cell_type":"markdown","metadata":{"id":"7twP-V0AHU5w"},"source":["# Adaptive Frame Rate (EffNet + Wav2vec2 + MLP)"]},{"cell_type":"markdown","metadata":{"id":"RsUw4AxuwtBo"},"source":["### EXPR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67stcFBHvCJh"},"outputs":[],"source":["delta = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":303689,"status":"ok","timestamp":1715936783706,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"TG-oVHWuHpFS","outputId":"1238e35b-fe29-4265-c212-058b4fba4931"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 199/199 [03:59<00:00,  1.20s/it]\n"]}],"source":["anno_path = os.path.join(root,f'data/Annotations/{tsk}/{split[0]}')\n","with open(os.path.join(root, f'data/Annotations/{tsk}/{typ[0]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{tsk}_{typ[0]}_{viau}.pkl'), 'rb') as f:\n","    data = pickle.load(f)\n","train_vid = {}\n","for vname in tqdm(vidnames):\n","        img, predict, label = [], [], []\n","        for imgname, val in sorted(data[tsk][vname].items()):\n","            vis_feat = torch.tensor(val[visual_feat]).to(device)\n","            if auft == 'nope':\n","                aud_feat = None\n","            else:\n","                aud_feat = torch.tensor(val[auft]).to(device)\n","            if tsk == task[2]:\n","                _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor([vpred, apred])\n","            else:\n","                _, pred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor(pred)\n","            ind = int(imgname.split('/')[1][:-4])\n","            img.append(ind)\n","            predict.append(preds)\n","            label.append(data[tsk][vname][imgname]['label'])\n","        train_vid[vname] = (img, predict, label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ayc_zt0GHf-V"},"outputs":[],"source":["stride2scores={}\n","for stride in [200, 100, 50, 25, 10]:\n","    total_true, predictions, max_decision_values = [],[],[]\n","    for vidname, (img, predict, label) in train_vid.items():\n","        index = []\n","        for i,ind in enumerate(img):\n","            total_true.append(label[i].cpu().numpy())\n","            index.append(ind-1)\n","        preds_proba = smooth_prediction(img, predict)\n","        for i in range(len(index)):\n","            best_ind, proba = slide_window(preds_proba, index[i], delta, stride)\n","            predictions.append(best_ind)\n","            max_decision_values.append(proba[best_ind])\n","    stride2scores[stride] = (np.array(total_true),np.array(predictions),np.array(max_decision_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvZ1nt6GGzaM"},"outputs":[],"source":["def get_threshold(stride,fpr_corrected):\n","    (total_true,predictions,max_decision_values) = stride2scores[stride]\n","    mistakes = max_decision_values[predictions != total_true]\n","    best_threshold = -1\n","    for i, threshold in enumerate(sorted(max_decision_values[predictions == total_true])[::-1]):\n","        tpr = i/len(predictions)\n","        fpr = (mistakes > threshold).sum()/len(predictions)\n","\n","        if fpr > fpr_corrected:\n","            if best_threshold == -1:\n","                best_threshold = threshold\n","            print(stride, 'best_threshold', best_threshold, i)\n","            break\n","        best_threshold = threshold\n","    return best_threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118925,"status":"ok","timestamp":1715860024143,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"m5HADD-QRJPj","outputId":"9eeb89c3-8569-4df8-d143-e76412358bba"},"outputs":[{"name":"stdout","output_type":"stream","text":["200 best_threshold 0.5828423 220303\n","100 best_threshold 0.54001206 253009\n","50 best_threshold 0.52979404 261165\n","25 best_threshold 0.523032 267088\n","10 best_threshold 0.51859856 271299\n","{200: 0.5828423, 100: 0.54001206, 50: 0.52979404, 25: 0.523032, 10: 0.51859856, 1: 0}\n"]}],"source":["stride2threshold = {}\n","for stride in stride2scores:\n","    fpr_corrected=0.05\n","    stride2threshold[stride] = get_threshold(stride,fpr_corrected)\n","stride2threshold[1] = 0\n","print(stride2threshold)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":392193,"status":"ok","timestamp":1715937532361,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"MUfOZyGRyfy9","outputId":"70e3f2b1-964e-455c-8066-dcd8e459cbb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["[200, 100, 50, 10, 1]\n","Acc: 0.532 F1: 0.443\n","28654087 55522377 0.516\n","Time: 17.52 seconds\n","[50, 25, 1]\n","Acc: 0.539 F1: 0.448\n","35626909 55522377 0.642\n","Time: 13.31 seconds\n","[50, 10, 1]\n","Acc: 0.539 F1: 0.448\n","35270322 55522377 0.635\n","Time: 13.57 seconds\n","[200, 50, 1]\n","Acc: 0.532 F1: 0.443\n","31363350 55522377 0.565\n","Time: 13.05 seconds\n","[100, 50, 1]\n","Acc: 0.538 F1: 0.447\n","33947035 55522377 0.611\n","Time: 13.10 seconds\n","[200, 1]\n","Acc: 0.532 F1: 0.443\n","35393313 55522377 0.637\n","Time: 9.46 seconds\n","[100, 1]\n","Acc: 0.538 F1: 0.447\n","36501721 55522377 0.657\n","Time: 12.46 seconds\n","[50, 1]\n","Acc: 0.539 F1: 0.448\n","37616958 55522377 0.678\n","Time: 9.93 seconds\n","[200]\n","Acc: 0.472 F1: 0.379\n","547752 55522377 0.01\n","Time: 5.42 seconds\n","[100]\n","Acc: 0.51 F1: 0.421\n","826609 55522377 0.015\n","Time: 5.61 seconds\n","[50]\n","Acc: 0.523 F1: 0.434\n","1378986 55522377 0.025\n","Time: 5.59 seconds\n","[25]\n","Acc: 0.532 F1: 0.441\n","2483889 55522377 0.045\n","Time: 5.56 seconds\n","[10]\n","Acc: 0.537 F1: 0.446\n","5798729 55522377 0.104\n","Time: 5.79 seconds\n","[1]\n","Acc: 0.539 F1: 0.448\n","55522377 55522377 1.0\n","Time: 6.85 seconds\n"]}],"source":["all_strides=[\n","    [200, 100, 50, 10, 1],\n","    [50, 25, 1],\n","    [50, 10, 1],\n","    [200,50,1],\n","    [100,50,1],\n","    [200,1],\n","    [100,1],\n","    [50,1]\n","]\n","for s in stride2threshold.keys():\n","    all_strides.append([s])\n","\n","for strides in all_strides:\n","    print(strides)\n","    last_stride=strides[-1]\n","\n","    total_true=[]\n","    total_preds=[]\n","    total_frames_processed,total_frames=0,0\n","    time_each = []\n","    start = time.time()\n","    for videoname, (img, predict, label) in test_vid.items():\n","        emotional_img=[]\n","        start1 = time.time()\n","        for i,ind in enumerate(img):\n","            total_true.append(label[i].cpu().numpy())\n","            emotional_img.append(ind-1)\n","        cur_ind=0\n","        preds_proba=[]\n","        for i in range(img[-1]):\n","            if img[cur_ind]-1==i:\n","                preds_proba.append(predict[cur_ind])\n","                cur_ind+=1\n","            else:\n","                if cur_ind==0:\n","                    preds_proba.append(predict[cur_ind])\n","                else:\n","                    w=(i-img[cur_ind-1]+1)/(img[cur_ind]-img[cur_ind-1])\n","                    pred=w*predict[cur_ind-1]+(1-w)*predict[cur_ind]\n","                    preds_proba.append(pred)\n","\n","        preds_proba=np.array([p.cpu().numpy() for p in preds_proba])\n","\n","        preds=-np.ones(len(emotional_img))\n","        end1 = time.time()\n","        time_each.append(end1 - start1)\n","        for stride in strides:\n","            threshold=stride2threshold[stride]\n","            for i in range(len(emotional_img)):\n","                if preds[i]<0:\n","                    i1=max(emotional_img[i]-delta,0)\n","                    cur_preds=preds_proba[i1:emotional_img[i]+delta+1:stride]\n","                    proba=np.mean(cur_preds,axis=0)\n","                    best_ind=np.argmax(proba)\n","                    if proba[best_ind]>=threshold or stride==last_stride:\n","                        total_frames_processed+=len(cur_preds)\n","                        total_frames+=len(preds_proba[i1:emotional_img[i]+delta+1])\n","                        preds[i]=best_ind\n","        for p in preds:\n","            total_preds.append(p)\n","    end = time.time()\n","    elapsed_time = end - start - sum(time_each)\n","    total_true=np.array(total_true)\n","    preds=np.array(total_preds)\n","    print('Acc:',round((preds==total_true).mean(),3), 'F1:',round(f1_score(y_true=total_true,y_pred=preds, average=\"macro\"),3))\n","    print(total_frames_processed,total_frames,round(total_frames_processed/total_frames,3))\n","    print(f\"Time: {elapsed_time:.2f} seconds\")"]},{"cell_type":"markdown","metadata":{"id":"TYIzmyKxYxo2"},"source":["### AU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YafCCaGnYRTF"},"outputs":[],"source":["delta = 15"]},{"cell_type":"code","source":["anno_path = os.path.join(root,f'data/Annotations/{tsk}/{split[0]}')\n","with open(os.path.join(root, f'data/Annotations/{tsk}/{typ[0]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","with open(os.path.join(root,f'models/ABAW6/{vis}/AU/{tsk}_{typ[0]}_visual.pkl'), 'rb') as f:\n","        data = pickle.load(f)\n","\n","task1 = task[1]\n","feature_a = 'audiofeat_wav2vec2'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","    feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","    for imgname, val in feature.items():\n","        if imgname in data[task1][vname]:\n","            data[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data[task1][vname].items()):\n","            if len(value) < 3:\n","                data[task1][vname].pop(img)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfuXLk4dgZLK","executionInfo":{"status":"ok","timestamp":1716130102774,"user_tz":-180,"elapsed":248175,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"b2f4ece8-3345-4f75-95c9-c10e8d4670de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 236/236 [01:50<00:00,  2.14it/s]\n"]}]},{"cell_type":"code","source":["train_vid = {}\n","for vname in tqdm(vidnames):\n","        img, predict, label = [], [], []\n","        for imgname, val in sorted(data[tsk][vname].items()):\n","            vis_feat = torch.tensor(val[visual_feat]).to(device)\n","            if auft == 'nope':\n","                aud_feat = None\n","            else:\n","                aud_feat = torch.tensor(val[auft]).to(device)\n","            if tsk == task[2]:\n","                _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor([vpred, apred])\n","            else:\n","                _, pred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor(pred)\n","            ind = int(imgname.split('/')[1][:-4])\n","            img.append(ind)\n","            predict.append(preds)\n","            label.append(data[tsk][vname][imgname]['label'])\n","        train_vid[vname] = (img, predict, label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d07KzjfNgiza","executionInfo":{"status":"ok","timestamp":1716130102774,"user_tz":-180,"elapsed":7,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"cf688525-dcb7-4656-885e-1f238f8a865c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 236/236 [00:00<00:00, 2371.61it/s]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwV6Kx3BS5IK"},"outputs":[],"source":["thresholds = np.array([0.30000000000000004, 0.2, 0.2, 0.30000000000000004, 0.5, 0.30000000000000004, 0.6, 0.1, 0.1, 0.1, 0.6, 0.30000000000000004])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGhqaGdgbT0L"},"outputs":[],"source":["stride2scores={}\n","for stride in [200, 100, 50, 25, 10]:\n","    total_true, predictions, max_decision_values = [],[],[]\n","    decision_values = [[] for _ in range(8)]\n","    for vidname, (img, predict, label) in train_vid.items():\n","        index = []\n","        for i,ind in enumerate(img):\n","            total_true.append(label[i].cpu().numpy())\n","            index.append(ind-1)\n","        preds_proba = smooth_prediction(img, predict)\n","        for i in range(len(index)):\n","            best_ind, proba = slide_window(preds_proba, index[i], delta, stride)\n","            aus = (proba>=thresholds)*1\n","            predictions.append(aus)\n","            for j in range(8):\n","                if proba[j] > thresholds[j]:\n","                    decision_values[j].append(proba[j])\n","                else:\n","                    decision_values[j].append(thresholds[j])\n","            max_decision_values.append([max(values) for values in decision_values])\n","    stride2scores[stride] = (np.array(total_true),np.array(predictions),np.array(max_decision_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Jplf72JbT0M"},"outputs":[],"source":["def get_threshold(stride,fpr_corrected):\n","    (total_true,predictions,max_decision_values) = stride2scores[stride]\n","    mistakes = max_decision_values[predictions.any() != total_true.any()]\n","    best_threshold = -1\n","    for i, threshold in enumerate(sorted(max_decision_values[predictions.all() == total_true.all()])[::-1]):\n","        fpr = (mistakes > threshold).sum()/len(predictions)\n","        if fpr > fpr_corrected:\n","            if best_threshold == -1:\n","                best_threshold = threshold\n","            print(stride, 'best_threshold', best_threshold, i)\n","            break\n","        best_threshold = threshold\n","    return best_threshold"]},{"cell_type":"code","source":["stride2threshold = {}\n","for stride in stride2scores:\n","    fpr_corrected=0.05\n","    stride2threshold[stride] = get_threshold(stride,fpr_corrected)\n","stride2threshold[1] = 0\n","print(stride2threshold)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-DzcMSWadHUO","executionInfo":{"status":"ok","timestamp":1716132334238,"user_tz":-180,"elapsed":4,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"3eb71779-e38c-4f24-dff7-a15bb748a67b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{200: array([[0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       [0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       [0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       ...,\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458],\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458],\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458]]), 100: array([[0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       [0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       [0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       ...,\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458],\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458],\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458]]), 50: array([[0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       [0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       [0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       ...,\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458],\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458],\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458]]), 25: array([[0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       [0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       [0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       ...,\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458],\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458],\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458]]), 10: array([[0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       [0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       [0.3       , 0.2       , 0.2       , ..., 0.3       , 0.6       ,\n","        0.1       ],\n","       ...,\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458],\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458],\n","       [0.88672322, 0.70943815, 0.91607934, ..., 0.99227798, 0.99390829,\n","        0.25974458]]), 1: 0}\n"]}]},{"cell_type":"code","source":["all_strides=[\n","    [200, 100, 50, 10, 1],\n","    [50, 25, 1],\n","    [50, 10, 1],\n","    [200,50,1],\n","    [100,50,1],\n","    [200,1],\n","    [100,1],\n","    [50,1]\n","]\n","for s in stride2threshold.keys():\n","    all_strides.append([s])\n","\n","for strides in all_strides:\n","    start = time.time()\n","    print(strides)\n","    last_stride=strides[-1]\n","\n","    total_true=[]\n","    total_preds=[]\n","    total_frames_processed,total_frames=0,0\n","    time_each = []\n","    start = time.time()\n","    for videoname, (img, predict, label) in test_vid.items():\n","        emotional_img=[]\n","        start1 = time.time()\n","        for i,ind in enumerate(img):\n","            total_true.append(label[i].cpu().numpy())\n","            emotional_img.append(ind-1)\n","        cur_ind=0\n","        preds_proba=[]\n","        for i in range(img[-1]):\n","            if img[cur_ind]-1==i:\n","                preds_proba.append(predict[cur_ind])\n","                cur_ind+=1\n","            else:\n","                if cur_ind==0:\n","                    preds_proba.append(predict[cur_ind])\n","                else:\n","                    w=(i-img[cur_ind-1]+1)/(img[cur_ind]-img[cur_ind-1])\n","                    pred=w*predict[cur_ind-1]+(1-w)*predict[cur_ind]\n","                    preds_proba.append(pred)\n","\n","        preds_proba=np.array([p.cpu().numpy() for p in preds_proba])\n","        preds=[[-1]*12 for _ in range(len(emotional_img))]\n","        end1 = time.time()\n","        time_each.append(end1-start1)\n","        for stride in strides:\n","            threshold=stride2threshold[stride]\n","            for i in range(len(emotional_img)):\n","                if max(preds[i])<0:\n","                    i1=max(emotional_img[i]-delta,0)\n","                    cur_preds=preds_proba[i1:emotional_img[i]+delta+1:stride]\n","                    proba=np.mean(cur_preds,axis=0)\n","                    aus=(proba>=thresholds)*1\n","                    if stride != 1:\n","                        if proba.all()>=threshold.all() or stride==last_stride:\n","                            total_frames_processed+=len(cur_preds)\n","                            total_frames+=len(preds_proba[i1:emotional_img[i]+delta+1])\n","                            preds[i] = aus\n","                    else:\n","                        if proba.all()>=threshold or stride==last_stride:\n","                            total_frames_processed+=len(cur_preds)\n","                            total_frames+=len(preds_proba[i1:emotional_img[i]+delta+1])\n","                            preds[i] = aus\n","        for p in preds:\n","            total_preds.append(p)\n","    end = time.time()\n","    elapsed_time = end - start - sum(time_each)\n","    total_true=np.array(total_true)\n","    pred=np.array(total_preds)\n","    print('Acc:',round((pred==total_true).mean(),3), 'F1:',round(np.mean([f1_score(y_true=total_true[:,i],y_pred=pred[:,i]) for i in range(pred.shape[1])]),3))\n","    print(total_frames_processed,total_frames,round(total_frames_processed/total_frames,3))\n","    print(f\"Time: {elapsed_time:.2f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMwuwG3QU5mm","executionInfo":{"status":"ok","timestamp":1716130567789,"user_tz":-180,"elapsed":435695,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"5c32e65b-6535-4783-8692-b5887041fc77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[200, 100, 50, 10, 1]\n","Acc: 0.84 F1: 0.461\n","445842 13797139 0.032\n","Time: 14.08 seconds\n","[50, 25, 1]\n","Acc: 0.84 F1: 0.461\n","445842 13797139 0.032\n","Time: 12.03 seconds\n","[50, 10, 1]\n","Acc: 0.84 F1: 0.461\n","445842 13797139 0.032\n","Time: 12.65 seconds\n","[200, 50, 1]\n","Acc: 0.84 F1: 0.461\n","445842 13797139 0.032\n","Time: 12.12 seconds\n","[100, 50, 1]\n","Acc: 0.84 F1: 0.461\n","445842 13797139 0.032\n","Time: 12.02 seconds\n","[200, 1]\n","Acc: 0.84 F1: 0.461\n","445842 13797139 0.032\n","Time: 11.22 seconds\n","[100, 1]\n","Acc: 0.84 F1: 0.461\n","445842 13797139 0.032\n","Time: 11.00 seconds\n","[50, 1]\n","Acc: 0.84 F1: 0.461\n","445842 13797139 0.032\n","Time: 11.44 seconds\n","[200]\n","Acc: 0.84 F1: 0.461\n","445842 13797139 0.032\n","Time: 10.24 seconds\n","[100]\n","Acc: 0.84 F1: 0.461\n","445842 13797139 0.032\n","Time: 10.22 seconds\n","[50]\n","Acc: 0.84 F1: 0.461\n","445842 13797139 0.032\n","Time: 10.23 seconds\n","[25]\n","Acc: 0.849 F1: 0.479\n","889687 13797139 0.064\n","Time: 10.24 seconds\n","[10]\n","Acc: 0.857 F1: 0.492\n","1779368 13797139 0.129\n","Time: 10.48 seconds\n","[1]\n","Acc: 0.86 F1: 0.496\n","13797139 13797139 1.0\n","Time: 10.59 seconds\n"]}]},{"cell_type":"markdown","metadata":{"id":"2vcOLk4UdsWr"},"source":["## VA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":181470,"status":"ok","timestamp":1716119815388,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"RzNOr3VueqYY","outputId":"2395b4bb-10ee-4304-8f46-2eb0da6d505d"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:42<00:00,  6.70it/s]\n"]}],"source":["anno_path = os.path.join(root,f'data/Annotations/{tsk}/{split[0]}')\n","with open(os.path.join(root, f'data/Annotations/{tsk}/{typ[0]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{tsk}_{typ[0]}_visual.pkl'), 'rb') as f:\n","        data = pickle.load(f)\n","\n","task1 = task[2]\n","feature_a = 'audiofeat_wav2vec2'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","    feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","    for imgname, val in feature.items():\n","        if imgname in data[task1][vname]:\n","            data[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data[task1][vname].items()):\n","            if len(value) < 3:\n","                data[task1][vname].pop(img)"]},{"cell_type":"code","source":["img, predict, label = [], [], []\n","vid = {}\n","iterator = iter(train_loader)\n","for i in range(len(train_loader)//32):\n","    VA = next(iterator)\n","    images = VA['frame']\n","    vis_feat, aud_feat, y = VA[visual_feat], VA[auft], VA['label']\n","    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n","    _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n","    preds = torch.cat((vpred, apred), dim=1)\n","    img.extend(images)\n","    predict.extend(preds)\n","    label.extend(y)"],"metadata":{"id":"3-J9sDest1jm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index, pred, lab = [], [], []\n","train_vid = {}\n","for i, val in enumerate(img):\n","    ind = int(val.split('/')[1][:-4])\n","    vname = val.split('/')[0]\n","    if i == 0:\n","        prename = vname\n","        index.append(ind)\n","        pred.append(predict[i])\n","        lab.append(label[i])\n","    else:\n","        if vname == prename:\n","            index.append(ind)\n","            pred.append(predict[i])\n","            lab.append(label[i])\n","        else:\n","            combined = list(zip(index, pred, lab))\n","            combined_sorted = sorted(combined, key=lambda x: x[0])\n","            index_list_sorted, pred_list_sorted, lab_list_sorted = zip(*combined_sorted)\n","            train_vid[prename] = (list(index_list_sorted), list(pred_list_sorted), list(lab_list_sorted))\n","            prename = vname\n","            index, pred, lab = [], [], []\n","            index.append(ind)\n","            pred.append(predict[i])\n","            lab.append(label[i])"],"metadata":{"id":"EagxiSSLt8Yq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_vid = {}\n","for vname in tqdm(vidnames):\n","        img, predict, label = [], [], []\n","        for imgname, val in sorted(data[tsk][vname].items()):\n","            vis_feat = torch.tensor(val[visual_feat]).to(device)\n","            if auft == 'nope':\n","                aud_feat = None\n","            else:\n","                aud_feat = torch.tensor(val[auft]).to(device)\n","            if tsk == task[2]:\n","                _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor([vpred, apred])\n","            else:\n","                _, pred = mlp_model(vis_feat, aud_feat)\n","                preds = torch.tensor(pred)\n","            ind = int(imgname.split('/')[1][:-4])\n","            img.append(ind)\n","            predict.append(preds)\n","            label.append(data[tsk][vname][imgname]['label'])\n","        train_vid[vname] = (img, predict, label)"],"metadata":{"id":"FFNHtCeY0J8n"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqjDo8w8vep5"},"outputs":[],"source":["delta = 30"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZYvCMOjvep6"},"outputs":[],"source":["stride2scores={}\n","for stride in [200, 100, 50, 25, 10]:\n","    total_true, predictions, max_decision_values, decision_values1, decision_values2 = [],[],[], [], []\n","    for vidname, (img, predict, label) in train_vid.items():\n","        index = []\n","        for i,ind in enumerate(img):\n","            total_true.append(label[i].cpu().numpy())\n","            index.append(ind-1)\n","        preds_proba = smooth_prediction(img, predict)\n","        for i in range(len(index)):\n","            best_ind, proba = slide_window(preds_proba, index[i], delta, stride)\n","            predictions.append(proba)\n","            decision_values1.append(proba[0])\n","            decision_values2.append(proba[1])\n","            max_decision_values.append([max(decision_values1),max(decision_values2)])\n","    stride2scores[stride] = (np.array(total_true),np.array(predictions),np.array(max_decision_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJwzvJ6Evep6"},"outputs":[],"source":["def get_threshold(stride,fpr_corrected):\n","    (total_true,predictions,max_decision_values) = stride2scores[stride]\n","    mistakes = max_decision_values[predictions.any() != total_true.any()]\n","    best_threshold = -1\n","    for i, threshold in enumerate(sorted(max_decision_values[predictions.any() == total_true.any()])[::-1]):\n","        tpr = i/len(predictions)\n","        fpr = (mistakes > threshold).sum()/len(predictions)\n","\n","        if fpr > fpr_corrected:\n","            if best_threshold == -1:\n","                best_threshold = threshold\n","            print(stride, 'best_threshold', best_threshold, i)\n","            break\n","        best_threshold = threshold\n","    return best_threshold"]},{"cell_type":"code","source":["stride2threshold = {}\n","for stride in stride2scores:\n","    fpr_corrected=0.05\n","    stride2threshold[stride] = get_threshold(stride,fpr_corrected)\n","stride2threshold[1] = 0\n","print(stride2threshold)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0UNKtn9ygh4","executionInfo":{"status":"ok","timestamp":1716121438120,"user_tz":-180,"elapsed":26,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"f32a62c6-72ea-4aa1-9320-e3d4536f624b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{200: array([[0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ]], dtype=float32), 100: array([[0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ]], dtype=float32), 50: array([[0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ]], dtype=float32), 25: array([[0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ]], dtype=float32), 10: array([[0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.24387085, 0.24387085],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.4420853 , 0.4420853 ],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.50624746, 0.50624746],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.54399276, 0.54399276],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.64593816, 0.64593816],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.74015045, 0.74015045],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ],\n","       [0.797523  , 0.797523  ]], dtype=float32), 1: 0}\n"]}]},{"cell_type":"code","source":["all_strides=[\n","    [200, 100, 50, 10, 1],\n","    [50, 25, 1],\n","    [50, 10, 1],\n","    [200,50,1],\n","    [100,50,1],\n","    [200,1],\n","    [100,1],\n","    [50,1]\n","]\n","for s in stride2threshold.keys():\n","    all_strides.append([s])\n","\n","for strides in all_strides:\n","    print(strides)\n","    last_stride=strides[-1]\n","\n","    total_true=[]\n","    total_preds=[]\n","    total_frames_processed,total_frames=0,0\n","    time_each = []\n","    start = time.time()\n","    for videoname, (img, predict, label) in test_vid.items():\n","        emotional_img=[]\n","        start1 = time.time()\n","        for i,ind in enumerate(img):\n","            total_true.append(label[i].cpu().numpy())\n","            emotional_img.append(ind-1)\n","        cur_ind=0\n","        preds_proba=[]\n","        for i in range(img[-1]):\n","            if img[cur_ind]-1==i:\n","                preds_proba.append(predict[cur_ind])\n","                cur_ind+=1\n","            else:\n","                if cur_ind==0:\n","                    preds_proba.append(predict[cur_ind])\n","                else:\n","                    w=(i-img[cur_ind-1]+1)/(img[cur_ind]-img[cur_ind-1])\n","                    pred=w*predict[cur_ind-1]+(1-w)*predict[cur_ind]\n","                    preds_proba.append(pred)\n","\n","        preds_proba=np.array([p.cpu().detach().numpy() for p in preds_proba])\n","\n","        preds=[[1]*2 for _ in range(len(emotional_img))]\n","        end1 = time.time()\n","        time_each.append(end1 - start1)\n","        for stride in strides:\n","            threshold=stride2threshold[stride]\n","            for i in range(len(emotional_img)):\n","                if preds[i][0]>=-1 and preds[i][1]>=-1:\n","                    i1=max(emotional_img[i]-delta,0)\n","                    cur_preds=preds_proba[i1:emotional_img[i]+delta+1:stride]\n","                    proba=np.mean(cur_preds,axis=0)\n","                    best_ind=np.argmax(proba)\n","                    if stride == 1:\n","                        if proba[best_ind]>=threshold or stride==last_stride:\n","                                total_frames_processed+=len(cur_preds)\n","                                total_frames+=len(preds_proba[i1:emotional_img[i]+delta+1])\n","                                preds[i]=proba\n","                    else:\n","                        if proba[0].all()>=threshold[0].all() or proba[1].all()>=threshold[1].all() or stride==last_stride:\n","                                total_frames_processed+=len(cur_preds)\n","                                total_frames+=len(preds_proba[i1:emotional_img[i]+delta+1])\n","                                preds[i]=proba\n","\n","        for p in preds:\n","            total_preds.append(p)\n","    end = time.time()\n","    elapsed_time = end - start - sum(time_each)\n","    total_true=np.array(total_true)\n","    preds=np.array(total_preds)\n","    ccc1, ccc2 = compute_VA_CCC(preds, total_true)\n","    print(f'CCCV: {ccc1:.3f}; CCCA: {ccc2:.3f}')\n","    print(total_frames_processed,total_frames,round(total_frames_processed/total_frames,3))\n","    print(f\"Time: {elapsed_time:.2f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5utrwVHu2AHH","executionInfo":{"status":"ok","timestamp":1716121455007,"user_tz":-180,"elapsed":16910,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"}},"outputId":"e8d99d50-83db-461d-e281-d8d7524dbeb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[200, 100, 50, 10, 1]\n","CCCV: 0.788; CCCA: 0.582\n","806494 3416235 0.236\n","Time: 1.66 seconds\n","[50, 25, 1]\n","CCCV: 0.788; CCCA: 0.582\n","739232 2049741 0.361\n","Time: 1.07 seconds\n","[50, 10, 1]\n","CCCV: 0.788; CCCA: 0.582\n","783940 2049741 0.382\n","Time: 1.00 seconds\n","[200, 50, 1]\n","CCCV: 0.788; CCCA: 0.582\n","716878 2049741 0.35\n","Time: 0.95 seconds\n","[100, 50, 1]\n","CCCV: 0.788; CCCA: 0.582\n","716878 2049741 0.35\n","Time: 0.98 seconds\n","[200, 1]\n","CCCV: 0.788; CCCA: 0.582\n","694524 1366494 0.508\n","Time: 0.63 seconds\n","[100, 1]\n","CCCV: 0.788; CCCA: 0.582\n","694524 1366494 0.508\n","Time: 0.53 seconds\n","[50, 1]\n","CCCV: 0.788; CCCA: 0.582\n","705601 1366494 0.516\n","Time: 0.62 seconds\n","[200]\n","CCCV: 0.755; CCCA: 0.547\n","11277 683247 0.017\n","Time: 0.36 seconds\n","[100]\n","CCCV: 0.755; CCCA: 0.547\n","11277 683247 0.017\n","Time: 0.24 seconds\n","[50]\n","CCCV: 0.776; CCCA: 0.562\n","22354 683247 0.033\n","Time: 0.30 seconds\n","[25]\n","CCCV: 0.783; CCCA: 0.575\n","33631 683247 0.049\n","Time: 0.30 seconds\n","[10]\n","CCCV: 0.787; CCCA: 0.579\n","78339 683247 0.115\n","Time: 0.28 seconds\n","[1]\n","CCCV: 0.788; CCCA: 0.582\n","683247 683247 1.0\n","Time: 0.20 seconds\n"]}]},{"cell_type":"markdown","metadata":{"id":"nAuuTRp6S1O1"},"source":["# Draft"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzLY_GXH6hho"},"outputs":[],"source":["task = ['EXPR_Recognition_Challenge','AU_Detection_Challenge','VA_Estimation_Challenge']\n","split = ['Train_Set', 'Validation_Set']\n","typ = ['Train','Val','Test']\n","visual_feat = 'visualfeat_enet_b2_8_best'\n","audio_feat = 'audiofeat_wav2vec2'\n","audio_feat1 = 'audiofeat_vggish'\n","batch_size = 32"]},{"cell_type":"markdown","metadata":{"id":"BOdVpA8hgz9O"},"source":["### EXPR"]},{"cell_type":"markdown","metadata":{"id":"5JI4I-PZtII3"},"source":["#### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8962410,"status":"ok","timestamp":1715176556492,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"T4VVeD08wZOe","outputId":"8d258348-da9a-486d-cb71-3f53979e1f08"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 199/199 [2:29:21<00:00, 45.03s/it]\n"]}],"source":["feature_v = 'visualfeat_enet_b2_8_best'\n","task1 = task[0]\n","feature_dims = 0\n","data1 = {}\n","data1[task1] = {}\n","iname = []\n","anno_path = os.path.join(root,f'data/Annotations/{task[0]}/{split[0]}')\n","with open(os.path.join(root, f'data/Annotations/{task[0]}/{typ[0]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","feat_root = os.path.join(root + '/models/ABAW6', feature_v)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        with open(os.path.join(anno_path, f'{vname}.txt')) as f:\n","            labels = f.read().splitlines()\n","        data1[task1][vname] = {}\n","\n","        for imgname, val in feature.items():\n","            for i,line in enumerate(labels):\n","                if i > 0:\n","                    imname = get_names(vname, i)\n","                    if imname == imgname:\n","                        exp = int(line)\n","                        if exp >= 0:\n","                            labs = torch.tensor(exp)\n","                            data1[task1][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                            iname.append(imname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sz5Rpwj7BDWc"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[0]}_{typ[0]}_visual.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314062,"status":"ok","timestamp":1715176998225,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"fyy9a4wUwlxd","outputId":"4e96a0ba-8afe-417f-dfba-17f075ae94b6"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 199/199 [05:13<00:00,  1.58s/it]\n"]}],"source":["feature_a = 'audiofeat_wav2vec2'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DwxpYHcsSfDk"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[0]}_{typ[0]}_visual_wav2vec2.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"markdown","metadata":{"id":"8LFHRhoPtM3l"},"source":["#### Val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87SNK_sFtwZ0"},"outputs":[],"source":["feature_v = 'visualfeat_enet_b2_8_best'\n","task1 = task[0]\n","feature_dims = 0\n","data1 = {}\n","data1[task1] = {}\n","iname = []\n","anno_path = os.path.join(root,f'data/Annotations/{task[0]}/{split[0]}')\n","with open(os.path.join(root, f'data/Annotations/{task[0]}/{typ[1]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","feat_root = os.path.join(root + '/models/ABAW6', feature_v)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        #fname = os.path.join(vname, '.npy')\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        with open(os.path.join(anno_path, f'{vname}.txt')) as f:\n","            labels = f.read().splitlines()\n","        data1[task1][vname] = {}\n","\n","        for imgname, val in feature.items():\n","            for i,line in enumerate(labels):\n","                if i > 0:\n","                    imname = get_names(vname, i)\n","                    if imname == imgname:\n","                        exp = int(line)\n","                        if exp >= 0:\n","                            labs = torch.tensor(exp)\n","                            data1[task1][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                            iname.append(imname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlK7DU_1tbyj"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[0]}_{typ[1]}_visual.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nF17mXA_tzjy"},"outputs":[],"source":["feature_a = 'audiofeat_wav2vec2'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPwHPffqtbyk"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[0]}_{typ[1]}_visual_wav2vec2.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"markdown","metadata":{"id":"zyANFH5ztVZx"},"source":["#### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3tFjxkBtojM"},"outputs":[],"source":["feature_v = 'visualfeat_enet_b2_8_best'\n","task1 = task[0]\n","feature_dims = 0\n","data1 = {}\n","data1[task1] = {}\n","iname = []\n","anno_path = os.path.join(root,f'data/Annotations/{task[0]}/{split[1]}')\n","with open(os.path.join(root, f'data/Annotations/{task[0]}/{typ[2]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","feat_root = os.path.join(root + '/models/ABAW6', feature_v)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        with open(os.path.join(anno_path, f'{vname}.txt')) as f:\n","            labels = f.read().splitlines()\n","        data1[task1][vname] = {}\n","\n","        for imgname, val in feature.items():\n","            for i,line in enumerate(labels):\n","                if i > 0:\n","                    imname = get_names(vname, i)\n","                    if imname == imgname:\n","                        exp = int(line)\n","                        if exp >= 0:\n","                            labs = torch.tensor(exp)\n","                            data1[task1][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                            iname.append(imname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8iCzT8VmtojM"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[0]}_{typ[2]}_visual.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2rHjPHBRtojM"},"outputs":[],"source":["feature_a = 'audiofeat_wav2vec2'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1Nv-ZzAtojM"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[0]}_{typ[2]}_visual_wav2vec2.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"markdown","metadata":{"id":"Jore2DsOg4NF"},"source":["### AU"]},{"cell_type":"markdown","metadata":{"id":"xlJ5Wr2buFQ-"},"source":["#### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xgh_8xtclV01","outputId":"9d829a04-448d-41a7-c8c7-81a4014c34f2"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 236/236 [2:19:24<00:00, 35.44s/it]    \n"]}],"source":["feature_v = 'visualfeat_enet_b2_8_best'\n","task1 = task[1]\n","feature_dims = 0\n","data1 = {}\n","data1[task1] = {}\n","iname = []\n","anno_path = os.path.join(root,f'data/Annotations/{task[1]}/{split[0]}')\n","with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[0]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","feat_root = os.path.join(root + '/models/ABAW6', feature_v)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        with open(os.path.join(anno_path, f'{vname}.txt')) as f:\n","            labels = f.read().splitlines()\n","        data1[task1][vname] = {}\n","\n","        for imgname, val in feature.items():\n","            for i,line in enumerate(labels):\n","                if i > 0:\n","                    imname = get_names(vname, i)\n","                    if imname == imgname:\n","                        splitted_line=line.split(',')\n","                        aus = list(map(int,splitted_line))\n","                        if min(aus) >= 0:\n","                            labs = torch.tensor(aus)\n","                            data1[task1][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                            iname.append(imname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AeDHfttuFQ_"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/AU/{task[1]}_{typ[0]}_visual.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAQziEjGlV02","outputId":"c4f2191b-ffc0-4a9b-cce9-0cd93b1df96a"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 236/236 [00:13<00:00, 17.86it/s]\n"]}],"source":["feature_a = 'audiofeat_wav2vec2'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kt43QwQsuFQ_"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/AU/{task[1]}_{typ[0]}_visual_wav2vec2.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_eNdY29ywdCk"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/AU/{task[1]}_{typ[0]}_visual.pkl'), 'rb') as f:\n","    data1 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApCwwx9ux_0v"},"outputs":[],"source":["dims = 0\n","iname = []\n","task1 = task[1]\n","with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[0]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20810,"status":"ok","timestamp":1715282809096,"user":{"displayName":"Vũ Thị Bích Duyên","userId":"14091490067824177935"},"user_tz":-180},"id":"pk3NfJUQwk6k","outputId":"2fea7464-841e-4948-8ce4-18485c67410d"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 236/236 [00:20<00:00, 11.35it/s]\n"]}],"source":["feature_a = 'audiofeat_vggish'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEPwCrlNwqd9"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/AU/{task[1]}_{typ[0]}_visual_vggish.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"markdown","metadata":{"id":"C7KsXeSCuH0W"},"source":["#### Val"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"elapsed":224961,"status":"error","timestamp":1715185082856,"user":{"displayName":"Uyên Võ","userId":"08500545526524026138"},"user_tz":-180},"id":"TwvdU9pz8FYE","outputId":"de54aa93-4921-4214-8e83-8c8e5731d2ef"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 59/59 [17:09<00:00, 17.45s/it]\n"]}],"source":["feature_v = 'visualfeat_enet_b2_8_best'\n","task1 = task[1]\n","feature_dims = 0\n","data1 = {}\n","data1[task1] = {}\n","iname = []\n","anno_path = os.path.join(root,f'data/Annotations/{task[1]}/{split[0]}')\n","with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[1]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","feat_root = os.path.join(root + '/models/ABAW6', feature_v)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        with open(os.path.join(anno_path, f'{vname}.txt')) as f:\n","            labels = f.read().splitlines()\n","        data1[task1][vname] = {}\n","\n","        for imgname, val in feature.items():\n","            for i,line in enumerate(labels):\n","                if i > 0:\n","                    imname = get_names(vname, i)\n","                    if imname == imgname:\n","                        splitted_line=line.split(',')\n","                        aus = list(map(int,splitted_line))\n","                        if min(aus) >= 0:\n","                            labs = torch.tensor(aus)\n","                            data1[task1][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                            iname.append(imname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eRFOkqhhCaV"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/AU/{task[1]}_{typ[1]}_visual.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LN-0HwtamOH5","outputId":"dd46c779-d3a0-43ab-88bd-8f061821019f"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 59/59 [00:02<00:00, 25.22it/s]\n"]}],"source":["feature_a = 'audiofeat_wav2vec2'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4N6SuJ187iz"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/AU/{task[1]}_{typ[1]}_visual_wav2vec2.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A27wdKNU2kST"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/AU/{task[1]}_{typ[1]}_visual.pkl'), 'rb') as f:\n","    data1 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekVBeNix2qqf"},"outputs":[],"source":["dims = 0\n","iname = []\n","task1 = task[1]\n","with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[1]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7531,"status":"ok","timestamp":1715283419325,"user":{"displayName":"Vũ Thị Bích Duyên","userId":"14091490067824177935"},"user_tz":-180},"id":"ie4EksNt2yd_","outputId":"bf0b33ad-9f80-4082-dabd-8119c9662a58"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 59/59 [00:07<00:00,  7.95it/s]\n"]}],"source":["feature_a = 'audiofeat_vggish'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nx8-_TsE22sE"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/AU/{task[1]}_{typ[1]}_visual_vggish.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"markdown","metadata":{"id":"KHA5NrbwuUcQ"},"source":["#### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShWpfJnimk91","outputId":"26818f15-5cc2-4d33-c05b-22d0f3e4a560"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 105/105 [29:38<00:00, 16.94s/it]\n"]}],"source":["feature_v = 'visualfeat_enet_b2_8_best'\n","task1 = task[2]\n","feature_dims = 0\n","data1 = {}\n","data1[task1] = {}\n","iname = []\n","anno_path = os.path.join(root,f'data/Annotations/{task[2]}/{split[1]}')\n","with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[2]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","feat_root = os.path.join(root + '/models/ABAW6', feature_v)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        #fname = os.path.join(vname, '.npy')\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        with open(os.path.join(anno_path, f'{vname}.txt')) as f:\n","            labels = f.read().splitlines()\n","        data1[task1][vname] = {}\n","\n","        for imgname, val in feature.items():\n","            for i,line in enumerate(labels):\n","                if i > 0:\n","                    imname = get_names(vname, i)\n","                    if imname == imgname:\n","                        splitted_line=line.split(',')\n","                        valence=float(splitted_line[0])\n","                        arousal=float(splitted_line[1])\n","                        if valence >= -1 and arousal >= -1:\n","                            labs = torch.tensor([valence, arousal])\n","                            data1[task1][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                            iname.append(imname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dXyuhDkuUcR"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/AU/{task[1]}_{typ[2]}_visual.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314062,"status":"ok","timestamp":1715176998225,"user":{"displayName":"Uyên Võ Ngọc Bích","userId":"13920339801757984356"},"user_tz":-180},"id":"fV5nIP7Cmsts","outputId":"4e96a0ba-8afe-417f-dfba-17f075ae94b6"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 105/105 [00:05<00:00, 20.62it/s]\n"]}],"source":["feature_a = 'audiofeat_wav2vec2'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzJ7PYERuUcR"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/AU/{task[1]}_{typ[2]}_visual_wav2vec2.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vwGrzgr4kz-"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/AU/{task[1]}_{typ[2]}_visual.pkl'), 'rb') as f:\n","    data1 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YEBgkZk4pqG"},"outputs":[],"source":["dims = 0\n","iname = []\n","task1 = task[1]\n","with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[2]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9160,"status":"ok","timestamp":1715283970676,"user":{"displayName":"Vũ Thị Bích Duyên","userId":"14091490067824177935"},"user_tz":-180},"id":"qgMTVWY24yRO","outputId":"7525e10b-8b58-4191-aca5-be25f6491baf"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 105/105 [00:09<00:00, 11.66it/s]\n"]}],"source":["feature_a = 'audiofeat_vggish'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3f51vekh42gU"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/AU/{task[1]}_{typ[2]}_visual_vggish.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"markdown","metadata":{"id":"TOJEhD0yknfs"},"source":["### VA"]},{"cell_type":"markdown","metadata":{"id":"L1iYigz5uYfD"},"source":["#### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHHDySYSuYfD"},"outputs":[],"source":["feature_v = 'visualfeat_enet_b2_8_best'\n","task1 = task[2]\n","feature_dims = 0\n","data1 = {}\n","data1[task1] = {}\n","iname = []\n","anno_path = os.path.join(root,f'data/Annotations/{task[2]}/{split[0]}')\n","with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[0]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","feat_root = os.path.join(root + '/models/ABAW6', feature_v)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        #fname = os.path.join(vname, '.npy')\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        with open(os.path.join(anno_path, f'{vname}.txt')) as f:\n","            labels = f.read().splitlines()\n","        data1[task1][vname] = {}\n","\n","        for imgname, val in feature.items():\n","            for i,line in enumerate(labels):\n","                if i > 0:\n","                    imname = get_names(vname, i)\n","                    if imname == imgname:\n","                        splitted_line=line.split(',')\n","                        valence=float(splitted_line[0])\n","                        arousal=float(splitted_line[1])\n","                        if valence >= -1 and arousal >= -1:\n","                            labs = torch.tensor([valence, arousal])\n","                            data1[task1][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                            iname.append(imname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDVqXxZhuYfD"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[2]}_{typ[0]}_visual.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBlUmH1ouYfD"},"outputs":[],"source":["feature_a = 'audiofeat_wav2vec2'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRL6P6KTuYfD"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[2]}_{typ[0]}_visual_wav2vec2.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"markdown","metadata":{"id":"HnJRnyhqt52Y"},"source":["#### Val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1gPQ3SBt52Z"},"outputs":[],"source":["feature_v = 'visualfeat_enet_b2_8_best'\n","task1 = task[2]\n","feature_dims = 0\n","data1 = {}\n","data1[task1] = {}\n","iname = []\n","anno_path = os.path.join(root,f'data/Annotations/{task[2]}/{split[0]}')\n","with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[1]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","feat_root = os.path.join(root + '/models/ABAW6', feature_v)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        with open(os.path.join(anno_path, f'{vname}.txt')) as f:\n","            labels = f.read().splitlines()\n","        data1[task1][vname] = {}\n","\n","        for imgname, val in feature.items():\n","            for i,line in enumerate(labels):\n","                if i > 0:\n","                    imname = get_names(vname, i)\n","                    if imname == imgname:\n","                        splitted_line=line.split(',')\n","                        valence=float(splitted_line[0])\n","                        arousal=float(splitted_line[1])\n","                        if valence >= -1 and arousal >= -1:\n","                            labs = torch.tensor([valence, arousal])\n","                            data1[task1][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                            iname.append(imname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RL4oV7kt52Z"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[2]}_{typ[1]}_visual.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iB1qXlN-t52a"},"outputs":[],"source":["feature_a = 'audiofeat_wav2vec2'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pM0N4OOdt52a"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[2]}_{typ[1]}_visual_wav2vec2.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6anQALX5TzO"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[1]}_visual.pkl'), 'rb') as f:\n","    data1 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpuAi7Be6QUz"},"outputs":[],"source":["dims = 0\n","iname = []\n","task1 = task[2]\n","with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[1]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15604,"status":"ok","timestamp":1715284380684,"user":{"displayName":"Vũ Thị Bích Duyên","userId":"14091490067824177935"},"user_tz":-180},"id":"2NLbnbMe6WNx","outputId":"8eb748a6-2160-4b15-ce44-46118b904999"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 71/71 [00:15<00:00,  4.64it/s]\n"]}],"source":["feature_a = 'audiofeat_vggish'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-K2D0MH6W3h"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{vis}/VA/{task[2]}_{typ[1]}_visual_vggish.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"markdown","metadata":{"id":"_S3eya8Ytv-g"},"source":["#### Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3798732,"status":"ok","timestamp":1715184001275,"user":{"displayName":"Uyên Võ","userId":"08500545526524026138"},"user_tz":-180},"id":"v3uwP6FEknft","outputId":"26818f15-5cc2-4d33-c05b-22d0f3e4a560"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 76/76 [1:03:17<00:00, 49.97s/it]\n"]}],"source":["feature_v = 'visualfeat_enet_b2_8_best'\n","task1 = task[2]\n","feature_dims = 0\n","data1 = {}\n","data1[task1] = {}\n","iname = []\n","anno_path = os.path.join(root,f'data/Annotations/{task[2]}/{split[1]}')\n","with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[2]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()\n","feat_root = os.path.join(root + '/models/ABAW6', feature_v)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        #fname = os.path.join(vname, '.npy')\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        with open(os.path.join(anno_path, f'{vname}.txt')) as f:\n","            labels = f.read().splitlines()\n","        data1[task1][vname] = {}\n","\n","        for imgname, val in feature.items():\n","            for i,line in enumerate(labels):\n","                if i > 0:\n","                    imname = get_names(vname, i)\n","                    if imname == imgname:\n","                        splitted_line=line.split(',')\n","                        valence=float(splitted_line[0])\n","                        arousal=float(splitted_line[1])\n","                        if valence >= -1 and arousal >= -1:\n","                            labs = torch.tensor([valence, arousal])\n","                            data1[task1][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n","                            iname.append(imname)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JUjvyg9knft"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[2]}_{typ[2]}_visual.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167594,"status":"ok","timestamp":1715184529452,"user":{"displayName":"Uyên Võ","userId":"08500545526524026138"},"user_tz":-180},"id":"GsuKStAEknfu","outputId":"5bbed5dc-ebc1-412e-a582-54170d145ca5"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 76/76 [02:47<00:00,  2.20s/it]\n"]}],"source":["feature_a = 'audiofeat_wav2vec2'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fNyLfAnknfu"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{task[2]}_{typ[2]}_visual_wav2vec2.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVwhRKSl6fDP"},"outputs":[],"source":["with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[2]}_visual.pkl'), 'rb') as f:\n","    data1 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MINUmz5a6flZ"},"outputs":[],"source":["dims = 0\n","iname = []\n","task1 = task[2]\n","with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[2]}.txt'), 'r') as f:\n","    vidnames = f.read().splitlines()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12224,"status":"ok","timestamp":1715284646461,"user":{"displayName":"Vũ Thị Bích Duyên","userId":"14091490067824177935"},"user_tz":-180},"id":"Y-pP1bg06sf9","outputId":"1c57912d-15dd-4789-fda1-9d34f9f32fed"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 76/76 [00:11<00:00,  6.43it/s]\n"]}],"source":["feature_a = 'audiofeat_vggish'\n","feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n","filenames = os.listdir(feat_root)[:]\n","for vname in tqdm(vidnames):\n","        feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n","        for imgname, val in feature.items():\n","            if imgname in data1[task1][vname]:\n","                data1[task1][vname][imgname].update({f'{feature_a}': val})\n","        for img, value in list(data1[task1][vname].items()):\n","            if len(value) < 3:\n","                data1[task1][vname].pop(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GACISloB6v_j"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/MSc/Thesis/models/ABAW6/{vis}/VA/{task[2]}_{typ[2]}_visual_vggish.pkl', 'wb') as f:\n","    pickle.dump(data1, f)"]},{"cell_type":"markdown","metadata":{"id":"1iSmZonyEJSE"},"source":["#### Smooth validation predictions"]},{"cell_type":"code","source":["img, predict, label = [], [], []\n","vid = {}\n","iterator = iter(test_loader)\n","for i in range(len(test_loader)//32):\n","    AU = next(iterator)\n","    images = AU['frame']\n","    vis_feat, y = AU[visual_feat], AU['label']\n","    vis_feat = vis_feat.to(device)\n","    aud_feat = None\n","    _, au_pred = AU_model(vis_feat, aud_feat)\n","    img.extend(images)\n","    predict.extend(au_pred)\n","    label.extend(y)"],"metadata":{"id":"vK9GtPAD3u6C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index, pred, lab = [], [], []\n","test_vid = {}\n","for i, val in enumerate(img):\n","    ind = int(val.split('/')[1][:-4])\n","    vname = val.split('/')[0]\n","    if i == 0:\n","        prename = vname\n","        index.append(ind)\n","        pred.append(predict[i])\n","        lab.append(label[i])\n","    else:\n","        if vname == prename:\n","            index.append(ind)\n","            pred.append(predict[i])\n","            lab.append(label[i])\n","        else:\n","            combined = list(zip(index, pred, lab))\n","            combined_sorted = sorted(combined, key=lambda x: x[0])\n","            index_list_sorted, pred_list_sorted, lab_list_sorted = zip(*combined_sorted)\n","            test_vid[prename] = (list(index_list_sorted), list(pred_list_sorted), list(lab_list_sorted))\n","            prename = vname\n","            index, pred, lab = [], [], []\n","            index.append(ind)\n","            pred.append(predict[i])\n","            lab.append(label[i])"],"metadata":{"id":"audGHaTSgn4N"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["MrIj0S-0YMHv","pa2wGLmhWRO1","97AGawC8PnRo","SwMmqGyJwnZg","HoofZGhrU6Ih","UQsS25mKxpIu","Z1h3aTTNLEnT","BAaSqtubbShq","7XrIUk5abShr","7zISVpqDy0Io","KDN9v__h-8gP","OsoLTqkz-8gP","ZWSTa4TEzNLi","3MwWOEO-ZRXk","IM-R-dPNZRXk","y7u9hUAvqCkN","4TELrToCUaxP","xD1f1Y8uIwkd","Fy_rcZd9QpC_","awNLFEWmT3C2","1lC8Qb7UT62p","sd8sIrCrwZIT","0_2hrWvEYRTB","RsUw4AxuwtBo","nAuuTRp6S1O1","BOdVpA8hgz9O"],"gpuType":"T4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}