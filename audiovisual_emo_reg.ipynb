{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u26MpPMGg4ps"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5920a52-b44a-4d4a-e7a3-0c3fc7f3a7b0",
        "id": "013USzEWg4pu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rqhy9k1j9L6q",
        "outputId": "4078eef4-2910-4fb2-f27c-7ad9551f0257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting timm\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting resampy\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from resampy) (1.25.2)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.10/dist-packages (from resampy) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy) (0.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->timm)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->timm)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->timm)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Installing collected packages: pydub, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, resampy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pydub-0.25.1 resampy-0.4.3 timm-0.9.16\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub timm resampy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yqedhbnugk91"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWla6BETg4pv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch import optim\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJu67xl0g4pw"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqXaIKjWg4pw"
      },
      "outputs": [],
      "source": [
        "root = '/content/drive/MyDrive/MSc/Thesis'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBZofuPUj_tq"
      },
      "outputs": [],
      "source": [
        "def setup_seed(seed):\n",
        "     torch.manual_seed(seed)\n",
        "     torch.cuda.manual_seed_all(seed)\n",
        "     np.random.seed(seed)\n",
        "     random.seed(seed)\n",
        "     torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx3Rgnpmrg1_"
      },
      "outputs": [],
      "source": [
        "setup_seed(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgoCljH14WXu"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYRIX0SkKJZf",
        "outputId": "fde84f8a-ebec-4e55-fb88-706de15c0c2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:00<00:00, 218785.29it/s]\n",
            "100%|██████████| 76/76 [00:00<00:00, 121342.64it/s]\n",
            "100%|██████████| 248/248 [00:00<00:00, 211119.83it/s]\n",
            "100%|██████████| 70/70 [00:00<00:00, 226719.14it/s]\n",
            "100%|██████████| 295/295 [00:00<00:00, 196555.95it/s]\n",
            "100%|██████████| 105/105 [00:00<00:00, 110737.22it/s]\n"
          ]
        }
      ],
      "source": [
        "#test_list = []\n",
        "a = []\n",
        "for d in ['VA_Estimation_Challenge','EXPR_Recognition_Challenge','AU_Detection_Challenge']:\n",
        "    data_dir=os.path.join(root,'data/Annotations',d)\n",
        "    for k in ['Train_Set','Validation_Set']:\n",
        "        data_label=os.path.join(data_dir,k)\n",
        "        with open(os.path.join(data_dir,f'{k}.txt'), 'w') as f:\n",
        "            for filename in tqdm(os.listdir(data_label)):\n",
        "                fn, ext = os.path.splitext(os.path.basename(filename))\n",
        "                if ext.lower()=='.txt':\n",
        "                    f.write(fn+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea5gmFdmV6EC",
        "outputId": "ab19ebca-9c67-4306-b1c3-871a73abd656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VA_Estimation_Challenge\n",
            "Train_set:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 285/285 [00:00<00:00, 300799.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val_set:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 71/71 [00:00<00:00, 387250.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test_set:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 78090.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EXPR_Recognition_Challenge\n",
            "Train_set:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 199/199 [00:00<00:00, 674205.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val_set:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 49/49 [00:00<00:00, 349525.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test_set:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 70/70 [00:00<00:00, 460190.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AU_Detection_Challenge\n",
            "Train_set:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 236/236 [00:00<00:00, 772118.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val_set:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 59/59 [00:00<00:00, 391556.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test_set:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:00<00:00, 522422.21it/s]\n"
          ]
        }
      ],
      "source": [
        "test_list = []\n",
        "\n",
        "for d in ['VA_Estimation_Challenge','EXPR_Recognition_Challenge','AU_Detection_Challenge']:\n",
        "    print(d)\n",
        "    with open(os.path.join(root,f'data/Annotations/{d}/Train_Set.txt'), 'r') as f:\n",
        "        files = f.read().splitlines()\n",
        "    random.shuffle(files)\n",
        "    ratio = int(len(files)/5) # 20%\n",
        "    val_set = files[:ratio]\n",
        "    test_list.extend(val_set)\n",
        "    train_set = files[ratio:]\n",
        "    print('Train_set:')\n",
        "    with open(os.path.join(root,f'data/Annotations/{d}/Train.txt'), 'w') as f:\n",
        "        for ftrain in tqdm(train_set):\n",
        "            f.write(ftrain+'\\n')\n",
        "    print('Val_set:')\n",
        "    with open(os.path.join(root,f'data/Annotations/{d}/Val.txt'), 'w') as f:\n",
        "        for fval in tqdm(val_set):\n",
        "            f.write(fval+'\\n')\n",
        "\n",
        "    with open(os.path.join(root,f'data/Annotations/{d}/Validation_Set.txt'), 'r') as f:\n",
        "        test_set = f.read().splitlines()\n",
        "    test_list.extend(test_set)\n",
        "    print('Test_set:')\n",
        "    with open(os.path.join(root,f'data/Annotations/{d}/Test.txt'), 'w') as f:\n",
        "        for ftest in tqdm(test_set):\n",
        "            f.write(ftest+'\\n')\n",
        "with open(os.path.join(root,'data/test_list.txt'), 'w') as f:\n",
        "    for fn in test_list:\n",
        "        f.write(fn+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU8CMY9CS-7K"
      },
      "outputs": [],
      "source": [
        "def get_names(vid, id):\n",
        "    name = \"\"\n",
        "    if id>=0 and id<10:\n",
        "        name = f\"{vid}/0000\" + str(id) + \".jpg\"\n",
        "    elif id>=10 and id<100:\n",
        "        name = f\"{vid}/000\" + str(id) + \".jpg\"\n",
        "    elif id>=100 and id<1000:\n",
        "        name = f\"{vid}/00\" + str(id) + \".jpg\"\n",
        "    elif id>=1000 and id<10000:\n",
        "        name = f\"{vid}/0\" + str(id) + \".jpg\"\n",
        "    else:\n",
        "        name = f\"{vid}/\" + str(id) + \".jpg\"\n",
        "    return name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NibryvaO4WXv"
      },
      "outputs": [],
      "source": [
        "def load_feature_cache(feature_names):\n",
        "\n",
        "    for feature_name in feature_names:\n",
        "        print('processing:', feature_name)\n",
        "        feat_root = os.path.join(root + '/models/ABAW6', feature_name)\n",
        "        save_root = feat_root+'.pkl'\n",
        "        if os.path.exists(save_root):\n",
        "            continue\n",
        "        filenames = os.listdir(feat_root)[:]\n",
        "        feat = {}\n",
        "        for fname in tqdm(filenames):\n",
        "            vname = fname.split('.')[0]\n",
        "            if filenames[0].endswith('.npy'):\n",
        "                fea = np.load(os.path.join(feat_root, fname), allow_pickle=True).tolist()\n",
        "            elif filenames[0].endswith('.pkl'):\n",
        "                with open(os.path.join(feat_root, fname), 'rb') as f:\n",
        "                    fea = pickle.load(f)\n",
        "            feat[vname] = fea\n",
        "        with open(save_root, 'wb') as f:\n",
        "            pickle.dump(feat, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ26nV_r4WXv"
      },
      "outputs": [],
      "source": [
        "feature_names = [\n",
        "        'visualfeat_enet_b2_8_best_cropped',\n",
        "        'visualfeat_enet_b2_8_best_cropped_aligned',\n",
        "        'audiofeat_vggish',\n",
        "        'audiofeat_wav2vec2'\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq3j5Dr_4WXv"
      },
      "outputs": [],
      "source": [
        "class ABAW_dataset(Dataset):\n",
        "    def __init__(self, root, split, typ, task, feature_v, feature_a):\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.typ = typ\n",
        "        self.task = task\n",
        "        self.anno_path = os.path.join(self.root,f'data/Annotations/{self.task}/{self.split}')\n",
        "        self.feature_v, self.feature_a = feature_v, feature_a\n",
        "        self.feature = [self.feature_v, self.feature_a]\n",
        "        with open(os.path.join(root, f'data/Annotations/{self.task}/{self.typ}.txt'), 'r') as f:\n",
        "                self.vidnames = f.read().splitlines()\n",
        "        self.feature_dims = 0\n",
        "        self.data = {}\n",
        "        self.data[self.task] = {}\n",
        "        self.iname = []\n",
        "        for feature_name in self.feature:\n",
        "            if 'visual' in feature_name:\n",
        "                self.data = self.load_feature_v(feature_name)\n",
        "            elif 'audio' in feature_name:\n",
        "                self.data = self.load_feature_a(feature_name)\n",
        "\n",
        "    def get_names(vid, id):\n",
        "            name = \"\"\n",
        "            if id>=0 and id<10:\n",
        "                name = f\"{vid}/0000\" + str(id) + \".jpg\"\n",
        "            elif id>=10 and id<100:\n",
        "                name = f\"{vid}/000\" + str(id) + \".jpg\"\n",
        "            elif id>=100 and id<1000:\n",
        "                name = f\"{vid}/00\" + str(id) + \".jpg\"\n",
        "            elif id>=1000 and id<10000:\n",
        "                name = f\"{vid}/0\" + str(id) + \".jpg\"\n",
        "            else:\n",
        "                name = f\"{vid}/\" + str(id) + \".jpg\"\n",
        "            return name\n",
        "\n",
        "    def load_feature_v(self, feature_v):\n",
        "            print(f'loading visual feature: {feature_v}')\n",
        "            feat_root = os.path.join(root + '/models/ABAW6', feature_v)\n",
        "            filenames = os.listdir(feat_root)[:]\n",
        "            for vname in tqdm(self.vidnames):\n",
        "                    feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n",
        "                    with open(os.path.join(self.anno_path, f'{vname}.txt')) as f:\n",
        "                        labels = f.read().splitlines()\n",
        "                    self.data[self.task][vname] = {}\n",
        "\n",
        "                    for imgname, val in feature.items():\n",
        "                        for i,line in enumerate(labels):\n",
        "                            if i > 0:\n",
        "                                imname = get_names(vname, i)\n",
        "                                if imname == imgname:\n",
        "                                    if self.task == 'AU_Detection_Challenge':\n",
        "                                        splitted_line=line.split(',')\n",
        "                                        aus = list(map(int,splitted_line))\n",
        "                                        if min(aus) >= 0:\n",
        "                                            labs = torch.tensor(aus)\n",
        "                                            self.data[self.task][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n",
        "                                            self.iname.append(imname)\n",
        "                                    elif self.task == 'VA_Estimation_Challenge':\n",
        "                                        splitted_line=line.split(',')\n",
        "                                        valence=float(splitted_line[0])\n",
        "                                        arousal=float(splitted_line[1])\n",
        "                                        if valence >= -1 and arousal >= -1:\n",
        "                                            labs = torch.tensor([valence, arousal])\n",
        "                                            self.data[self.task][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n",
        "                                            self.iname.append(imname)\n",
        "                                    elif self.task == 'EXPR_Recognition_Challenge':\n",
        "                                        exp = int(line)\n",
        "                                        if exp >= 0:\n",
        "                                            labs = torch.tensor(exp)\n",
        "                                            self.data[self.task][vname][imgname] = {f'{feature_v}': val, 'label': labs}\n",
        "                                            self.iname.append(imname)\n",
        "                    self.feature_dims += len(self.data[self.task][vname])\n",
        "            return self.data\n",
        "\n",
        "    def load_feature_a(self, feature_a):\n",
        "            print(f'loading audio feature: {feature_a}')\n",
        "            feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n",
        "            filenames = os.listdir(feat_root)[:]\n",
        "            for vname in tqdm(self.vidnames):\n",
        "                    feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n",
        "                    for imgname, val in feature.items():\n",
        "                        if imgname in self.data[self.task][vname]:\n",
        "                            self.data[self.task][vname][imgname].update({f'{feature_a}': val})\n",
        "\n",
        "                    for img, value in list(self.data[self.task][vname].items()):\n",
        "                        if len(value) < 3:\n",
        "                            self.data[self.task][vname].pop(img)\n",
        "            return self.data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "            frame = self.iname[index]\n",
        "            vname = frame.split('/')[0]\n",
        "            data = self.data[self.task][vname][frame]\n",
        "            data['frame'] = frame\n",
        "            data['vid'] = vname\n",
        "            data['label'] = self.data[self.task][vname][frame]['label']\n",
        "            return data\n",
        "\n",
        "    def __len__(self):\n",
        "            return self.feature_dims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yObcGVx9re2M"
      },
      "outputs": [],
      "source": [
        "class ABAW_dataset1(Dataset):\n",
        "    def __init__(self, data, iname, dims, task):\n",
        "        self.data = data\n",
        "        self.iname = iname\n",
        "        self.task = task\n",
        "        self.feature_dims = dims\n",
        "    def __getitem__(self, index):\n",
        "        frame = self.iname[index]\n",
        "        vname = frame.split('/')[0]\n",
        "        data = self.data[self.task][vname][frame]\n",
        "        data['frame'] = frame\n",
        "        data['vid'] = vname\n",
        "        data['label'] = self.data[self.task][vname][frame]['label']\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "            return self.feature_dims"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrIj0S-0YMHv"
      },
      "source": [
        "# Convert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_rLZldeIKPy"
      },
      "source": [
        "## Video to Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckW2n3lNYREr"
      },
      "outputs": [],
      "source": [
        "def vid2aud(video_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    for file_name in tqdm(os.listdir(video_folder)):\n",
        "        video_path = os.path.join(video_folder, file_name)\n",
        "        audio_file_name = os.path.splitext(file_name)[0] + '.wav'\n",
        "        audio_file_path = os.path.join(output_folder, audio_file_name)\n",
        "        if os.path.exists(audio_file_path):\n",
        "            continue\n",
        "        if os.path.isfile(video_path) and file_name.lower().endswith(('.mp4', '.mov', '.avi', '.mkv')):\n",
        "            video_clip = VideoFileClip(video_path)\n",
        "            audio_clip = video_clip.audio\n",
        "            audio_clip.write_audiofile(audio_file_path)\n",
        "            audio_clip.close()\n",
        "            video_clip.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrIqAzrZfvy6",
        "outputId": "9961f33f-c3f2-444c-8ed8-5e3214d7456e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing /content/drive/MyDrive/MSc/Thesis/data/video/batch1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 475/475 [00:02<00:00, 192.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing /content/drive/MyDrive/MSc/Thesis/data/video/batch2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 73/73 [00:00<00:00, 2263.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing /content/drive/MyDrive/MSc/Thesis/data/video/new_vids\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 540.31it/s]\n"
          ]
        }
      ],
      "source": [
        "for i in ['batch1','batch2','new_vids']:\n",
        "    video_folder= os.path.join(root,'data/video', i)\n",
        "    print(f'\\nProcessing {video_folder}')\n",
        "    output_folder= os.path.join(root, 'data/audio')\n",
        "    vid2aud(video_folder, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtRjOH5jKB6S"
      },
      "source": [
        "## Audio to Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7dzFPvZ6Mq-"
      },
      "outputs": [],
      "source": [
        "class Fuxi():\n",
        "    def __init__(self, max_duration=50):\n",
        "        self.appkey = 'phr-fuxi'\n",
        "        self.appsecret = '74c72dee-bf9a-4de2-8c1f-96be1a1ecabd'\n",
        "        self.url = 'http://api-test.vop.netease.com/phone_rec'\n",
        "        self.max_duration = max_duration\n",
        "\n",
        "    def run(self, wav_path, lang='en', type=\"wav\",max_duration=50):\n",
        "        words = []\n",
        "\n",
        "        audio = AudioSegment.from_file(wav_path, format='mp3')\n",
        "        duration_ms = len(audio)\n",
        "        if duration_ms < 500:\n",
        "            return ''\n",
        "        chunk_length_ms = max_duration * 1000  # 60s\n",
        "        text = ''\n",
        "        long_vid = []\n",
        "        for i in range(0, duration_ms,chunk_length_ms):\n",
        "            new_audio = audio[i:min(i+chunk_length_ms,duration_ms)]\n",
        "            byte_io = BytesIO()\n",
        "            new_audio.export(byte_io, format=\"wav\")\n",
        "            speech = byte_io.getvalue()\n",
        "\n",
        "            curtime = str(int(time.time()))\n",
        "            hl = hashlib.md5()\n",
        "            hl.update((self.appkey + curtime).encode(encoding='utf-8'))\n",
        "            sign = hmac.new(self.appsecret.encode('utf-8'),\n",
        "                            hl.hexdigest().encode('utf-8'), hashlib.sha1).digest()\n",
        "            checksum = base64.b64encode(sign)\n",
        "            params = {'appkey': self.appkey, 'lan': lang}\n",
        "            headers = {\n",
        "                'curtime': curtime,\n",
        "                'checksum': checksum,\n",
        "                'content-type': 'audio/wav',\n",
        "                'cuid': 'fuxi-avatarlib'\n",
        "            }\n",
        "            response = requests.post(self.url,\n",
        "                                        params=params,\n",
        "                                        headers=headers,\n",
        "                                        data=speech)\n",
        "            r = response.json()\n",
        "            if r['ret_code'] != 1:\n",
        "                #error = r['ret_msg']\n",
        "                #raise RuntimeError('Hangyan Rec Error: ' + error+f'[{wav_path}]')\n",
        "                long_vid.append(wav_path)\n",
        "                pass\n",
        "            aligned_text = r['result']\n",
        "            text += ' '.join([w['word'] for w in aligned_text if w['word'] != 'sil'])\n",
        "        return text\n",
        "\n",
        "    def split_wave(self, wav_path, save_root, max_duration=50):\n",
        "        wav_name = wav_path.split('/')[-1].replace('.wav', '')\n",
        "        fin = wave.open(wav_path, 'rb')\n",
        "        fs_orig = fin.getframerate()\n",
        "        audio_length = fin.getnframes() * (1/fs_orig)\n",
        "        fin.close()\n",
        "\n",
        "        audio = AudioSegment.from_wav(wav_path)\n",
        "        n = int(audio_length//max_duration + 1)\n",
        "        for i in range(n):\n",
        "            new_audio = audio[i*max_duration*1000:(i+1)*max_duration*1000]\n",
        "            new_audio.export(os.path.join(save_root, wav_name+f'_{i}.wav'), format='wav')\n",
        "        return n\n",
        "\n",
        "    def read_wave(self, wav_path, max_duration=60):\n",
        "        with open(wav_path, 'rb') as f:\n",
        "            wav_data = f.read()\n",
        "        return wav_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT2vPUqjpfrT"
      },
      "outputs": [],
      "source": [
        "def aud2text():\n",
        "\n",
        "    speech2text = Fuxi()\n",
        "    print('loading model')\n",
        "    root_wav = os.path.join(root, 'data/audio')\n",
        "    wav_files = os.listdir(root_wav)[::-1]\n",
        "\n",
        "    print(f'processing {root_wav}')\n",
        "    save_folder = os.path.join(root, f'models/ABAW6/aud2text')\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "    res_dict = {}\n",
        "    for audio_name in tqdm(wav_files):\n",
        "        audio_file = os.path.join(root_wav, audio_name)\n",
        "        save_path = os.path.join(save_folder, audio_name.split('.')[0]+ '.json')\n",
        "        if os.path.exists(save_path):\n",
        "            with open(save_path, 'r') as f:\n",
        "                text = json.load(f)\n",
        "            continue\n",
        "        else:\n",
        "            text = speech2text.run(audio_file)\n",
        "            with open(save_path, 'w') as f:\n",
        "                json.dump(text, f)\n",
        "        res_dict[audio_name] = text.strip()\n",
        "\n",
        "    with open(os.path.join(save_folder,'alltext.json'), 'w') as f:\n",
        "        json.dump(res_dict, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTcsP9fAKhey",
        "outputId": "c1ec7628-4d63-4000-f8c2-f4de4a3bbc3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading model\n",
            "processing /content/drive/MyDrive/MSc/Thesis/data/audio\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 173/173 [52:28<00:00, 18.20s/it]\n"
          ]
        }
      ],
      "source": [
        "aud2text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97AGawC8PnRo"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZvRVuFzAkfc"
      },
      "source": [
        "### Compute loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrddZOWU9WGc"
      },
      "outputs": [],
      "source": [
        "def compute_EXP_loss(pred, label, weights):\n",
        "    cri_exp = nn.CrossEntropyLoss(weights)\n",
        "    cls_loss = cri_exp(pred, label)\n",
        "    return cls_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_AU_loss(pred, label, class_weights):\n",
        "    class_weights = torch.from_numpy(class_weights).to(device)\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, label.float(), reduction='none')\n",
        "    weights = (class_weights[:, 0]**(1 - label)) * (class_weights[:, 1]**label)\n",
        "    weighted_bce = bce * weights\n",
        "    loss = torch.mean(weighted_bce)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "C3HV86yF6N0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2BwARxBRcwF"
      },
      "outputs": [],
      "source": [
        "def CCC_loss(x, y):\n",
        "    x, y = x.view(-1), y.view(-1)\n",
        "    vx = x - torch.mean(x)\n",
        "    vy = y - torch.mean(y)\n",
        "    rho =  torch.sum(vx * vy) / (torch.sqrt(torch.sum(torch.pow(vx, 2))) * torch.sqrt(torch.sum(torch.pow(vy, 2)))+1e-8)\n",
        "    x_m, y_m = torch.mean(x), torch.mean(y)\n",
        "    x_s, y_s = torch.std(x), torch.std(y)\n",
        "    ccc = 2*rho*x_s*y_s/(torch.pow(x_s, 2) + torch.pow(y_s, 2) + torch.pow(x_m - y_m, 2))\n",
        "    return 1-ccc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7kdT2piBc_G"
      },
      "outputs": [],
      "source": [
        "def compute_VA_loss(Vout,Aout,label):\n",
        "    ccc_loss = CCC_loss(Vout[:,0],label[:,0]) + CCC_loss(Aout[:,0],label[:,1])\n",
        "    mse_loss = nn.MSELoss()(Vout,label[:,0]) + nn.MSELoss()(Aout,label[:,1])\n",
        "    return mse_loss,ccc_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd_uu91kAhJE"
      },
      "source": [
        "### Compute F1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhrQC_kWxaqa"
      },
      "outputs": [],
      "source": [
        "def compute_EXP_F1(pred, target):\n",
        "    pred_labels = np.argmax(pred, axis=1)\n",
        "    target_labels = np.argmax(target, axis=1)\n",
        "    macro_f1 = f1_score(target_labels,pred_labels,average='macro')\n",
        "    acc = accuracy_score(target_labels, pred_labels)\n",
        "    return macro_f1, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke0-RvC2mi69"
      },
      "outputs": [],
      "source": [
        "def f1s_max_AU(label, pred, thresh, i=0):\n",
        "    pred = np.array(pred)\n",
        "    label = np.array(label)\n",
        "    label = label[:,i]\n",
        "    pred = pred[:,i]\n",
        "    acc = []\n",
        "    F1 = []\n",
        "    for i in thresh:\n",
        "        new_pred = ((pred >= i) * 1).flatten()\n",
        "        acc.append(accuracy_score(label.flatten(), new_pred))\n",
        "        F1.append(f1_score(label.flatten(), new_pred))\n",
        "\n",
        "    F1_MAX = max(F1)\n",
        "    if F1_MAX < 0 or math.isnan(F1_MAX):\n",
        "        F1_MAX = 0\n",
        "        F1_THRESH = 0\n",
        "        accuracy = 0\n",
        "    else:\n",
        "        idx_thresh = np.argmax(F1)\n",
        "        F1_THRESH = thresh[idx_thresh]\n",
        "        accuracy = acc[idx_thresh]\n",
        "    return F1, F1_MAX, F1_THRESH, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdDnfAtpmuKr"
      },
      "outputs": [],
      "source": [
        "def compute_AU_F1(pred,label,thresh=np.arange(0.1,1,0.1)):\n",
        "    F1s = []\n",
        "    F1t = []\n",
        "    acc = []\n",
        "    for i in range(12):\n",
        "        F1, F1_MAX, F1_THRESH, accuracy = f1s_max_AU(label,pred,thresh,i)\n",
        "        F1s.append(F1_MAX)\n",
        "        F1t.append(F1_THRESH)\n",
        "        acc.append(accuracy)\n",
        "    acc = [round(a,3) for a in acc]\n",
        "    return np.mean(F1s),np.mean(F1t),acc, F1t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSYSCjAnLcf3"
      },
      "source": [
        "### Concordance Correlation Coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDWknJ9zKyQA"
      },
      "outputs": [],
      "source": [
        "def CCC_score(x, y):\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    vx = x - np.mean(x)\n",
        "    vy = y - np.mean(y)\n",
        "    rho = np.sum(vx * vy) / (np.sqrt(np.sum(vx**2)) * np.sqrt(np.sum(vy**2)))\n",
        "    x_m = np.mean(x)\n",
        "    y_m = np.mean(y)\n",
        "    x_s = np.std(x)\n",
        "    y_s = np.std(y)\n",
        "    ccc = 2*rho*x_s*y_s/(x_s**2 + y_s**2 + (x_m - y_m)**2)\n",
        "    return ccc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMqwnXAhKLcO"
      },
      "outputs": [],
      "source": [
        "def compute_VA_CCC(x,y):\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    x[x>1] = 1\n",
        "    x[x<-1] = -1\n",
        "    ccc1 = CCC_score(x[:,0],y[:,0])\n",
        "    ccc2 = CCC_score(x[:,1],y[:,1])\n",
        "\n",
        "    return ccc1,ccc2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmzvE5czs3hX"
      },
      "source": [
        "# Smooth utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9Nyajzm5_ww"
      },
      "outputs": [],
      "source": [
        "def smooth_prediction(img, predict):\n",
        "    cur_ind = 0\n",
        "    preds_proba = []\n",
        "    if img:\n",
        "        for i in range(img[-1]):\n",
        "            if img[cur_ind] - 1 == i:\n",
        "                preds_proba.append(predict[cur_ind])\n",
        "                cur_ind += 1\n",
        "            else:\n",
        "                if cur_ind == 0:\n",
        "                    preds_proba.append(predict[cur_ind])\n",
        "                else:\n",
        "                    w = (i - img[cur_ind - 1] + 1) / (img[cur_ind] - img[cur_ind - 1])\n",
        "                    pred = w * predict[cur_ind - 1] + (1 - w) * predict[cur_ind]\n",
        "                    preds_proba.append(pred)\n",
        "        try:\n",
        "            preds_proba = np.array([p.cpu().detach().numpy() for p in preds_proba])\n",
        "        except:\n",
        "            preds_proba = np.array(preds_proba)\n",
        "        return preds_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y-D0QuO9ubK"
      },
      "outputs": [],
      "source": [
        "def slide_window(preds_proba, i, delta, typ):\n",
        "    i1 = max(i - delta, 0)\n",
        "    if typ == 'mean':\n",
        "        proba = np.mean(preds_proba[i1:i+delta+1], axis=0)\n",
        "    elif typ == 'median':\n",
        "        proba = np.median(preds_proba[i1:i+delta+1], axis=0)\n",
        "    else:\n",
        "        proba = np.median(preds_proba[i1:i+delta+1:int(typ)], axis=0)\n",
        "    return np.argmax(proba), proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwMmqGyJwnZg"
      },
      "source": [
        "# Challenges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhIC4geYv9HJ"
      },
      "outputs": [],
      "source": [
        "task = ['EXPR_Recognition_Challenge','AU_Detection_Challenge','VA_Estimation_Challenge']\n",
        "split = ['Train_Set', 'Validation_Set']\n",
        "typ = ['Train','Val','Test']\n",
        "vis_typ = ['cropped_aligned', 'cropped', 'cropped_aligned_b0']\n",
        "visual_feat = 'visualfeat_enet_b2_8_best'\n",
        "visual_feat_1 = 'visualfeat_enet_b0_8_va_mtl'\n",
        "audio_feat = ['audiofeat_wav2vec2','audiofeat_vggish','nope']\n",
        "vis_aud = ['visual_wav2vec2','visual_vggish','visual']\n",
        "batch_size = 32\n",
        "model_type = ['fusion', 'mlp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoofZGhrU6Ih"
      },
      "source": [
        "## EXPR Recognition Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubi-WwKO6a1R"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBMZbYSi7ya1"
      },
      "outputs": [],
      "source": [
        "# Cropped_aligned images\n",
        "vis = vis_typ[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56gxaRmr5pIT"
      },
      "source": [
        "#### Effnet + wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPSj-MXy5pIT"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[0]\n",
        "viau = vis_aud[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx7Mwlg35pIT"
      },
      "source": [
        "#### Effnet + vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZpK72FP5pIT"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[1]\n",
        "viau = vis_aud[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM4A0l6x5pIU"
      },
      "source": [
        "#### Effnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5Utm-WR5pIU"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[2]\n",
        "viau = vis_aud[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLXBg1h55pIU"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5QxjJ9WOFMS"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{task[0]}_{typ[0]}_{viau}.pkl'), 'rb') as f:\n",
        "    data1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAG_gkOP6Fts"
      },
      "outputs": [],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[0]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[0]}/{typ[0]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data1[task1][vname])\n",
        "    for img in data1[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEcd8nuV5pIU"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data1, iname, dims, task1)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAlXrkVW5pIV"
      },
      "source": [
        "#### Val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by-jdZt55pIV"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{task[0]}_{typ[1]}_{viau}.pkl'), 'rb') as f:\n",
        "    data2 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Drw-STzDV8yp"
      },
      "outputs": [],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[0]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[0]}/{typ[1]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data2[task1][vname])\n",
        "    for img in data2[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnMKSN3l5pIV"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data2, iname, dims, task1)\n",
        "val_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekKJDTam5pIV"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0wIAuSo5pIV"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{task[0]}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n",
        "    data3 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPKBKszq5pIV",
        "outputId": "440ca65f-4ace-4f25-fc1a-b0d863da2078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [00:00<00:00, 1553.79it/s]\n"
          ]
        }
      ],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[0]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[0]}/{typ[2]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data3[task1][vname])\n",
        "    for img in data3[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBPEYhxI5pIW"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data3, iname, dims, task1)\n",
        "test_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQsS25mKxpIu"
      },
      "source": [
        "#### Another way (Effnet + wav2vec2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaTxGab6qik1"
      },
      "outputs": [],
      "source": [
        "print(f'{task[0]}')\n",
        "print(f'loading {typ[0]}')\n",
        "dataset = ABAW_dataset(root, split[0], typ[0], task[0], feature_v=visual_feat, feature_a=audio_feat)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
        "torch.save(loader, os.path.join(root,f'models/ABAW6/{task[0]}_{typ[0]}.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-8VBt_BqlKc"
      },
      "outputs": [],
      "source": [
        "print(f'{task[0]}')\n",
        "print(f'loading {typ[1]}')\n",
        "dataset = ABAW_dataset(root, split[0], typ[1], task[0], feature_v=visual_feat, feature_a=audio_feat)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
        "torch.save(loader, os.path.join(root,f'models/ABAW6/{task[0]}_{typ[1]}.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtJ6FD4IqmWl"
      },
      "outputs": [],
      "source": [
        "print(f'{task[0]}')\n",
        "print(f'loading {typ[2]}')\n",
        "dataset = ABAW_dataset(root, split[1], typ[2], task[0], feature_v=visual_feat, feature_a=audio_feat)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2,drop_last=True)\n",
        "torch.save(loader, os.path.join(root,f'models/ABAW6/{task[0]}_{typ[2]}.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsK6_d4rmPmm"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.load(os.path.join(root,f'models/ABAW6/EXPR/{task[0]}_{typ[0]}.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ2giDx1bH-_"
      },
      "outputs": [],
      "source": [
        "val_loader = torch.load(os.path.join(root,f'models/ABAW6/EXPR/{task[0]}_{typ[1]}.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W43ekmn8DLLf"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.load(os.path.join(root,f'models/ABAW6/EXPR/{task[0]}_{typ[2]}.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5qpMbpk6faK"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer encoder"
      ],
      "metadata": {
        "id": "XP9zBPUAWfHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKhsZIv4Wod4"
      },
      "outputs": [],
      "source": [
        "class EXP_fusion(nn.Module):\n",
        "    def __init__(self, batchsize = batch_size, audio_ft = auft, hidden_size = [512, 128, batch_size]):\n",
        "        super(EXP_fusion, self).__init__()\n",
        "        self.batchsize = batchsize\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.hidden_size = hidden_size\n",
        "        self.feat_fc = nn.Conv1d(self.concat_dim, hidden_size[0], 1, padding=0)\n",
        "        self.activ = nn.LeakyReLU(0.1)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.conv1 = nn.Conv1d(hidden_size[0], hidden_size[1], 1, padding=0)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size[1], nhead=4, dim_feedforward=hidden_size[1], dropout=0.3)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "        self.head = nn.Sequential(\n",
        "                nn.Linear(hidden_size[1], hidden_size[2]),\n",
        "                nn.BatchNorm1d(hidden_size[2]),\n",
        "                nn.Dropout(p=0.3),\n",
        "                nn.Linear(hidden_size[2], 8))\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs,dim=1)\n",
        "        feat = torch.transpose(feat,0,1)\n",
        "        feat = self.feat_fc(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.conv1(feat)\n",
        "        out = torch.transpose(out,0,1)\n",
        "        out = self.transformer_encoder(out)\n",
        "        out = self.head(out)\n",
        "\n",
        "        return out, torch.softmax(out, dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p25D4iquOI0I",
        "outputId": "efe97635-2128-4014-ad1d-7d9593015a47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EXP_fusion(\n",
              "  (feat_fc): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n",
              "  (activ): LeakyReLU(negative_slope=0.1)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=32, out_features=8, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "EXP_model = EXP_fusion().to(device)\n",
        "EXP_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP"
      ],
      "metadata": {
        "id": "ZKCxyUITWjBw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvTwWPbHdiqK"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, audio_ft = auft, num_classes=8):\n",
        "        super(MLPModel, self).__init__()\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.activ = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(self.concat_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs, dim=1)\n",
        "        feat = self.fc1(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.fc2(feat)\n",
        "        return out, torch.softmax(out, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtPaQhzHfHkD",
        "outputId": "9382ce1a-8fbf-41bb-eef0-17d0f304def6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPModel(\n",
              "  (activ): ReLU()\n",
              "  (fc1): Linear(in_features=2176, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYow-aj8W6E8"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = []\n",
        "iterator = iter(train_loader)\n",
        "i = 0\n",
        "while True:\n",
        "    try:\n",
        "        EXPR = next(iterator)\n",
        "        y_train.extend(EXPR['label'].numpy())\n",
        "    except:\n",
        "        break"
      ],
      "metadata": {
        "id": "3Oi5prmviKia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights=compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "weights=torch.tensor(class_weights,dtype=torch.float).to(device)\n",
        "print(f'{weights = }')"
      ],
      "metadata": {
        "id": "Ex3R32ODiMOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z-Yc8Zz4A7_"
      },
      "outputs": [],
      "source": [
        "weights = [0.4260, 4.1334, 5.9334, 6.8653, 0.7257, 0.9578, 2.0518, 0.4572]\n",
        "weights = torch.tensor(weights).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiUYQl4UvwV6"
      },
      "outputs": [],
      "source": [
        "def one_hot_transfer(label, class_num):\n",
        "    one_hot = torch.eye(class_num)\n",
        "    one_hot = one_hot.to(device)\n",
        "    return one_hot[label]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjBHb1zf6Eyg"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, EXP_model.parameters()), lr=0.00001, betas=(0.9, 0.999), weight_decay=0.00005)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgZJkGfnysSq"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, mlp_model.parameters()), lr=0.00001, betas=(0.9, 0.999), weight_decay=0.00005)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "BZKD5Ml4idoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, mod_type, train_loader, val_loader, epoch, batch_size, optim, au_feat, weight, vi_au):\n",
        "\n",
        "    model.train(True)\n",
        "    model.eval()\n",
        "    best_loss = float('inf')\n",
        "    f1best, accbest = 0, 0\n",
        "    loss_value = []\n",
        "    loss_train = []\n",
        "    loss_val = []\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for e in range(epoch):\n",
        "        print(f'Epoch: {e+1}')\n",
        "        iterator = iter(train_loader)\n",
        "        while True:\n",
        "            try:\n",
        "                EXPR = next(iterator)\n",
        "                if au_feat == 'nope':\n",
        "                    vis_feat, y = EXPR[visual_feat], EXPR['label']\n",
        "                    vis_feat, y = vis_feat.to(device), y.to(device)\n",
        "                    aud_feat = None\n",
        "                else:\n",
        "                    vis_feat, aud_feat, y = EXPR[visual_feat], EXPR[au_feat], EXPR['label']\n",
        "                    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "                y_onehot = one_hot_transfer(y, 8).to(device)\n",
        "                model.zero_grad()\n",
        "                pred, exp_pred = model(vis_feat, aud_feat)\n",
        "                loss = compute_EXP_loss(pred, y_onehot, weight)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                loss_value.append(loss.item())\n",
        "                all_preds.extend(exp_pred.cpu().tolist())\n",
        "                all_targets.extend(y_onehot.cpu().tolist())\n",
        "            except:\n",
        "                break\n",
        "        avg_loss = round(np.mean(loss_value),3)\n",
        "        loss_train.append(avg_loss)\n",
        "        f1_scores, accuracy = compute_EXP_F1(all_preds, all_targets)\n",
        "        print(f'Train Loss: {avg_loss}, Accuracy: {round(accuracy,3)}')\n",
        "\n",
        "        val_loss, f1s, acc = evaluate_model(model, val_loader, au_feat, weight)\n",
        "        loss_val.append(val_loss)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_{mod_type}_{vi_au}.pth'))\n",
        "            f1best = f1s\n",
        "            accbest = acc\n",
        "\n",
        "        print(f'Validation Loss: {val_loss}, Accuracy: {acc}')\n",
        "        scheduler.step(val_loss)\n",
        "    return loss_train, loss_val, best_loss, f1best, accbest"
      ],
      "metadata": {
        "id": "8GW-dUAJifcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, au_feat, weight):\n",
        "    model.eval()\n",
        "    total_loss = []\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        iterator = iter(data_loader)\n",
        "        for EXPR in iterator:\n",
        "            if au_feat == 'nope':\n",
        "                vis_feat, y = EXPR[visual_feat], EXPR['label']\n",
        "                vis_feat, y = vis_feat.to(device), y.to(device)\n",
        "                aud_feat = None\n",
        "            else:\n",
        "                vis_feat, aud_feat, y = EXPR[visual_feat], EXPR[au_feat], EXPR['label']\n",
        "                vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "            y_onehot = one_hot_transfer(y, 8).to(device)\n",
        "            pred, exp_pred = model(vis_feat, aud_feat)\n",
        "            loss = compute_EXP_loss(pred, y_onehot, weight)\n",
        "            total_loss.append(loss.item())\n",
        "            all_preds.extend(exp_pred.cpu().tolist())\n",
        "            all_targets.extend(y_onehot.cpu().tolist())\n",
        "\n",
        "    f1_scores, acc = compute_EXP_F1(all_preds, all_targets)\n",
        "    return round(np.mean(total_loss),3), round(f1_scores,3), round(acc,3)"
      ],
      "metadata": {
        "id": "7SJugj3fZCQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmUvIMgNVuel"
      },
      "source": [
        "#### Cropped_aligned images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1h3aTTNLEnT"
      },
      "source": [
        "##### Effnet + Wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv7Tqh0zK_wi",
        "outputId": "4095f809-d3b1-4827-ce25-3ec727577ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.68, Accuracy: 0.737\n",
            "Validation Loss: 1.494, Accuracy: 0.468\n",
            "Epoch: 2\n",
            "Train Loss: 0.497, Accuracy: 0.801\n",
            "Validation Loss: 1.709, Accuracy: 0.459\n",
            "Epoch: 3\n",
            "Train Loss: 0.407, Accuracy: 0.833\n",
            "Validation Loss: 1.757, Accuracy: 0.494\n",
            "Epoch: 4\n",
            "Train Loss: 0.351, Accuracy: 0.853\n",
            "Validation Loss: 1.883, Accuracy: 0.48\n",
            "Epoch: 5\n",
            "Train Loss: 0.31, Accuracy: 0.868\n",
            "Validation Loss: 2.133, Accuracy: 0.484\n",
            "Epoch: 6\n",
            "Train Loss: 0.28, Accuracy: 0.879\n",
            "Validation Loss: 2.186, Accuracy: 0.497\n",
            "Epoch: 7\n",
            "Train Loss: 0.256, Accuracy: 0.888\n",
            "Validation Loss: 2.404, Accuracy: 0.489\n",
            "Epoch: 8\n",
            "Train Loss: 0.236, Accuracy: 0.896\n",
            "Validation Loss: 2.459, Accuracy: 0.471\n",
            "Epoch: 9\n",
            "Train Loss: 0.219, Accuracy: 0.902\n",
            "Validation Loss: 2.515, Accuracy: 0.507\n",
            "Epoch: 10\n",
            "Train Loss: 0.205, Accuracy: 0.908\n",
            "Validation Loss: 2.705, Accuracy: 0.502\n",
            "28min 25s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgGorrmVwhmZ",
        "outputId": "76cddda8-d35e-4393-dc7f-6a518e1adb66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.282, accuracy: 0.468\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n",
        "    EXP_model = EXP_fusion().to(device)\n",
        "    EXP_model.load_state_dict(EXP_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX-9jUd6y2W3",
        "outputId": "4d0e1c29-fd39-4dd5-bb18-8c98e8a66431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 1.274, Accuracy: 0.539\n",
            "Validation Loss: 1.278, Accuracy: 0.443\n",
            "Epoch: 2\n",
            "Train Loss: 1.071, Accuracy: 0.602\n",
            "Validation Loss: 1.366, Accuracy: 0.447\n",
            "Epoch: 3\n",
            "Train Loss: 0.96, Accuracy: 0.64\n",
            "Validation Loss: 1.414, Accuracy: 0.452\n",
            "Epoch: 4\n",
            "Train Loss: 0.885, Accuracy: 0.666\n",
            "Validation Loss: 1.453, Accuracy: 0.456\n",
            "Epoch: 5\n",
            "Train Loss: 0.829, Accuracy: 0.685\n",
            "Validation Loss: 1.481, Accuracy: 0.456\n",
            "Epoch: 6\n",
            "Train Loss: 0.786, Accuracy: 0.701\n",
            "Validation Loss: 1.524, Accuracy: 0.456\n",
            "Epoch: 7\n",
            "Train Loss: 0.751, Accuracy: 0.713\n",
            "Validation Loss: 1.546, Accuracy: 0.452\n",
            "Epoch: 8\n",
            "Train Loss: 0.721, Accuracy: 0.724\n",
            "Validation Loss: 1.563, Accuracy: 0.456\n",
            "Epoch: 9\n",
            "Train Loss: 0.695, Accuracy: 0.733\n",
            "Validation Loss: 1.586, Accuracy: 0.451\n",
            "Epoch: 10\n",
            "Train Loss: 0.673, Accuracy: 0.741\n",
            "Validation Loss: 1.604, Accuracy: 0.45\n",
            "7min 57s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYnPVp9U1T-o",
        "outputId": "b151398f-38db-4ca4-f604-94deabe6e771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.29, accuracy: 0.452\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlxUSwAVzKmC"
      },
      "source": [
        "##### Effnet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yztp8PKzFhp",
        "outputId": "b6c96861-d14b-4f00-8f0b-90bc4934e3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.668, Accuracy: 0.74\n",
            "Validation Loss: 1.387, Accuracy: 0.492\n",
            "Epoch: 2\n",
            "Train Loss: 0.471, Accuracy: 0.805\n",
            "Validation Loss: 1.575, Accuracy: 0.481\n",
            "Epoch: 3\n",
            "Train Loss: 0.373, Accuracy: 0.84\n",
            "Validation Loss: 1.922, Accuracy: 0.473\n",
            "Epoch: 4\n",
            "Train Loss: 0.311, Accuracy: 0.864\n",
            "Validation Loss: 2.178, Accuracy: 0.495\n",
            "Epoch: 5\n",
            "Train Loss: 0.268, Accuracy: 0.881\n",
            "Validation Loss: 2.274, Accuracy: 0.467\n",
            "Epoch: 6\n",
            "Train Loss: 0.236, Accuracy: 0.894\n",
            "Validation Loss: 2.43, Accuracy: 0.495\n",
            "Epoch: 7\n",
            "Train Loss: 0.211, Accuracy: 0.905\n",
            "Validation Loss: 2.607, Accuracy: 0.475\n",
            "Epoch: 8\n",
            "Train Loss: 0.191, Accuracy: 0.913\n",
            "Validation Loss: 2.66, Accuracy: 0.488\n",
            "Epoch: 9\n",
            "Train Loss: 0.175, Accuracy: 0.92\n",
            "Validation Loss: 2.806, Accuracy: 0.477\n",
            "Epoch: 10\n",
            "Train Loss: 0.161, Accuracy: 0.926\n",
            "Validation Loss: 2.994, Accuracy: 0.47\n",
            "29min 23s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO2AH_gYHBXy",
        "outputId": "e107318e-10f7-4d65-8b63-4fbe584132d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.286, accuracy: 0.492\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n",
        "    EXP_model = EXP_fusion().to(device)\n",
        "    EXP_model.load_state_dict(EXP_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_iGBHRhKeVM",
        "outputId": "da26cd7f-bea7-4495-e9cd-45d50c6b30ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 1.417, Accuracy: 0.495\n",
            "Validation Loss: 1.21, Accuracy: 0.416\n",
            "Epoch: 2\n",
            "Train Loss: 1.232, Accuracy: 0.548\n",
            "Validation Loss: 1.213, Accuracy: 0.454\n",
            "Epoch: 3\n",
            "Train Loss: 1.121, Accuracy: 0.585\n",
            "Validation Loss: 1.218, Accuracy: 0.489\n",
            "Epoch: 4\n",
            "Train Loss: 1.043, Accuracy: 0.613\n",
            "Validation Loss: 1.23, Accuracy: 0.493\n",
            "Epoch: 5\n",
            "Train Loss: 0.985, Accuracy: 0.633\n",
            "Validation Loss: 1.229, Accuracy: 0.501\n",
            "Epoch: 6\n",
            "Train Loss: 0.938, Accuracy: 0.649\n",
            "Validation Loss: 1.24, Accuracy: 0.499\n",
            "Epoch: 7\n",
            "Train Loss: 0.9, Accuracy: 0.663\n",
            "Validation Loss: 1.237, Accuracy: 0.506\n",
            "Epoch: 8\n",
            "Train Loss: 0.868, Accuracy: 0.674\n",
            "Validation Loss: 1.252, Accuracy: 0.496\n",
            "Epoch: 9\n",
            "Train Loss: 0.84, Accuracy: 0.683\n",
            "Validation Loss: 1.237, Accuracy: 0.496\n",
            "Epoch: 10\n",
            "Train Loss: 0.816, Accuracy: 0.692\n",
            "Validation Loss: 1.26, Accuracy: 0.499\n",
            "7min 35s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model,model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5Us3nak9BqF",
        "outputId": "88bf4eb9-7df3-4439-fd6e-516dc91fc7fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.309, accuracy: 0.501\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfkTbqlzzLyw"
      },
      "source": [
        "##### Effnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6xR8ij6P9uF",
        "outputId": "d4428b0a-661d-44d3-e420-fab8b2e41977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.7, Accuracy: 0.729\n",
            "Validation Loss: 1.413, Accuracy: 0.46\n",
            "Epoch: 2\n",
            "Train Loss: 0.529, Accuracy: 0.786\n",
            "Validation Loss: 1.596, Accuracy: 0.445\n",
            "Epoch: 3\n",
            "Train Loss: 0.441, Accuracy: 0.816\n",
            "Validation Loss: 1.822, Accuracy: 0.478\n",
            "Epoch: 4\n",
            "Train Loss: 0.385, Accuracy: 0.837\n",
            "Validation Loss: 1.869, Accuracy: 0.472\n",
            "Epoch: 5\n",
            "Train Loss: 0.345, Accuracy: 0.852\n",
            "Validation Loss: 2.031, Accuracy: 0.466\n",
            "Epoch: 6\n",
            "Train Loss: 0.314, Accuracy: 0.863\n",
            "Validation Loss: 2.083, Accuracy: 0.434\n",
            "Epoch: 7\n",
            "Train Loss: 0.289, Accuracy: 0.873\n",
            "Validation Loss: 2.298, Accuracy: 0.472\n",
            "Epoch: 8\n",
            "Train Loss: 0.268, Accuracy: 0.881\n",
            "Validation Loss: 2.361, Accuracy: 0.459\n",
            "Epoch: 9\n",
            "Train Loss: 0.251, Accuracy: 0.888\n",
            "Validation Loss: 2.384, Accuracy: 0.455\n",
            "Epoch: 10\n",
            "Train Loss: 0.236, Accuracy: 0.894\n",
            "Validation Loss: 2.432, Accuracy: 0.464\n",
            "27min 18s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTivl9yf9oqm",
        "outputId": "dd4d0cfa-270e-4f66-8a3b-088ef0f9702c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.262, accuracy: 0.478\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n",
        "    EXP_model = EXP_fusion().to(device)\n",
        "    EXP_model.load_state_dict(EXP_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3REomkYQCwJ",
        "outputId": "76ba3d35-98a8-4f2e-f53a-bff6510c1f71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 1.477, Accuracy: 0.467\n",
            "Validation Loss: 1.214, Accuracy: 0.388\n",
            "Epoch: 2\n",
            "Train Loss: 1.325, Accuracy: 0.505\n",
            "Validation Loss: 1.222, Accuracy: 0.402\n",
            "Epoch: 3\n",
            "Train Loss: 1.232, Accuracy: 0.535\n",
            "Validation Loss: 1.235, Accuracy: 0.412\n",
            "Epoch: 4\n",
            "Train Loss: 1.164, Accuracy: 0.558\n",
            "Validation Loss: 1.25, Accuracy: 0.428\n",
            "Epoch: 5\n",
            "Train Loss: 1.11, Accuracy: 0.577\n",
            "Validation Loss: 1.261, Accuracy: 0.43\n",
            "Epoch: 6\n",
            "Train Loss: 1.065, Accuracy: 0.593\n",
            "Validation Loss: 1.281, Accuracy: 0.429\n",
            "Epoch: 7\n",
            "Train Loss: 1.027, Accuracy: 0.606\n",
            "Validation Loss: 1.278, Accuracy: 0.441\n",
            "Epoch: 8\n",
            "Train Loss: 0.995, Accuracy: 0.618\n",
            "Validation Loss: 1.291, Accuracy: 0.436\n",
            "Epoch: 9\n",
            "Train Loss: 0.967, Accuracy: 0.628\n",
            "Validation Loss: 1.301, Accuracy: 0.439\n",
            "Epoch: 10\n",
            "Train Loss: 0.942, Accuracy: 0.637\n",
            "Validation Loss: 1.315, Accuracy: 0.436\n",
            "6min 29s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwDZL_5KR9wL",
        "outputId": "f7ffdd18-c6dc-4812-db9b-4793a4bde22d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.275, accuracy: 0.441\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBlwdqAWbShk"
      },
      "source": [
        "#### Cropped images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAaSqtubbShq"
      },
      "source": [
        "##### Effnet + Wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK2A9RyQbShq",
        "outputId": "a57b4b44-96c0-482d-9893-dffac75ccd20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.742, Accuracy: 0.71\n",
            "Validation Loss: 1.544, Accuracy: 0.483\n",
            "Epoch: 2\n",
            "Train Loss: 0.559, Accuracy: 0.775\n",
            "Validation Loss: 1.756, Accuracy: 0.477\n",
            "Epoch: 3\n",
            "Train Loss: 0.468, Accuracy: 0.807\n",
            "Validation Loss: 1.852, Accuracy: 0.514\n",
            "Epoch: 4\n",
            "Train Loss: 0.409, Accuracy: 0.828\n",
            "Validation Loss: 2.096, Accuracy: 0.512\n",
            "Epoch: 5\n",
            "Train Loss: 0.367, Accuracy: 0.843\n",
            "Validation Loss: 2.13, Accuracy: 0.509\n",
            "Epoch: 6\n",
            "Train Loss: 0.334, Accuracy: 0.855\n",
            "Validation Loss: 2.323, Accuracy: 0.489\n",
            "Epoch: 7\n",
            "Train Loss: 0.308, Accuracy: 0.865\n",
            "Validation Loss: 2.354, Accuracy: 0.506\n",
            "Epoch: 8\n",
            "Train Loss: 0.286, Accuracy: 0.873\n",
            "Validation Loss: 2.506, Accuracy: 0.511\n",
            "Epoch: 9\n",
            "Train Loss: 0.268, Accuracy: 0.88\n",
            "Validation Loss: 2.579, Accuracy: 0.518\n",
            "Epoch: 10\n",
            "Train Loss: 0.252, Accuracy: 0.886\n",
            "Validation Loss: 2.697, Accuracy: 0.503\n",
            "28min 16s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AY7fWH3bShq",
        "outputId": "6f4238e7-31d6-41dd-ba97-0c34ef38d8dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.269, accuracy: 0.518\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n",
        "    EXP_model = EXP_fusion().to(device)\n",
        "    EXP_model.load_state_dict(EXP_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIz06FHpbShq",
        "outputId": "26419a57-3d37-437a-db14-54ef7c037040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 1.372, Accuracy: 0.495\n",
            "Validation Loss: 1.248, Accuracy: 0.425\n",
            "Epoch: 2\n",
            "Train Loss: 1.168, Accuracy: 0.56\n",
            "Validation Loss: 1.331, Accuracy: 0.452\n",
            "Epoch: 3\n",
            "Train Loss: 1.052, Accuracy: 0.6\n",
            "Validation Loss: 1.379, Accuracy: 0.472\n",
            "Epoch: 4\n",
            "Train Loss: 0.973, Accuracy: 0.628\n",
            "Validation Loss: 1.454, Accuracy: 0.467\n",
            "Epoch: 5\n",
            "Train Loss: 0.915, Accuracy: 0.649\n",
            "Validation Loss: 1.492, Accuracy: 0.465\n",
            "Epoch: 6\n",
            "Train Loss: 0.87, Accuracy: 0.665\n",
            "Validation Loss: 1.535, Accuracy: 0.467\n",
            "Epoch: 7\n",
            "Train Loss: 0.833, Accuracy: 0.678\n",
            "Validation Loss: 1.575, Accuracy: 0.464\n",
            "Epoch: 8\n",
            "Train Loss: 0.803, Accuracy: 0.69\n",
            "Validation Loss: 1.607, Accuracy: 0.463\n",
            "Epoch: 9\n",
            "Train Loss: 0.776, Accuracy: 0.699\n",
            "Validation Loss: 1.635, Accuracy: 0.467\n",
            "Epoch: 10\n",
            "Train Loss: 0.754, Accuracy: 0.707\n",
            "Validation Loss: 1.656, Accuracy: 0.472\n",
            "7min 25s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsxN5ONHbShq",
        "outputId": "ddaef3e8-de2b-4dae-a522-57ba11c6a45e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.308, accuracy: 0.472\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XrIUk5abShr"
      },
      "source": [
        "##### Effnet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd0X2UGxbShr",
        "outputId": "2f7e9501-301e-4f03-a76f-4a19b18a4145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.779, Accuracy: 0.695\n",
            "Validation Loss: 1.356, Accuracy: 0.511\n",
            "Epoch: 2\n",
            "Train Loss: 0.586, Accuracy: 0.759\n",
            "Validation Loss: 1.504, Accuracy: 0.512\n",
            "Epoch: 3\n",
            "Train Loss: 0.484, Accuracy: 0.794\n",
            "Validation Loss: 1.683, Accuracy: 0.509\n",
            "Epoch: 4\n",
            "Train Loss: 0.416, Accuracy: 0.818\n",
            "Validation Loss: 1.844, Accuracy: 0.507\n",
            "Epoch: 5\n",
            "Train Loss: 0.366, Accuracy: 0.836\n",
            "Validation Loss: 2.024, Accuracy: 0.501\n",
            "Epoch: 6\n",
            "Train Loss: 0.329, Accuracy: 0.85\n",
            "Validation Loss: 2.264, Accuracy: 0.495\n",
            "Epoch: 7\n",
            "Train Loss: 0.298, Accuracy: 0.862\n",
            "Validation Loss: 2.425, Accuracy: 0.493\n",
            "Epoch: 8\n",
            "Train Loss: 0.273, Accuracy: 0.872\n",
            "Validation Loss: 2.425, Accuracy: 0.5\n",
            "Epoch: 9\n",
            "Train Loss: 0.252, Accuracy: 0.881\n",
            "Validation Loss: 2.808, Accuracy: 0.493\n",
            "Epoch: 10\n",
            "Train Loss: 0.235, Accuracy: 0.889\n",
            "Validation Loss: 2.79, Accuracy: 0.497\n",
            "28min 26s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tCmcYL3bShr",
        "outputId": "daa79001-045e-41a8-c238-cce1315a2003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.295, accuracy: 0.511\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n",
        "    EXP_model = EXP_fusion().to(device)\n",
        "    EXP_model.load_state_dict(EXP_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeWlEc2jbShr",
        "outputId": "b48b0ddb-d43c-4dc5-fa43-37a6fe7c4942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 1.533, Accuracy: 0.458\n",
            "Validation Loss: 1.177, Accuracy: 0.416\n",
            "Epoch: 2\n",
            "Train Loss: 1.36, Accuracy: 0.506\n",
            "Validation Loss: 1.188, Accuracy: 0.46\n",
            "Epoch: 3\n",
            "Train Loss: 1.251, Accuracy: 0.541\n",
            "Validation Loss: 1.207, Accuracy: 0.484\n",
            "Epoch: 4\n",
            "Train Loss: 1.172, Accuracy: 0.568\n",
            "Validation Loss: 1.227, Accuracy: 0.494\n",
            "Epoch: 5\n",
            "Train Loss: 1.112, Accuracy: 0.588\n",
            "Validation Loss: 1.247, Accuracy: 0.499\n",
            "Epoch: 6\n",
            "Train Loss: 1.064, Accuracy: 0.605\n",
            "Validation Loss: 1.265, Accuracy: 0.502\n",
            "Epoch: 7\n",
            "Train Loss: 1.025, Accuracy: 0.618\n",
            "Validation Loss: 1.282, Accuracy: 0.504\n",
            "Epoch: 8\n",
            "Train Loss: 0.992, Accuracy: 0.63\n",
            "Validation Loss: 1.298, Accuracy: 0.505\n",
            "Epoch: 9\n",
            "Train Loss: 0.964, Accuracy: 0.639\n",
            "Validation Loss: 1.312, Accuracy: 0.506\n",
            "Epoch: 10\n",
            "Train Loss: 0.939, Accuracy: 0.648\n",
            "Validation Loss: 1.326, Accuracy: 0.507\n",
            "7min 37s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model,model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jdj4ROSbShr",
        "outputId": "b6ccfa94-c870-44ea-d713-590e9a34934f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.308, accuracy: 0.494\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ki-roz0bShr"
      },
      "source": [
        "##### Effnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSZM8RPjbShr",
        "outputId": "1704f652-0649-4ff7-baaf-86473a54dbba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.925, Accuracy: 0.644\n",
            "Validation Loss: 1.389, Accuracy: 0.506\n",
            "Epoch: 2\n",
            "Train Loss: 0.745, Accuracy: 0.704\n",
            "Validation Loss: 1.538, Accuracy: 0.492\n",
            "Epoch: 3\n",
            "Train Loss: 0.65, Accuracy: 0.735\n",
            "Validation Loss: 1.683, Accuracy: 0.485\n",
            "Epoch: 4\n",
            "Train Loss: 0.586, Accuracy: 0.757\n",
            "Validation Loss: 1.792, Accuracy: 0.488\n",
            "Epoch: 5\n",
            "Train Loss: 0.539, Accuracy: 0.772\n",
            "Validation Loss: 1.888, Accuracy: 0.486\n",
            "Epoch: 6\n",
            "Train Loss: 0.501, Accuracy: 0.785\n",
            "Validation Loss: 1.986, Accuracy: 0.487\n",
            "Epoch: 7\n",
            "Train Loss: 0.47, Accuracy: 0.796\n",
            "Validation Loss: 2.051, Accuracy: 0.487\n",
            "Epoch: 8\n",
            "Train Loss: 0.443, Accuracy: 0.805\n",
            "Validation Loss: 2.161, Accuracy: 0.487\n",
            "Epoch: 9\n",
            "Train Loss: 0.42, Accuracy: 0.813\n",
            "Validation Loss: 2.263, Accuracy: 0.477\n",
            "Epoch: 10\n",
            "Train Loss: 0.4, Accuracy: 0.82\n",
            "Validation Loss: 2.335, Accuracy: 0.495\n",
            "27min 18s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(EXP_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7yJt29ubShr",
        "outputId": "b3a93753-5010-442a-adc5-4a725e443138"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.281, accuracy: 0.506\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_fusion_{viau}.pth'))\n",
        "    EXP_model = EXP_fusion().to(device)\n",
        "    EXP_model.load_state_dict(EXP_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(EXP_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYMTOB5_bShr",
        "outputId": "d995d54f-7c82-4ed8-9e28-be9306de94bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 1.596, Accuracy: 0.408\n",
            "Validation Loss: 1.162, Accuracy: 0.406\n",
            "Epoch: 2\n",
            "Train Loss: 1.466, Accuracy: 0.443\n",
            "Validation Loss: 1.163, Accuracy: 0.441\n",
            "Epoch: 3\n",
            "Train Loss: 1.383, Accuracy: 0.468\n",
            "Validation Loss: 1.18, Accuracy: 0.461\n",
            "Epoch: 4\n",
            "Train Loss: 1.322, Accuracy: 0.489\n",
            "Validation Loss: 1.205, Accuracy: 0.473\n",
            "Epoch: 5\n",
            "Train Loss: 1.272, Accuracy: 0.508\n",
            "Validation Loss: 1.232, Accuracy: 0.478\n",
            "Epoch: 6\n",
            "Train Loss: 1.231, Accuracy: 0.523\n",
            "Validation Loss: 1.257, Accuracy: 0.479\n",
            "Epoch: 7\n",
            "Train Loss: 1.196, Accuracy: 0.536\n",
            "Validation Loss: 1.28, Accuracy: 0.48\n",
            "Epoch: 8\n",
            "Train Loss: 1.166, Accuracy: 0.548\n",
            "Validation Loss: 1.299, Accuracy: 0.48\n",
            "Epoch: 9\n",
            "Train Loss: 1.139, Accuracy: 0.558\n",
            "Validation Loss: 1.316, Accuracy: 0.48\n",
            "Epoch: 10\n",
            "Train Loss: 1.116, Accuracy: 0.567\n",
            "Validation Loss: 1.331, Accuracy: 0.479\n",
            "6min 53s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTdI0AYzbShs",
        "outputId": "22191d28-d2b3-4fcb-f50e-56dda490e935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.298, accuracy: 0.461\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "    val_loss, f1s, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjx794E5bdTe"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUFrGk1S__N4"
      },
      "source": [
        "##### EffNet + Wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHHY7_sZ2Mwd",
        "outputId": "ef6724e9-e65f-4d00-c715-3056723babd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EXP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: f1_score 0.33, accuracy: 0.457\n",
            "34.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('EXP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_fusion_{viau}.pth'))\n",
        "EXP_model = EXP_fusion().to(device)\n",
        "EXP_model.load_state_dict(EXP_best_model)\n",
        "test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH42fbzzP4q-",
        "outputId": "d4c4a8d3-74d4-44d0-8d84-0eccfaacccd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: f1_score 0.394, accuracy: 0.488\n",
            "12.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be0A7dxPAGJx"
      },
      "source": [
        "##### EffNet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbSQ4IBSI_d6",
        "outputId": "0b669b23-713f-462b-a5b3-b96c6b441f31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EXP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: f1_score 0.316, accuracy: 0.475\n",
            "33 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('EXP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_fusion_{viau}.pth'))\n",
        "EXP_model = EXP_fusion().to(device)\n",
        "EXP_model.load_state_dict(EXP_best_model)\n",
        "test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OM1_LawOeZg",
        "outputId": "8c7f2f78-7282-4b65-81dc-17e99598c5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: f1_score 0.379, accuracy: 0.498\n",
            "11.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibOhaO0vAOvH"
      },
      "source": [
        "##### EffNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kcSYFJIZ42k",
        "outputId": "5ca3235e-4d5c-4829-abd6-fc89874bcec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EXP_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: f1_score 0.31, accuracy: 0.447\n",
            "29.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('EXP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_fusion_{viau}.pth'))\n",
        "EXP_model = EXP_fusion().to(device)\n",
        "EXP_model.load_state_dict(EXP_best_model)\n",
        "test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwduIUw-_EUU",
        "outputId": "c09762d9-1623-48eb-cd9c-cdf29739416e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: f1_score 0.327, accuracy: 0.431\n",
            "8.93 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd8sIrCrwZIT"
      },
      "source": [
        "### Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGEXpsL6IsME"
      },
      "outputs": [],
      "source": [
        "tsk = task[0]\n",
        "vis = vis_typ[0]\n",
        "viau = vis_aud[0]\n",
        "auft = audio_feat[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOXjdlJRYS76"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, audio_ft = auft, num_classes=8):\n",
        "        super(MLPModel, self).__init__()\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.activ = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(self.concat_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        inputs = [vis_feat]\n",
        "        inputs.append(aud_feat)\n",
        "        feat = torch.cat(inputs, dim=0)\n",
        "        feat = self.fc1(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.fc2(feat)\n",
        "        return out, torch.softmax(out, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz30ISAQwj-v",
        "outputId": "5b3d3179-2740-431d-b91f-880b1375376b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34D1o-JyEPeb"
      },
      "outputs": [],
      "source": [
        "anno_path = os.path.join(root,f'data/Annotations/{tsk}/{split[1]}')\n",
        "with open(os.path.join(root, f'data/Annotations/{tsk}/{typ[2]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{tsk}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnxZ2o1TD9mB",
        "outputId": "509f09be-1d1b-43af-ad91-73c316880837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [01:22<00:00,  1.18s/it]\n"
          ]
        }
      ],
      "source": [
        "test_vid = {}\n",
        "for vname in tqdm(vidnames):\n",
        "        img, predict, label = [], [], []\n",
        "        for imgname, val in sorted(data[tsk][vname].items()):\n",
        "            vis_feat = torch.tensor(val[visual_feat]).to(device)\n",
        "            if auft == 'nope':\n",
        "                aud_feat = None\n",
        "            else:\n",
        "                aud_feat = torch.tensor(val[auft]).to(device)\n",
        "            if tsk == task[2]:\n",
        "                _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n",
        "                preds = torch.tensor([vpred, apred])\n",
        "            else:\n",
        "                _, pred = mlp_model(vis_feat, aud_feat)\n",
        "                preds = torch.tensor(pred)\n",
        "            ind = int(imgname.split('/')[1][:-4])\n",
        "            img.append(ind)\n",
        "            predict.append(preds)\n",
        "            label.append(data[tsk][vname][imgname]['label'])\n",
        "        test_vid[vname] = (img, predict, label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams=[(isMean,delta) for delta in [0, 5, 15, 30, 60, 100, 200] for isMean in [1,0] if not (isMean==0 and delta==0)]\n",
        "total_true=[]\n",
        "total_preds=[[] for _ in range(len(hyperparams))]\n",
        "timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n",
        "for videoname,(img, predict, label) in test_vid.items():\n",
        "    for i,ind in enumerate(img):\n",
        "        total_true.append(label[i].cpu().numpy())\n",
        "    preds_proba = smooth_prediction(img, predict)\n",
        "    for hInd,(isMean,delta) in enumerate(hyperparams):\n",
        "        preds=[]\n",
        "        start = time.time()\n",
        "        for i in range(len(preds_proba)):\n",
        "            i1=max(i-delta,0)\n",
        "            if isMean:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'mean')\n",
        "            else:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'median')\n",
        "            preds.append(best_ind)\n",
        "        for i,ind in enumerate(img):\n",
        "            if label[i]>=0:\n",
        "                total_preds[hInd].append(preds[ind-1])\n",
        "        end = time.time()\n",
        "        timing_results[(isMean, delta)] += end - start\n",
        "total_true=np.array(total_true)"
      ],
      "metadata": {
        "id": "AWQOiHvVogjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for hInd, (isMean, delta) in enumerate(hyperparams):\n",
        "    preds = np.array(total_preds[hInd])\n",
        "    accuracy = round((preds == total_true).mean(), 3)\n",
        "    f1 = round(f1_score(y_true=total_true, y_pred=preds, average='macro'), 3)\n",
        "    mean_or_median = 'mean' if isMean else 'median'\n",
        "    time_taken = round(timing_results[(isMean, delta)],3)\n",
        "    print(f'{mean_or_median}; delta: {delta}; Acc: {accuracy}; F1: {f1}; Time: {time_taken}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ5rX14EVeuR",
        "outputId": "ad602aff-a5c9-444c-842d-5cbdad72f8a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean; delta: 0; Acc: 0.488; F1: 0.394; Time: 7.318s\n",
            "mean; delta: 5; Acc: 0.512; F1: 0.418; Time: 7.51s\n",
            "median; delta: 5; Acc: 0.509; F1: 0.417; Time: 16.631s\n",
            "mean; delta: 15; Acc: 0.524; F1: 0.429; Time: 7.611s\n",
            "median; delta: 15; Acc: 0.523; F1: 0.43; Time: 17.372s\n",
            "mean; delta: 30; Acc: 0.533; F1: 0.44; Time: 8.184s\n",
            "median; delta: 30; Acc: 0.533; F1: 0.442; Time: 18.457s\n",
            "mean; delta: 60; Acc: 0.537; F1: 0.443; Time: 7.855s\n",
            "median; delta: 60; Acc: 0.538; F1: 0.449; Time: 20.305s\n",
            "mean; delta: 100; Acc: 0.539; F1: 0.448; Time: 8.482s\n",
            "median; delta: 100; Acc: 0.541; F1: 0.454; Time: 22.453s\n",
            "mean; delta: 200; Acc: 0.538; F1: 0.437; Time: 10.033s\n",
            "median; delta: 200; Acc: 0.544; F1: 0.457; Time: 27.948s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for hInd, (isMean, delta) in enumerate(hyperparams):\n",
        "    preds = np.array(total_preds[hInd])\n",
        "    accuracy = round((preds == total_true).mean(), 3)\n",
        "    f1 = round(f1_score(y_true=total_true, y_pred=preds, average='macro'), 3)\n",
        "    mean_or_median = 'mean' if isMean else 'median'\n",
        "    time_taken = round(timing_results[(isMean, delta)],3)\n",
        "    print(f'{mean_or_median}; delta: {delta}; Acc: {accuracy}; F1: {f1}; Time: {time_taken}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AqaXWC7-tL3",
        "outputId": "1d11cfa1-0b6c-4a7a-d3a5-1c2a99dccb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean; delta: 0; Acc: 0.488; F1: 0.394; Time: 9.554s\n",
            "mean; delta: 15; Acc: 0.524; F1: 0.429; Time: 9.97s\n",
            "median; delta: 15; Acc: 0.523; F1: 0.43; Time: 20.216s\n",
            "mean; delta: 30; Acc: 0.533; F1: 0.44; Time: 9.814s\n",
            "median; delta: 30; Acc: 0.533; F1: 0.442; Time: 20.944s\n",
            "mean; delta: 60; Acc: 0.537; F1: 0.443; Time: 9.286s\n",
            "median; delta: 60; Acc: 0.538; F1: 0.449; Time: 22.854s\n",
            "mean; delta: 100; Acc: 0.539; F1: 0.448; Time: 9.671s\n",
            "median; delta: 100; Acc: 0.541; F1: 0.454; Time: 25.641s\n",
            "mean; delta: 200; Acc: 0.538; F1: 0.437; Time: 11.106s\n",
            "median; delta: 200; Acc: 0.544; F1: 0.457; Time: 31.662s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsUw4AxuwtBo"
      },
      "source": [
        "### Adaptive Frame Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67stcFBHvCJh"
      },
      "outputs": [],
      "source": [
        "delta = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG-oVHWuHpFS",
        "outputId": "bcc7eb56-7f9c-4026-ce43-077f3fc99e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199/199 [02:23<00:00,  1.39it/s]\n"
          ]
        }
      ],
      "source": [
        "anno_path = os.path.join(root,f'data/Annotations/{tsk}/{split[0]}')\n",
        "with open(os.path.join(root, f'data/Annotations/{tsk}/{typ[0]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{tsk}_{typ[0]}_{viau}.pkl'), 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "train_vid = {}\n",
        "for vname in tqdm(vidnames):\n",
        "        img, predict, label = [], [], []\n",
        "        for imgname, val in sorted(data[tsk][vname].items()):\n",
        "            vis_feat = torch.tensor(val[visual_feat]).to(device)\n",
        "            if auft == 'nope':\n",
        "                aud_feat = None\n",
        "            else:\n",
        "                aud_feat = torch.tensor(val[auft]).to(device)\n",
        "            if tsk == task[2]:\n",
        "                _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n",
        "                preds = torch.tensor([vpred, apred])\n",
        "            else:\n",
        "                _, pred = mlp_model(vis_feat, aud_feat)\n",
        "                preds = torch.tensor(pred)\n",
        "            ind = int(imgname.split('/')[1][:-4])\n",
        "            img.append(ind)\n",
        "            predict.append(preds)\n",
        "            label.append(data[tsk][vname][imgname]['label'])\n",
        "        train_vid[vname] = (img, predict, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ayc_zt0GHf-V"
      },
      "outputs": [],
      "source": [
        "stride2scores={}\n",
        "for stride in [200, 100, 50, 25, 10]:\n",
        "    total_true, predictions, max_decision_values = [],[],[]\n",
        "    for vidname, (img, predict, label) in train_vid.items():\n",
        "        index = []\n",
        "        for i,ind in enumerate(img):\n",
        "            total_true.append(label[i].cpu().numpy())\n",
        "            index.append(ind-1)\n",
        "        preds_proba = smooth_prediction(img, predict)\n",
        "        for i in range(len(index)):\n",
        "            best_ind, proba = slide_window(preds_proba, index[i], delta, stride)\n",
        "            predictions.append(best_ind)\n",
        "            max_decision_values.append(proba[best_ind])\n",
        "    stride2scores[stride] = (np.array(total_true),np.array(predictions),np.array(max_decision_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvZ1nt6GGzaM"
      },
      "outputs": [],
      "source": [
        "def get_threshold(stride,fpr_corrected):\n",
        "    (total_true,predictions,max_decision_values) = stride2scores[stride]\n",
        "    mistakes = max_decision_values[predictions != total_true]\n",
        "    best_threshold = -1\n",
        "    for i, threshold in enumerate(sorted(max_decision_values[predictions == total_true])[::-1]):\n",
        "        tpr = i/len(predictions)\n",
        "        fpr = (mistakes > threshold).sum()/len(predictions)\n",
        "        if fpr > fpr_corrected:\n",
        "            if best_threshold == -1:\n",
        "                best_threshold = threshold\n",
        "            print(stride, 'best_threshold', best_threshold, i)\n",
        "            break\n",
        "        best_threshold = threshold\n",
        "    return best_threshold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stride2threshold = {}\n",
        "for stride in stride2scores:\n",
        "    fpr_corrected=0.05\n",
        "    stride2threshold[stride] = get_threshold(stride,fpr_corrected)\n",
        "stride2threshold[1] = 0\n",
        "print(stride2threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb_5USiNj1gY",
        "outputId": "24ec0a76-58c9-4078-c431-d42fdd072fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 best_threshold 0.5776766 230724\n",
            "100 best_threshold 0.5594592 241835\n",
            "50 best_threshold 0.54468054 251611\n",
            "25 best_threshold 0.538801 255266\n",
            "10 best_threshold 0.53340304 259316\n",
            "{200: 0.5776766, 100: 0.5594592, 50: 0.54468054, 25: 0.538801, 10: 0.53340304, 1: 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_strides=[\n",
        "    [200, 100, 50, 10, 1],\n",
        "    [50, 25, 1],\n",
        "    [50, 10, 1],\n",
        "    [200,50,1],\n",
        "    [100,50,1],\n",
        "    [200,1],\n",
        "    [100,1],\n",
        "    [50,1]\n",
        "]\n",
        "for s in stride2threshold.keys():\n",
        "    all_strides.append([s])\n",
        "\n",
        "for strides in all_strides:\n",
        "    print(strides)\n",
        "    last_stride=strides[-1]\n",
        "\n",
        "    total_true=[]\n",
        "    total_preds=[]\n",
        "    total_frames_processed,total_frames=0,0\n",
        "    time_each = []\n",
        "    start = time.time()\n",
        "    for videoname, (img, predict, label) in test_vid.items():\n",
        "        emotional_img=[]\n",
        "        start1 = time.time()\n",
        "        for i,ind in enumerate(img):\n",
        "            total_true.append(label[i].cpu().numpy())\n",
        "            emotional_img.append(ind-1)\n",
        "        cur_ind=0\n",
        "        preds_proba=[]\n",
        "        for i in range(img[-1]):\n",
        "            if img[cur_ind]-1==i:\n",
        "                preds_proba.append(predict[cur_ind])\n",
        "                cur_ind+=1\n",
        "            else:\n",
        "                if cur_ind==0:\n",
        "                    preds_proba.append(predict[cur_ind])\n",
        "                else:\n",
        "                    w=(i-img[cur_ind-1]+1)/(img[cur_ind]-img[cur_ind-1])\n",
        "                    pred=w*predict[cur_ind-1]+(1-w)*predict[cur_ind]\n",
        "                    preds_proba.append(pred)\n",
        "\n",
        "        preds_proba=np.array([p.cpu().numpy() for p in preds_proba])\n",
        "\n",
        "        preds=-np.ones(len(emotional_img))\n",
        "        end1 = time.time()\n",
        "        time_each.append(end1 - start1)\n",
        "        for stride in strides:\n",
        "            threshold=stride2threshold[stride]\n",
        "            for i in range(len(emotional_img)):\n",
        "                if preds[i]<0:\n",
        "                    i1=max(emotional_img[i]-delta,0)\n",
        "                    cur_preds=preds_proba[i1:emotional_img[i]+delta+1:stride]\n",
        "                    proba=np.median(cur_preds,axis=0)\n",
        "                    best_ind=np.argmax(proba)\n",
        "                    if proba[best_ind]>=threshold or stride==last_stride:\n",
        "                        total_frames_processed+=len(cur_preds)\n",
        "                        total_frames+=len(preds_proba[i1:emotional_img[i]+delta+1])\n",
        "                        preds[i]=best_ind\n",
        "        for p in preds:\n",
        "            total_preds.append(p)\n",
        "    end = time.time()\n",
        "    elapsed_time = end - start - sum(time_each)\n",
        "    total_true=np.array(total_true)\n",
        "    preds=np.array(total_preds)\n",
        "    print('Acc:',round((preds==total_true).mean(),3), 'F1:',round(f1_score(y_true=total_true,y_pred=preds, average=\"macro\"),3))\n",
        "    print(total_frames_processed,total_frames,round(total_frames_processed/total_frames,3))\n",
        "    print(f\"Time: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fq28CZopD88",
        "outputId": "b4b16d5d-3b7d-4b4d-9bd3-4467e06b8ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200, 100, 50, 10, 1]\n",
            "Acc: 0.543 F1: 0.454\n",
            "66718832 109811197 0.608\n",
            "Time: 50.22 seconds\n",
            "[50, 25, 1]\n",
            "Acc: 0.544 F1: 0.457\n",
            "75888291 109811197 0.691\n",
            "Time: 35.62 seconds\n",
            "[50, 10, 1]\n",
            "Acc: 0.544 F1: 0.457\n",
            "74809429 109811197 0.681\n",
            "Time: 35.97 seconds\n",
            "[200, 50, 1]\n",
            "Acc: 0.543 F1: 0.455\n",
            "72017266 109811197 0.656\n",
            "Time: 34.61 seconds\n",
            "[100, 50, 1]\n",
            "Acc: 0.544 F1: 0.456\n",
            "75050361 109811197 0.683\n",
            "Time: 35.59 seconds\n",
            "[200, 1]\n",
            "Acc: 0.543 F1: 0.455\n",
            "81565887 109811197 0.743\n",
            "Time: 29.01 seconds\n",
            "[100, 1]\n",
            "Acc: 0.544 F1: 0.456\n",
            "80951438 109811197 0.737\n",
            "Time: 28.60 seconds\n",
            "[50, 1]\n",
            "Acc: 0.544 F1: 0.457\n",
            "79911794 109811197 0.728\n",
            "Time: 28.14 seconds\n",
            "[200]\n",
            "Acc: 0.498 F1: 0.411\n",
            "816872 109811197 0.007\n",
            "Time: 13.12 seconds\n",
            "[100]\n",
            "Acc: 0.514 F1: 0.429\n",
            "1364594 109811197 0.012\n",
            "Time: 13.06 seconds\n",
            "[50]\n",
            "Acc: 0.527 F1: 0.443\n",
            "2459930 109811197 0.022\n",
            "Time: 13.21 seconds\n",
            "[25]\n",
            "Acc: 0.536 F1: 0.45\n",
            "4650733 109811197 0.042\n",
            "Time: 13.65 seconds\n",
            "[10]\n",
            "Acc: 0.541 F1: 0.455\n",
            "11223222 109811197 0.102\n",
            "Time: 13.91 seconds\n",
            "[1]\n",
            "Acc: 0.544 F1: 0.457\n",
            "109811197 109811197 1.0\n",
            "Time: 21.66 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zISVpqDy0Io"
      },
      "source": [
        "## AU Detection Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t9KlVut-8gN"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lIzVtIHXjAa"
      },
      "outputs": [],
      "source": [
        "# Cropped_aligned images\n",
        "vis = vis_typ[0]\n",
        "vis1 = vis_typ[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-e2tiWc-8gO"
      },
      "source": [
        "#### Effnet + wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z8HNxRg-8gO"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[0]\n",
        "viau = vis_aud[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEbORJYi-8gO"
      },
      "source": [
        "#### Effnet + vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmDO2-je-8gO"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[1]\n",
        "viau = vis_aud[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWvFn9TS-8gP"
      },
      "source": [
        "#### Effnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZmMqmOj-8gP"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[2]\n",
        "viau = vis_aud[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cSL3k0G-8gP"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if viau == vis_aud[0]: #Visual+wav2vec2\n",
        "    with open(os.path.join(root,f'models/ABAW6/{vis}/AU/{task[1]}_{typ[0]}_visual.pkl'), 'rb') as f:\n",
        "        data1 = pickle.load(f)\n",
        "    with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[0]}.txt'), 'r') as f:\n",
        "        vidnames = f.read().splitlines()\n",
        "    task1 = task[1]\n",
        "    feature_a = 'audiofeat_wav2vec2'\n",
        "    feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n",
        "    filenames = os.listdir(feat_root)[:]\n",
        "    for vname in tqdm(vidnames):\n",
        "            feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n",
        "            for imgname, val in feature.items():\n",
        "                if imgname in data1[task1][vname]:\n",
        "                    data1[task1][vname][imgname].update({f'{feature_a}': val})\n",
        "            for img, value in list(data1[task1][vname].items()):\n",
        "                if len(value) < 3:\n",
        "                    data1[task1][vname].pop(img)\n",
        "else:\n",
        "    with open(os.path.join(root,f'models/ABAW6/{vis}/AU/{task[1]}_{typ[0]}_{viau}.pkl'), 'rb') as f:\n",
        "        data1 = pickle.load(f)"
      ],
      "metadata": {
        "id": "FwxwoVP6zkim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt-McDNz-8gP",
        "outputId": "3f2b2cb7-8481-4544-b580-c24e3f8543ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 236/236 [00:00<00:00, 1347.13it/s]\n"
          ]
        }
      ],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[1]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[0]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data1[task1][vname])\n",
        "    for img in data1[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgDumTlK-8gP"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data1, iname, dims, task1)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDN9v__h-8gP"
      },
      "source": [
        "#### Val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAqHUP3e-8gP"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis}/AU/{task[1]}_{typ[1]}_{viau}.pkl'), 'rb') as f:\n",
        "    data2 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuLYjlyK-8gP",
        "outputId": "ca5e7ae5-0d1c-4747-ee63-a9c66a2362bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 59/59 [00:00<00:00, 1230.53it/s]\n"
          ]
        }
      ],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[1]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[1]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data2[task1][vname])\n",
        "    for img in data2[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYAfwbLp-8gP"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data2, iname, dims, task1)\n",
        "val_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsoLTqkz-8gP"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B2"
      ],
      "metadata": {
        "id": "8vFUXiAyjdSs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuLsvz6J-8gQ"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis}/AU/{task[1]}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n",
        "    data3 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGQmZCVl-8gQ",
        "outputId": "593d5733-59ce-4dde-fa9f-a3e886198af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 105/105 [00:00<00:00, 1203.40it/s]\n"
          ]
        }
      ],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[1]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[2]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data3[task1][vname])\n",
        "    for img in data3[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x8-0eME-8gQ"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data3, iname, dims, task1)\n",
        "test_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B0"
      ],
      "metadata": {
        "id": "47UnYX3TjhOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis1}/AU/{task[1]}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n",
        "    data3 = pickle.load(f)"
      ],
      "metadata": {
        "id": "xaPuN8q4oVOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5MqbxDkH4bi"
      },
      "outputs": [],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[1]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[2]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data3[task1][vname])\n",
        "    for img in data3[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEXe6RgdH4bj"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data3, iname, dims, task1)\n",
        "test_loader_b0 = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKO9rSzdO9t5"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOHiQBFK4DEG"
      },
      "outputs": [],
      "source": [
        "class AU_fusion(nn.Module):\n",
        "    def __init__(self, batchsize = batch_size, audio_ft = auft, hidden_size = [512, 128, batch_size]):\n",
        "        super(AU_fusion, self).__init__()\n",
        "        self.batchsize = batchsize\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.hidden_size = hidden_size\n",
        "        self.feat_fc = nn.Conv1d(self.concat_dim, hidden_size[0], 1, padding=0)\n",
        "        self.activ = nn.LeakyReLU(0.1)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.conv1 = nn.Conv1d(hidden_size[0], hidden_size[1], 1, padding=0)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size[1], nhead=4, dim_feedforward=hidden_size[1], dropout=0.3)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "        self.head = nn.Sequential(\n",
        "                nn.Linear(hidden_size[1], hidden_size[2]),\n",
        "                nn.BatchNorm1d(hidden_size[2]),\n",
        "                nn.Linear(hidden_size[2], 12))\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs,dim=1)\n",
        "        feat = torch.transpose(feat,0,1)\n",
        "        feat = self.feat_fc(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.conv1(feat)\n",
        "        out = torch.transpose(out,0,1)\n",
        "        out = self.transformer_encoder(out)\n",
        "        out = self.head(out)\n",
        "\n",
        "        return out, torch.sigmoid(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7xqRYlKLJn0",
        "outputId": "5dd0908a-4211-4be6-fe80-c8f251a2aff8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AU_fusion(\n",
              "  (feat_fc): Conv1d(1408, 512, kernel_size=(1,), stride=(1,))\n",
              "  (activ): LeakyReLU(negative_slope=0.1)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Linear(in_features=32, out_features=12, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "AU_model = AU_fusion().to(device)\n",
        "AU_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djfaj8fAZVZw"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, audio_ft = auft, num_classes=12):\n",
        "        super(MLPModel, self).__init__()\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.activ = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(self.concat_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs, dim=1)\n",
        "        feat = self.fc1(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.fc2(feat)\n",
        "        return out, torch.sigmoid(out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel_b0(nn.Module):\n",
        "    def __init__(self, audio_ft = auft, num_classes=12):\n",
        "        super(MLPModel_b0, self).__init__()\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2048 #1280+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1408 #1280+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1280 #1280    #visual only\n",
        "        self.activ = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(self.concat_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs, dim=1)\n",
        "        feat = self.fc1(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.fc2(feat)\n",
        "        return out, torch.sigmoid(out)"
      ],
      "metadata": {
        "id": "Et_Mmy63lZHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqfnlKCdKmBf",
        "outputId": "1da8cec3-f8af-4420-80b1-4c31f11f47e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPModel(\n",
              "  (activ): ReLU()\n",
              "  (fc1): Linear(in_features=1536, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLuj2x8-jozH"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeWn8UQRjozI"
      },
      "outputs": [],
      "source": [
        "weights1 = torch.tensor([0.54733899, 0.44180561, 0.56990565, 0.61997328, 0.73956417,0.74692377, 0.72684634, 0.33222808, 0.17383676, 0.20608964, 0.83688068, 0.33890931]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = []\n",
        "iterator = iter(train_loader)\n",
        "i = 0\n",
        "while True:\n",
        "    try:\n",
        "        AU = next(iterator)\n",
        "        y_train.extend(AU['label'].numpy())\n",
        "    except:\n",
        "        break"
      ],
      "metadata": {
        "id": "QC6EAC0i7mb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(y_train)\n",
        "weights = np.empty([12, 2])\n",
        "\n",
        "for i in range(12):\n",
        "    neg, pos = np.bincount(y_train[:, i])\n",
        "    total = neg + pos\n",
        "    weight_for_0 = (1 / neg) * (total / 2.0) if neg > 0 else 0\n",
        "    weight_for_1 = (1 / pos) * (total / 2.0) if pos > 0 else 0\n",
        "\n",
        "    weights[i][0] = weight_for_0\n",
        "    weights[i][1] = weight_for_1\n",
        "print(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMuFUFuW8e8R",
        "outputId": "96840026-a399-45a6-9c44-c5ad5894cc9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.57226378  3.95954782]\n",
            " [ 0.52683025  9.81784119]\n",
            " [ 0.59477198  3.13791059]\n",
            " [ 0.67850865  1.90049238]\n",
            " [ 0.82921345  1.25938575]\n",
            " [ 0.75994361  1.46174704]\n",
            " [ 0.65729978  2.089322  ]\n",
            " [ 0.51501945 17.1450862 ]\n",
            " [ 0.51587402 16.24900868]\n",
            " [ 0.51460027 17.62297096]\n",
            " [ 1.33900704  0.79797128]\n",
            " [ 0.54113295  6.57785288]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRmF5dylrBsF"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, AU_model.parameters()), lr=0.00001, betas=(0.9, 0.999), weight_decay=0.00005)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zELaPXWaKuzJ"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, mlp_model.parameters()), lr=0.0001, betas=(0.9, 0.999), weight_decay=0.00005)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "oV-pm2FnjsNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, mod_type, train_loader, val_loader, epoch, batch_size, optim, au_feat, weight, vi_au):\n",
        "    model.train(True)\n",
        "    model.eval()\n",
        "    best_loss = float('inf')\n",
        "    f1s_best, accbest = 0, 0\n",
        "    loss_value = []\n",
        "    loss_train = []\n",
        "    loss_val = []\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for e in range(epoch):\n",
        "        print(f'Epoch: {e+1}')\n",
        "        torch.manual_seed(2809)\n",
        "        iterator = iter(train_loader)\n",
        "        for AU in iterator:\n",
        "                if au_feat == 'nope':\n",
        "                    vis_feat, y = AU[visual_feat_1], AU['label']\n",
        "                    vis_feat, y = vis_feat.to(device), y.to(device)\n",
        "                    aud_feat = None\n",
        "                else:\n",
        "                    vis_feat, aud_feat, y = AU[visual_feat_1], AU[au_feat], AU['label']\n",
        "                    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "                model.zero_grad()\n",
        "                pred, au_pred = model(vis_feat, aud_feat)\n",
        "                loss = compute_AU_loss(pred, y, weight)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                loss_value.append(loss.item())\n",
        "                all_preds.extend(au_pred.cpu().tolist())\n",
        "                all_targets.extend(y.cpu().tolist())\n",
        "\n",
        "        avg_loss = round(np.mean(loss_value),3)\n",
        "        loss_train.append(avg_loss)\n",
        "        f1_scores, f1_thresh, accuracy, threshold = compute_AU_F1(all_preds, all_targets)\n",
        "        print(f'Train Loss: {avg_loss}, Accuracy of 12 AU classes: {accuracy}')\n",
        "\n",
        "        val_loss, f1s, f1t, acc, f1_threshold = evaluate_model(model, val_loader, visual_feature, au_feat, weight)\n",
        "        loss_val.append(val_loss)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_AU_{mod_type}_{vi_au}.pth'))\n",
        "            f1best = f1s\n",
        "            accbest = acc\n",
        "\n",
        "        print(f'Validation Loss: {val_loss}, Accuracy of 12 AU classes: {acc}')\n",
        "        scheduler.step(val_loss)\n",
        "    return loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest"
      ],
      "metadata": {
        "id": "Zeoq8a3dj0n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUHE6yoY-bTy"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, vi_feat, au_feat, weight):\n",
        "    model.eval()\n",
        "    total_loss = []\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        iterator = iter(data_loader)\n",
        "        for AU in iterator:\n",
        "            if au_feat == 'nope':\n",
        "                vis_feat, y = AU[vi_feat], AU['label']\n",
        "                vis_feat, y = vis_feat.to(device), y.to(device)\n",
        "                aud_feat = None\n",
        "            else:\n",
        "                vis_feat, aud_feat, y = AU[vi_feat], AU[au_feat], AU['label']\n",
        "                vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "            pred, au_pred = model(vis_feat, aud_feat)\n",
        "            loss = compute_AU_loss(pred, y, weight)\n",
        "            total_loss.append(loss.item())\n",
        "            all_preds.extend(au_pred.cpu().tolist())\n",
        "            all_targets.extend(y.cpu().tolist())\n",
        "\n",
        "    f1_scores, f1_thresh, acc, threshold = compute_AU_F1(all_preds, all_targets)\n",
        "    return round(np.mean(total_loss),3), round(f1_scores,3), round(f1_thresh,3), acc, threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gRlrEmVaH9R"
      },
      "source": [
        "#### Cropped_aligned images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc0qUsCawNt4"
      },
      "source": [
        "##### EffNet + Wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xHnag9zZQMP",
        "outputId": "f7ff2c0b-403a-4742-a5a4-2b2e67b53d88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.141, Accuracy of 12 AU classes: [0.905, 0.949, 0.929, 0.882, 0.807, 0.828, 0.896, 0.936, 0.878, 0.916, 0.82, 0.895]\n",
            "Validation Loss: 0.177, Accuracy of 12 AU classes: [0.914, 0.947, 0.866, 0.797, 0.744, 0.79, 0.861, 0.883, 0.919, 0.976, 0.777, 0.913]\n",
            "Epoch: 2\n",
            "Train Loss: 0.128, Accuracy of 12 AU classes: [0.914, 0.953, 0.938, 0.897, 0.822, 0.841, 0.906, 0.968, 0.908, 0.973, 0.835, 0.927]\n",
            "Validation Loss: 0.184, Accuracy of 12 AU classes: [0.914, 0.947, 0.867, 0.781, 0.745, 0.77, 0.859, 0.911, 0.925, 0.976, 0.773, 0.914]\n",
            "Epoch: 3\n",
            "Train Loss: 0.12, Accuracy of 12 AU classes: [0.92, 0.956, 0.944, 0.905, 0.831, 0.848, 0.912, 0.97, 0.917, 0.977, 0.844, 0.93]\n",
            "Validation Loss: 0.191, Accuracy of 12 AU classes: [0.912, 0.945, 0.866, 0.78, 0.725, 0.787, 0.856, 0.921, 0.929, 0.976, 0.773, 0.916]\n",
            "Epoch: 4\n",
            "Train Loss: 0.115, Accuracy of 12 AU classes: [0.93, 0.958, 0.947, 0.911, 0.838, 0.854, 0.916, 0.972, 0.921, 0.979, 0.851, 0.932]\n",
            "Validation Loss: 0.196, Accuracy of 12 AU classes: [0.91, 0.943, 0.838, 0.76, 0.727, 0.785, 0.855, 0.923, 0.931, 0.962, 0.772, 0.878]\n",
            "Epoch: 5\n",
            "Train Loss: 0.111, Accuracy of 12 AU classes: [0.933, 0.959, 0.95, 0.916, 0.843, 0.859, 0.92, 0.972, 0.962, 0.981, 0.856, 0.934]\n",
            "Validation Loss: 0.201, Accuracy of 12 AU classes: [0.91, 0.942, 0.836, 0.759, 0.729, 0.783, 0.854, 0.924, 0.929, 0.964, 0.771, 0.878]\n",
            "Epoch: 6\n",
            "Train Loss: 0.107, Accuracy of 12 AU classes: [0.935, 0.961, 0.952, 0.92, 0.847, 0.862, 0.923, 0.973, 0.962, 0.982, 0.861, 0.936]\n",
            "Validation Loss: 0.207, Accuracy of 12 AU classes: [0.911, 0.942, 0.837, 0.756, 0.729, 0.782, 0.852, 0.929, 0.929, 0.964, 0.77, 0.882]\n",
            "Epoch: 7\n",
            "Train Loss: 0.104, Accuracy of 12 AU classes: [0.937, 0.962, 0.953, 0.924, 0.85, 0.866, 0.925, 0.974, 0.962, 0.982, 0.865, 0.937]\n",
            "Validation Loss: 0.212, Accuracy of 12 AU classes: [0.912, 0.942, 0.835, 0.756, 0.729, 0.766, 0.853, 0.93, 0.929, 0.976, 0.762, 0.882]\n",
            "Epoch: 8\n",
            "Train Loss: 0.102, Accuracy of 12 AU classes: [0.939, 0.963, 0.955, 0.927, 0.854, 0.869, 0.928, 0.974, 0.962, 0.983, 0.868, 0.939]\n",
            "Validation Loss: 0.218, Accuracy of 12 AU classes: [0.913, 0.942, 0.837, 0.757, 0.728, 0.765, 0.852, 0.93, 0.928, 0.976, 0.763, 0.884]\n",
            "1h 52min 54s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 8, 32, optimizer, auft, weights1, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VOG7YkJr_j5",
        "outputId": "2478e010-0771-4d9f-c564-9c06854cc331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.487, accuracy: [0.914, 0.947, 0.866, 0.797, 0.744, 0.79, 0.861, 0.883, 0.919, 0.976, 0.777, 0.913], f1_threshold: [0.30000000000000004, 0.2, 0.2, 0.4, 0.5, 0.4, 0.4, 0.1, 0.1, 0.2, 0.4, 0.2]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}_loss.pth'))\n",
        "    AU_model = AU_fusion().to(device)\n",
        "    AU_model.load_state_dict(AU_best_model)\n",
        "    val_loss, f1s, f1t, acc, threshold = evaluate_model(AU_model, val_loader, auft, weights1)\n",
        "    print(f'best metric: f1_score {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53LSD8h9Og5J",
        "outputId": "e5961b99-4d89-4e54-f2d2-bda7d67db50c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.488, Accuracy of 12 AU classes: [0.884, 0.943, 0.868, 0.793, 0.754, 0.771, 0.865, 0.947, 0.961, 0.946, 0.759, 0.899]\n",
            "Validation Loss: 0.472, Accuracy of 12 AU classes: [0.927, 0.95, 0.86, 0.797, 0.73, 0.775, 0.851, 0.89, 0.841, 0.973, 0.777, 0.908]\n",
            "Epoch: 2\n",
            "Train Loss: 0.456, Accuracy of 12 AU classes: [0.889, 0.942, 0.892, 0.832, 0.767, 0.784, 0.87, 0.951, 0.947, 0.967, 0.776, 0.896]\n",
            "Validation Loss: 0.468, Accuracy of 12 AU classes: [0.925, 0.944, 0.862, 0.8, 0.735, 0.78, 0.856, 0.906, 0.914, 0.969, 0.785, 0.903]\n",
            "Epoch: 3\n",
            "Train Loss: 0.437, Accuracy of 12 AU classes: [0.892, 0.941, 0.898, 0.839, 0.774, 0.79, 0.872, 0.952, 0.939, 0.967, 0.785, 0.895]\n",
            "Validation Loss: 0.469, Accuracy of 12 AU classes: [0.924, 0.941, 0.864, 0.8, 0.737, 0.783, 0.858, 0.91, 0.911, 0.966, 0.787, 0.901]\n",
            "Epoch: 4\n",
            "Train Loss: 0.424, Accuracy of 12 AU classes: [0.894, 0.941, 0.902, 0.845, 0.78, 0.795, 0.874, 0.952, 0.934, 0.968, 0.791, 0.895]\n",
            "Validation Loss: 0.47, Accuracy of 12 AU classes: [0.924, 0.955, 0.866, 0.801, 0.739, 0.785, 0.859, 0.913, 0.91, 0.965, 0.788, 0.9]\n",
            "Epoch: 5\n",
            "Train Loss: 0.414, Accuracy of 12 AU classes: [0.895, 0.942, 0.905, 0.849, 0.784, 0.799, 0.876, 0.952, 0.931, 0.968, 0.795, 0.895]\n",
            "Validation Loss: 0.472, Accuracy of 12 AU classes: [0.923, 0.954, 0.868, 0.8, 0.74, 0.785, 0.86, 0.914, 0.909, 0.964, 0.773, 0.924]\n",
            "Epoch: 6\n",
            "Train Loss: 0.406, Accuracy of 12 AU classes: [0.897, 0.942, 0.907, 0.852, 0.787, 0.802, 0.877, 0.951, 0.959, 0.968, 0.799, 0.895]\n",
            "Validation Loss: 0.474, Accuracy of 12 AU classes: [0.923, 0.954, 0.87, 0.8, 0.741, 0.786, 0.86, 0.916, 0.909, 0.98, 0.774, 0.923]\n",
            "Epoch: 7\n",
            "Train Loss: 0.398, Accuracy of 12 AU classes: [0.898, 0.942, 0.909, 0.855, 0.79, 0.804, 0.878, 0.951, 0.958, 0.969, 0.801, 0.895]\n",
            "Validation Loss: 0.475, Accuracy of 12 AU classes: [0.922, 0.953, 0.871, 0.799, 0.742, 0.786, 0.86, 0.917, 0.91, 0.98, 0.775, 0.923]\n",
            "Epoch: 8\n",
            "Train Loss: 0.392, Accuracy of 12 AU classes: [0.899, 0.942, 0.911, 0.858, 0.792, 0.806, 0.879, 0.951, 0.957, 0.969, 0.803, 0.895]\n",
            "Validation Loss: 0.477, Accuracy of 12 AU classes: [0.922, 0.953, 0.872, 0.799, 0.715, 0.786, 0.86, 0.917, 0.91, 0.979, 0.776, 0.922]\n",
            "1h 10min 35s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 8, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7346109a-373c-44be-a0e5-78c24810cb95",
        "id": "eZ1UARwGZaD3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best metric: f1_score 0.504, accuracy: [0.922, 0.953, 0.871, 0.799, 0.742, 0.786, 0.86, 0.917, 0.91, 0.98, 0.775, 0.923], f1_threshold: [0.8, 0.9, 0.6, 0.6, 0.5, 0.5, 0.6, 0.7000000000000001, 0.7000000000000001, 0.9, 0.2, 0.8]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}_acc.pth'))\n",
        "    mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, f1s, f1t, acc, threshold = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XxpA4cm5AeF"
      },
      "source": [
        "##### EffNet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 8, 32, optimizer, auft, weights, viau)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-ivgpabOAhW",
        "outputId": "f17b4f95-7905-482d-dcdb-909e81848885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.373, Accuracy of 12 AU classes: [0.902, 0.942, 0.919, 0.866, 0.797, 0.808, 0.882, 0.97, 0.949, 0.978, 0.802, 0.896]\n",
            "Validation Loss: 0.51, Accuracy of 12 AU classes: [0.918, 0.952, 0.884, 0.794, 0.744, 0.78, 0.87, 0.946, 0.913, 0.968, 0.785, 0.915]\n",
            "Epoch: 2\n",
            "Train Loss: 0.327, Accuracy of 12 AU classes: [0.921, 0.945, 0.929, 0.88, 0.811, 0.82, 0.899, 0.97, 0.965, 0.981, 0.818, 0.924]\n",
            "Validation Loss: 0.581, Accuracy of 12 AU classes: [0.912, 0.95, 0.878, 0.795, 0.727, 0.778, 0.865, 0.936, 0.915, 0.971, 0.768, 0.913]\n",
            "Epoch: 3\n",
            "Train Loss: 0.3, Accuracy of 12 AU classes: [0.926, 0.96, 0.934, 0.889, 0.819, 0.827, 0.905, 0.97, 0.965, 0.983, 0.827, 0.928]\n",
            "Validation Loss: 0.651, Accuracy of 12 AU classes: [0.913, 0.95, 0.877, 0.785, 0.734, 0.78, 0.861, 0.945, 0.919, 0.963, 0.769, 0.899]\n",
            "Epoch: 4\n",
            "Train Loss: 0.281, Accuracy of 12 AU classes: [0.93, 0.962, 0.942, 0.895, 0.825, 0.833, 0.909, 0.971, 0.965, 0.984, 0.834, 0.931]\n",
            "Validation Loss: 0.719, Accuracy of 12 AU classes: [0.924, 0.942, 0.866, 0.786, 0.736, 0.78, 0.858, 0.938, 0.91, 0.958, 0.77, 0.903]\n",
            "Epoch: 5\n",
            "Train Loss: 0.266, Accuracy of 12 AU classes: [0.933, 0.964, 0.945, 0.901, 0.829, 0.837, 0.912, 0.972, 0.965, 0.985, 0.839, 0.934]\n",
            "Validation Loss: 0.78, Accuracy of 12 AU classes: [0.923, 0.943, 0.866, 0.784, 0.734, 0.779, 0.857, 0.942, 0.898, 0.96, 0.77, 0.906]\n",
            "Epoch: 6\n",
            "Train Loss: 0.254, Accuracy of 12 AU classes: [0.936, 0.966, 0.948, 0.906, 0.833, 0.841, 0.915, 0.972, 0.966, 0.986, 0.843, 0.937]\n",
            "Validation Loss: 0.841, Accuracy of 12 AU classes: [0.922, 0.945, 0.866, 0.766, 0.731, 0.776, 0.854, 0.943, 0.906, 0.957, 0.769, 0.897]\n",
            "Epoch: 7\n",
            "Train Loss: 0.243, Accuracy of 12 AU classes: [0.938, 0.968, 0.95, 0.914, 0.836, 0.844, 0.918, 0.973, 0.966, 0.987, 0.847, 0.939]\n",
            "Validation Loss: 0.905, Accuracy of 12 AU classes: [0.919, 0.94, 0.866, 0.768, 0.728, 0.777, 0.846, 0.941, 0.901, 0.959, 0.768, 0.898]\n",
            "Epoch: 8\n",
            "Train Loss: 0.235, Accuracy of 12 AU classes: [0.94, 0.969, 0.952, 0.918, 0.839, 0.857, 0.92, 0.974, 0.967, 0.988, 0.851, 0.941]\n",
            "Validation Loss: 0.958, Accuracy of 12 AU classes: [0.917, 0.94, 0.865, 0.767, 0.728, 0.779, 0.845, 0.938, 0.903, 0.96, 0.767, 0.906]\n",
            "1h 50min 26s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEUxbNEd5AeN",
        "outputId": "56daafce-d166-4a48-d3c2-dd127b6d19f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.494, accuracy: [0.919, 0.949, 0.876, 0.779, 0.76, 0.77, 0.861, 0.956, 0.882, 0.972, 0.777, 0.918], f1_threshold: [0.30000000000000004, 0.2, 0.2, 0.30000000000000004, 0.5, 0.30000000000000004, 0.4, 0.2, 0.1, 0.2, 0.4, 0.2]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_fusion_{viau}.pth'))\n",
        "    AU_model = AU_fusion().to(device)\n",
        "    AU_model.load_state_dict(AU_best_model)\n",
        "    val_loss, f1s, f1t, acc, threshold = evaluate_model(AU_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 8, 32, optimizer, auft, weights, viau)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691cc00b-4a30-4159-9af1-2aa5097484a4",
        "id": "EdC_EbilOVCf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.501, Accuracy of 12 AU classes: [0.854, 0.943, 0.863, 0.788, 0.75, 0.768, 0.864, 0.892, 0.91, 0.95, 0.751, 0.896]\n",
            "Validation Loss: 0.469, Accuracy of 12 AU classes: [0.908, 0.952, 0.865, 0.795, 0.739, 0.781, 0.864, 0.884, 0.841, 0.977, 0.778, 0.91]\n",
            "Epoch: 2\n",
            "Train Loss: 0.47, Accuracy of 12 AU classes: [0.885, 0.942, 0.87, 0.801, 0.764, 0.78, 0.868, 0.95, 0.95, 0.968, 0.769, 0.893]\n",
            "Validation Loss: 0.461, Accuracy of 12 AU classes: [0.927, 0.95, 0.865, 0.799, 0.743, 0.786, 0.855, 0.902, 0.845, 0.973, 0.785, 0.909]\n",
            "Epoch: 3\n",
            "Train Loss: 0.452, Accuracy of 12 AU classes: [0.888, 0.941, 0.891, 0.81, 0.771, 0.786, 0.87, 0.95, 0.942, 0.968, 0.778, 0.893]\n",
            "Validation Loss: 0.459, Accuracy of 12 AU classes: [0.927, 0.949, 0.884, 0.8, 0.743, 0.789, 0.868, 0.906, 0.918, 0.97, 0.787, 0.909]\n",
            "Epoch: 4\n",
            "Train Loss: 0.44, Accuracy of 12 AU classes: [0.891, 0.941, 0.894, 0.838, 0.775, 0.791, 0.872, 0.95, 0.937, 0.968, 0.784, 0.893]\n",
            "Validation Loss: 0.459, Accuracy of 12 AU classes: [0.926, 0.947, 0.885, 0.801, 0.744, 0.79, 0.869, 0.908, 0.917, 0.981, 0.788, 0.93]\n",
            "Epoch: 5\n",
            "Train Loss: 0.431, Accuracy of 12 AU classes: [0.892, 0.941, 0.897, 0.842, 0.779, 0.794, 0.873, 0.95, 0.934, 0.968, 0.788, 0.893]\n",
            "Validation Loss: 0.46, Accuracy of 12 AU classes: [0.925, 0.946, 0.871, 0.802, 0.718, 0.791, 0.869, 0.949, 0.916, 0.981, 0.789, 0.93]\n",
            "Epoch: 6\n",
            "Train Loss: 0.423, Accuracy of 12 AU classes: [0.893, 0.941, 0.899, 0.845, 0.782, 0.797, 0.874, 0.95, 0.931, 0.968, 0.791, 0.893]\n",
            "Validation Loss: 0.462, Accuracy of 12 AU classes: [0.924, 0.945, 0.872, 0.802, 0.72, 0.791, 0.869, 0.948, 0.916, 0.98, 0.789, 0.929]\n",
            "Epoch: 7\n",
            "Train Loss: 0.416, Accuracy of 12 AU classes: [0.895, 0.941, 0.901, 0.848, 0.784, 0.799, 0.875, 0.95, 0.96, 0.968, 0.794, 0.893]\n",
            "Validation Loss: 0.464, Accuracy of 12 AU classes: [0.922, 0.943, 0.874, 0.802, 0.721, 0.791, 0.869, 0.947, 0.915, 0.98, 0.773, 0.928]\n",
            "Epoch: 8\n",
            "Train Loss: 0.411, Accuracy of 12 AU classes: [0.895, 0.941, 0.903, 0.85, 0.786, 0.801, 0.876, 0.95, 0.959, 0.968, 0.796, 0.893]\n",
            "Validation Loss: 0.465, Accuracy of 12 AU classes: [0.922, 0.942, 0.875, 0.802, 0.722, 0.791, 0.869, 0.947, 0.914, 0.979, 0.773, 0.927]\n",
            "1h 10min 4s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8bca035-87be-4313-afbc-f93279342272",
        "id": "YxbWr2utOaD7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best metric: f1_score 0.515, accuracy: [0.924, 0.945, 0.872, 0.802, 0.72, 0.791, 0.869, 0.948, 0.916, 0.98, 0.789, 0.929], f1_threshold: [0.8, 0.8, 0.6, 0.6, 0.4, 0.5, 0.7000000000000001, 0.8, 0.7000000000000001, 0.9, 0.30000000000000004, 0.8]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, f1s, f1t, acc, threshold = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1laG6DbGwUoR"
      },
      "source": [
        "##### EffNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9ehojtewUoR",
        "outputId": "c7a5a6da-1cff-4dc1-93af-bb5fa152f555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.364, Accuracy of 12 AU classes: [0.901, 0.939, 0.92, 0.867, 0.8, 0.814, 0.893, 0.971, 0.949, 0.979, 0.807, 0.921]\n",
            "Validation Loss: 0.524, Accuracy of 12 AU classes: [0.917, 0.946, 0.867, 0.785, 0.718, 0.782, 0.851, 0.943, 0.932, 0.968, 0.769, 0.916]\n",
            "Epoch: 2\n",
            "Train Loss: 0.323, Accuracy of 12 AU classes: [0.92, 0.956, 0.929, 0.88, 0.814, 0.826, 0.9, 0.971, 0.965, 0.981, 0.823, 0.924]\n",
            "Validation Loss: 0.576, Accuracy of 12 AU classes: [0.915, 0.948, 0.875, 0.771, 0.736, 0.788, 0.855, 0.953, 0.919, 0.973, 0.77, 0.917]\n",
            "Epoch: 3\n",
            "Train Loss: 0.299, Accuracy of 12 AU classes: [0.925, 0.958, 0.934, 0.887, 0.822, 0.833, 0.904, 0.971, 0.965, 0.982, 0.832, 0.927]\n",
            "Validation Loss: 0.626, Accuracy of 12 AU classes: [0.914, 0.948, 0.871, 0.772, 0.741, 0.789, 0.845, 0.943, 0.921, 0.976, 0.758, 0.903]\n",
            "Epoch: 4\n",
            "Train Loss: 0.283, Accuracy of 12 AU classes: [0.928, 0.96, 0.942, 0.893, 0.828, 0.838, 0.908, 0.971, 0.965, 0.983, 0.838, 0.93]\n",
            "Validation Loss: 0.674, Accuracy of 12 AU classes: [0.913, 0.937, 0.873, 0.769, 0.741, 0.788, 0.853, 0.944, 0.922, 0.976, 0.759, 0.906]\n",
            "Epoch: 5\n",
            "Train Loss: 0.27, Accuracy of 12 AU classes: [0.93, 0.961, 0.944, 0.897, 0.832, 0.842, 0.911, 0.972, 0.965, 0.984, 0.843, 0.932]\n",
            "Validation Loss: 0.718, Accuracy of 12 AU classes: [0.913, 0.935, 0.866, 0.752, 0.741, 0.788, 0.852, 0.945, 0.906, 0.973, 0.759, 0.906]\n",
            "Epoch: 6\n",
            "Train Loss: 0.26, Accuracy of 12 AU classes: [0.933, 0.963, 0.947, 0.901, 0.836, 0.846, 0.913, 0.972, 0.965, 0.984, 0.847, 0.933]\n",
            "Validation Loss: 0.76, Accuracy of 12 AU classes: [0.902, 0.936, 0.871, 0.754, 0.741, 0.79, 0.85, 0.944, 0.907, 0.971, 0.761, 0.909]\n",
            "Epoch: 7\n",
            "Train Loss: 0.251, Accuracy of 12 AU classes: [0.934, 0.964, 0.948, 0.91, 0.839, 0.859, 0.915, 0.973, 0.965, 0.985, 0.85, 0.935]\n",
            "Validation Loss: 0.799, Accuracy of 12 AU classes: [0.913, 0.935, 0.862, 0.755, 0.74, 0.791, 0.849, 0.937, 0.894, 0.976, 0.76, 0.907]\n",
            "Epoch: 8\n",
            "Train Loss: 0.243, Accuracy of 12 AU classes: [0.936, 0.965, 0.95, 0.912, 0.842, 0.861, 0.917, 0.973, 0.966, 0.986, 0.853, 0.937]\n",
            "Validation Loss: 0.842, Accuracy of 12 AU classes: [0.913, 0.935, 0.865, 0.759, 0.74, 0.794, 0.849, 0.941, 0.896, 0.978, 0.762, 0.909]\n",
            "2h 21min 13s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 8, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtADdvikwUoR",
        "outputId": "e61ee5d3-f6d2-4cef-8771-be999227eb60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best metric: f1_score 0.495, accuracy: [0.917, 0.946, 0.867, 0.785, 0.718, 0.782, 0.851, 0.943, 0.932, 0.968, 0.769, 0.916], f1_threshold: [0.8, 0.8, 0.5, 0.6, 0.4, 0.5, 0.7000000000000001, 0.7000000000000001, 0.7000000000000001, 0.8, 0.30000000000000004, 0.7000000000000001]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}_loss.pth'))\n",
        "    AU_model = AU_fusion().to(device)\n",
        "    AU_model.load_state_dict(AU_best_model)\n",
        "    val_loss, f1s, f1t, acc, threshold = evaluate_model(AU_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 8, 32, optimizer, auft, weights, viau)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNDrWZdP8vgb",
        "outputId": "500cca73-fc36-4f43-a941-5e93e3ddba34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.507, Accuracy of 12 AU classes: [0.878, 0.941, 0.86, 0.782, 0.701, 0.767, 0.863, 0.891, 0.908, 0.947, 0.747, 0.894]\n",
            "Validation Loss: 0.476, Accuracy of 12 AU classes: [0.905, 0.95, 0.878, 0.795, 0.726, 0.779, 0.853, 0.885, 0.846, 0.975, 0.774, 0.904]\n",
            "Epoch: 2\n",
            "Train Loss: 0.479, Accuracy of 12 AU classes: [0.883, 0.939, 0.865, 0.794, 0.761, 0.778, 0.866, 0.951, 0.95, 0.966, 0.765, 0.891]\n",
            "Validation Loss: 0.47, Accuracy of 12 AU classes: [0.904, 0.947, 0.879, 0.798, 0.733, 0.784, 0.854, 0.899, 0.921, 0.973, 0.781, 0.928]\n",
            "Epoch: 3\n",
            "Train Loss: 0.462, Accuracy of 12 AU classes: [0.887, 0.939, 0.87, 0.803, 0.767, 0.785, 0.868, 0.951, 0.942, 0.966, 0.775, 0.89]\n",
            "Validation Loss: 0.468, Accuracy of 12 AU classes: [0.923, 0.944, 0.882, 0.797, 0.737, 0.787, 0.854, 0.902, 0.918, 0.971, 0.783, 0.928]\n",
            "Epoch: 4\n",
            "Train Loss: 0.45, Accuracy of 12 AU classes: [0.889, 0.939, 0.89, 0.809, 0.771, 0.789, 0.869, 0.951, 0.937, 0.967, 0.781, 0.89]\n",
            "Validation Loss: 0.469, Accuracy of 12 AU classes: [0.922, 0.941, 0.884, 0.796, 0.739, 0.788, 0.866, 0.95, 0.917, 0.983, 0.783, 0.927]\n",
            "Epoch: 5\n",
            "Train Loss: 0.44, Accuracy of 12 AU classes: [0.891, 0.939, 0.893, 0.836, 0.774, 0.792, 0.871, 0.951, 0.934, 0.967, 0.785, 0.89]\n",
            "Validation Loss: 0.471, Accuracy of 12 AU classes: [0.921, 0.957, 0.885, 0.795, 0.74, 0.788, 0.866, 0.95, 0.917, 0.982, 0.784, 0.927]\n",
            "Epoch: 6\n",
            "Train Loss: 0.433, Accuracy of 12 AU classes: [0.893, 0.94, 0.896, 0.839, 0.777, 0.794, 0.872, 0.951, 0.931, 0.967, 0.788, 0.891]\n",
            "Validation Loss: 0.473, Accuracy of 12 AU classes: [0.92, 0.957, 0.885, 0.794, 0.741, 0.788, 0.865, 0.95, 0.917, 0.981, 0.784, 0.926]\n",
            "Epoch: 7\n",
            "Train Loss: 0.426, Accuracy of 12 AU classes: [0.894, 0.94, 0.898, 0.842, 0.779, 0.796, 0.872, 0.951, 0.929, 0.968, 0.791, 0.891]\n",
            "Validation Loss: 0.476, Accuracy of 12 AU classes: [0.919, 0.956, 0.868, 0.793, 0.742, 0.788, 0.864, 0.949, 0.917, 0.98, 0.767, 0.925]\n",
            "Epoch: 8\n",
            "Train Loss: 0.42, Accuracy of 12 AU classes: [0.894, 0.94, 0.9, 0.844, 0.781, 0.798, 0.873, 0.951, 0.959, 0.968, 0.793, 0.891]\n",
            "Validation Loss: 0.478, Accuracy of 12 AU classes: [0.919, 0.955, 0.868, 0.792, 0.717, 0.788, 0.863, 0.949, 0.916, 0.979, 0.768, 0.923]\n",
            "1h 36min 6s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD5X7Z7bu4sZ",
        "outputId": "aca0e7fb-cab3-4ee6-f4a8-310b157bd696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best metric: f1_score 0.51, accuracy: [0.92, 0.957, 0.885, 0.794, 0.741, 0.788, 0.865, 0.95, 0.917, 0.981, 0.784, 0.926], f1_threshold: [0.8, 0.9, 0.7000000000000001, 0.6, 0.5, 0.5, 0.7000000000000001, 0.8, 0.7000000000000001, 0.9, 0.30000000000000004, 0.8]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, f1s, f1t, acc, threshold = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z2X3UCtFQQC"
      },
      "source": [
        "#### Cropped images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bje0cpc1FQQL"
      },
      "source": [
        "##### EffNet + Wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NFf5x3kFQQL",
        "outputId": "0f6e850e-2f2e-486d-dbb1-901b27b672cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.248, Accuracy of 12 AU classes: [0.464, 0.616, 0.696, 0.494, 0.59, 0.726, 0.822, 0.821, 0.398, 0.027, 0.698, 0.474]\n",
            "Validation Loss: 0.201, Accuracy of 12 AU classes: [0.84, 0.875, 0.523, 0.893, 0.783, 0.88, 0.895, 0.906, 0.176, 0.55, 0.755, 0.465]\n",
            "Epoch: 2\n",
            "Train Loss: 0.224, Accuracy of 12 AU classes: [0.863, 0.773, 0.788, 0.641, 0.668, 0.754, 0.838, 0.881, 0.228, 0.089, 0.721, 0.633]\n",
            "Validation Loss: 0.177, Accuracy of 12 AU classes: [0.913, 0.916, 0.754, 0.884, 0.826, 0.877, 0.898, 0.955, 0.176, 0.333, 0.743, 0.734]\n",
            "Epoch: 3\n",
            "Train Loss: 0.21, Accuracy of 12 AU classes: [0.872, 0.828, 0.82, 0.698, 0.701, 0.767, 0.847, 0.769, 0.135, 0.3, 0.735, 0.705]\n",
            "Validation Loss: 0.168, Accuracy of 12 AU classes: [0.91, 0.919, 0.768, 0.888, 0.833, 0.881, 0.892, 0.978, 0.219, 0.754, 0.755, 0.867]\n",
            "Epoch: 4\n",
            "Train Loss: 0.2, Accuracy of 12 AU classes: [0.877, 0.944, 0.838, 0.808, 0.72, 0.776, 0.853, 0.812, 0.274, 0.437, 0.746, 0.889]\n",
            "Validation Loss: 0.164, Accuracy of 12 AU classes: [0.908, 0.92, 0.801, 0.892, 0.847, 0.883, 0.89, 0.936, 0.376, 0.804, 0.757, 0.888]\n",
            "Epoch: 5\n",
            "Train Loss: 0.193, Accuracy of 12 AU classes: [0.881, 0.946, 0.85, 0.817, 0.733, 0.783, 0.858, 0.839, 0.383, 0.527, 0.755, 0.895]\n",
            "Validation Loss: 0.163, Accuracy of 12 AU classes: [0.92, 0.923, 0.814, 0.894, 0.846, 0.884, 0.88, 0.946, 0.505, 0.846, 0.759, 0.896]\n",
            "Epoch: 6\n",
            "Train Loss: 0.187, Accuracy of 12 AU classes: [0.884, 0.948, 0.858, 0.824, 0.743, 0.789, 0.862, 0.858, 0.465, 0.59, 0.763, 0.899]\n",
            "Validation Loss: 0.163, Accuracy of 12 AU classes: [0.921, 0.926, 0.862, 0.892, 0.838, 0.884, 0.879, 0.955, 0.564, 0.877, 0.762, 0.9]\n",
            "Epoch: 7\n",
            "Train Loss: 0.182, Accuracy of 12 AU classes: [0.887, 0.949, 0.865, 0.83, 0.751, 0.794, 0.865, 0.872, 0.527, 0.636, 0.77, 0.902]\n",
            "Validation Loss: 0.164, Accuracy of 12 AU classes: [0.922, 0.928, 0.855, 0.892, 0.845, 0.884, 0.88, 0.959, 0.602, 0.881, 0.76, 0.903]\n",
            "Epoch: 8\n",
            "Train Loss: 0.177, Accuracy of 12 AU classes: [0.889, 0.949, 0.871, 0.835, 0.759, 0.799, 0.869, 0.882, 0.575, 0.672, 0.777, 0.904]\n",
            "Validation Loss: 0.165, Accuracy of 12 AU classes: [0.923, 0.927, 0.852, 0.891, 0.842, 0.883, 0.877, 0.963, 0.634, 0.871, 0.761, 0.904]\n",
            "Epoch: 9\n",
            "Train Loss: 0.173, Accuracy of 12 AU classes: [0.891, 0.95, 0.876, 0.84, 0.765, 0.803, 0.872, 0.89, 0.614, 0.699, 0.783, 0.906]\n",
            "Validation Loss: 0.166, Accuracy of 12 AU classes: [0.923, 0.928, 0.867, 0.889, 0.837, 0.88, 0.878, 0.966, 0.658, 0.856, 0.76, 0.905]\n",
            "Epoch: 10\n",
            "Train Loss: 0.169, Accuracy of 12 AU classes: [0.893, 0.951, 0.88, 0.845, 0.771, 0.807, 0.875, 0.897, 0.646, 0.722, 0.788, 0.908]\n",
            "Validation Loss: 0.168, Accuracy of 12 AU classes: [0.918, 0.928, 0.865, 0.888, 0.831, 0.886, 0.876, 0.9, 0.663, 0.939, 0.757, 0.906]\n",
            "4min 29s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD7JJRE2FQQL",
        "outputId": "932130ed-f958-43fb-f86b-0a72eb3ceca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.552, f1_threshold: 0.258, accuracy: [0.92, 0.923, 0.814, 0.894, 0.846, 0.884, 0.88, 0.946, 0.505, 0.846, 0.759, 0.896]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n",
        "    AU_model = AU_fusion().to(device)\n",
        "    AU_model.load_state_dict(AU_best_model)\n",
        "    val_loss, f1s, f1t, acc = evaluate_model(AU_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahlzskb3FQQM",
        "outputId": "8885bc71-7739-4f8f-b45e-34e33b81052b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.287, Accuracy of 12 AU classes: [0.126, 0.109, 0.306, 0.452, 0.535, 0.368, 0.505, 0.146, 0.073, 0.277, 0.63, 0.078]\n",
            "Validation Loss: 0.24, Accuracy of 12 AU classes: [0.12, 0.577, 0.138, 0.878, 0.683, 0.843, 0.855, 0.933, 0.513, 0.089, 0.645, 0.089]\n",
            "Epoch: 2\n",
            "Train Loss: 0.256, Accuracy of 12 AU classes: [0.182, 0.47, 0.557, 0.607, 0.62, 0.491, 0.664, 0.294, 0.466, 0.362, 0.637, 0.24]\n",
            "Validation Loss: 0.215, Accuracy of 12 AU classes: [0.51, 0.867, 0.409, 0.865, 0.811, 0.851, 0.882, 0.972, 0.821, 0.933, 0.723, 0.837]\n",
            "Epoch: 3\n",
            "Train Loss: 0.241, Accuracy of 12 AU classes: [0.244, 0.622, 0.644, 0.662, 0.652, 0.557, 0.717, 0.516, 0.633, 0.564, 0.655, 0.431]\n",
            "Validation Loss: 0.201, Accuracy of 12 AU classes: [0.651, 0.881, 0.655, 0.891, 0.814, 0.853, 0.879, 0.986, 0.824, 0.936, 0.736, 0.89]\n",
            "Epoch: 4\n",
            "Train Loss: 0.23, Accuracy of 12 AU classes: [0.734, 0.699, 0.688, 0.69, 0.67, 0.594, 0.744, 0.629, 0.717, 0.666, 0.67, 0.536]\n",
            "Validation Loss: 0.192, Accuracy of 12 AU classes: [0.721, 0.892, 0.754, 0.868, 0.819, 0.855, 0.88, 0.986, 0.824, 0.936, 0.719, 0.896]\n",
            "Epoch: 5\n",
            "Train Loss: 0.223, Accuracy of 12 AU classes: [0.759, 0.746, 0.716, 0.707, 0.682, 0.618, 0.761, 0.697, 0.767, 0.727, 0.68, 0.599]\n",
            "Validation Loss: 0.186, Accuracy of 12 AU classes: [0.905, 0.904, 0.808, 0.871, 0.823, 0.874, 0.881, 0.985, 0.824, 0.936, 0.74, 0.89]\n",
            "Epoch: 6\n",
            "Train Loss: 0.218, Accuracy of 12 AU classes: [0.775, 0.776, 0.735, 0.72, 0.691, 0.636, 0.772, 0.742, 0.801, 0.768, 0.689, 0.641]\n",
            "Validation Loss: 0.182, Accuracy of 12 AU classes: [0.908, 0.908, 0.833, 0.874, 0.83, 0.881, 0.882, 0.982, 0.824, 0.936, 0.749, 0.881]\n",
            "Epoch: 7\n",
            "Train Loss: 0.214, Accuracy of 12 AU classes: [0.787, 0.796, 0.749, 0.729, 0.698, 0.649, 0.781, 0.774, 0.825, 0.797, 0.695, 0.669]\n",
            "Validation Loss: 0.178, Accuracy of 12 AU classes: [0.909, 0.911, 0.844, 0.875, 0.832, 0.885, 0.885, 0.973, 0.824, 0.936, 0.755, 0.874]\n",
            "Epoch: 8\n",
            "Train Loss: 0.21, Accuracy of 12 AU classes: [0.796, 0.812, 0.76, 0.737, 0.704, 0.659, 0.788, 0.797, 0.843, 0.819, 0.7, 0.69]\n",
            "Validation Loss: 0.175, Accuracy of 12 AU classes: [0.912, 0.911, 0.85, 0.876, 0.834, 0.887, 0.886, 0.961, 0.824, 0.936, 0.76, 0.862]\n",
            "Epoch: 9\n",
            "Train Loss: 0.207, Accuracy of 12 AU classes: [0.803, 0.823, 0.769, 0.742, 0.709, 0.668, 0.793, 0.815, 0.857, 0.836, 0.704, 0.706]\n",
            "Validation Loss: 0.172, Accuracy of 12 AU classes: [0.914, 0.909, 0.852, 0.877, 0.835, 0.887, 0.887, 0.945, 0.824, 0.936, 0.762, 0.856]\n",
            "Epoch: 10\n",
            "Train Loss: 0.205, Accuracy of 12 AU classes: [0.809, 0.832, 0.776, 0.747, 0.713, 0.675, 0.797, 0.828, 0.868, 0.849, 0.708, 0.718]\n",
            "Validation Loss: 0.17, Accuracy of 12 AU classes: [0.914, 0.908, 0.856, 0.878, 0.837, 0.888, 0.888, 0.933, 0.824, 0.937, 0.764, 0.852]\n",
            "2min 50s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwS9hLYpFQQM",
        "outputId": "9c4b9e8c-5aa5-4d1a-ae59-fec82ef4d85a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.502, f1_threshold: 0.258, accuracy: [0.914, 0.908, 0.856, 0.878, 0.837, 0.888, 0.888, 0.933, 0.824, 0.937, 0.764, 0.852]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, f1s, f1t, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dTKws0fFQQM"
      },
      "source": [
        "##### EffNet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUn2vMUNFQQM",
        "outputId": "e97326b5-e4e5-47be-e80d-b03275f5ad4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.237, Accuracy of 12 AU classes: [0.831, 0.388, 0.805, 0.657, 0.687, 0.737, 0.774, 0.873, 0.539, 0.495, 0.702, 0.372]\n",
            "Validation Loss: 0.19, Accuracy of 12 AU classes: [0.897, 0.593, 0.896, 0.875, 0.819, 0.889, 0.884, 0.989, 0.176, 0.342, 0.781, 0.812]\n",
            "Epoch: 2\n",
            "Train Loss: 0.216, Accuracy of 12 AU classes: [0.861, 0.625, 0.844, 0.726, 0.708, 0.757, 0.84, 0.904, 0.324, 0.277, 0.726, 0.63]\n",
            "Validation Loss: 0.171, Accuracy of 12 AU classes: [0.915, 0.884, 0.819, 0.89, 0.845, 0.883, 0.895, 0.971, 0.446, 0.642, 0.795, 0.909]\n",
            "Epoch: 3\n",
            "Train Loss: 0.205, Accuracy of 12 AU classes: [0.873, 0.891, 0.858, 0.754, 0.718, 0.768, 0.846, 0.805, 0.51, 0.491, 0.739, 0.72]\n",
            "Validation Loss: 0.164, Accuracy of 12 AU classes: [0.912, 0.915, 0.889, 0.877, 0.83, 0.877, 0.897, 0.987, 0.673, 0.941, 0.78, 0.911]\n",
            "Epoch: 4\n",
            "Train Loss: 0.197, Accuracy of 12 AU classes: [0.88, 0.906, 0.866, 0.771, 0.725, 0.775, 0.851, 0.838, 0.611, 0.323, 0.727, 0.767]\n",
            "Validation Loss: 0.161, Accuracy of 12 AU classes: [0.917, 0.915, 0.875, 0.885, 0.842, 0.88, 0.896, 0.953, 0.72, 0.626, 0.784, 0.908]\n",
            "Epoch: 5\n",
            "Train Loss: 0.191, Accuracy of 12 AU classes: [0.884, 0.915, 0.872, 0.783, 0.731, 0.781, 0.855, 0.86, 0.674, 0.431, 0.736, 0.795]\n",
            "Validation Loss: 0.16, Accuracy of 12 AU classes: [0.931, 0.919, 0.862, 0.886, 0.833, 0.881, 0.896, 0.961, 0.74, 0.799, 0.782, 0.811]\n",
            "Epoch: 6\n",
            "Train Loss: 0.186, Accuracy of 12 AU classes: [0.887, 0.922, 0.877, 0.792, 0.736, 0.785, 0.858, 0.875, 0.717, 0.508, 0.744, 0.814]\n",
            "Validation Loss: 0.16, Accuracy of 12 AU classes: [0.932, 0.92, 0.872, 0.887, 0.85, 0.881, 0.898, 0.968, 0.743, 0.87, 0.782, 0.817]\n",
            "Epoch: 7\n",
            "Train Loss: 0.182, Accuracy of 12 AU classes: [0.889, 0.926, 0.881, 0.831, 0.741, 0.789, 0.861, 0.887, 0.748, 0.565, 0.75, 0.828]\n",
            "Validation Loss: 0.16, Accuracy of 12 AU classes: [0.932, 0.92, 0.85, 0.888, 0.85, 0.888, 0.899, 0.974, 0.75, 0.896, 0.78, 0.824]\n",
            "Epoch: 8\n",
            "Train Loss: 0.178, Accuracy of 12 AU classes: [0.891, 0.93, 0.884, 0.835, 0.745, 0.793, 0.864, 0.896, 0.771, 0.609, 0.756, 0.838]\n",
            "Validation Loss: 0.16, Accuracy of 12 AU classes: [0.932, 0.92, 0.846, 0.901, 0.85, 0.889, 0.898, 0.978, 0.755, 0.904, 0.779, 0.83]\n",
            "Epoch: 9\n",
            "Train Loss: 0.175, Accuracy of 12 AU classes: [0.893, 0.933, 0.887, 0.839, 0.749, 0.796, 0.866, 0.903, 0.79, 0.644, 0.761, 0.846]\n",
            "Validation Loss: 0.16, Accuracy of 12 AU classes: [0.931, 0.921, 0.845, 0.901, 0.848, 0.89, 0.889, 0.98, 0.764, 0.914, 0.775, 0.836]\n",
            "Epoch: 10\n",
            "Train Loss: 0.172, Accuracy of 12 AU classes: [0.894, 0.935, 0.889, 0.842, 0.753, 0.799, 0.868, 0.909, 0.805, 0.671, 0.766, 0.852]\n",
            "Validation Loss: 0.16, Accuracy of 12 AU classes: [0.931, 0.921, 0.843, 0.899, 0.845, 0.89, 0.889, 0.982, 0.766, 0.918, 0.777, 0.842]\n",
            "4min 27s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MpKsUFxFQQM",
        "outputId": "015e9108-0245-4566-ef8a-55181aa30e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.57, f1_threshold: 0.258, accuracy: [0.931, 0.919, 0.862, 0.886, 0.833, 0.881, 0.896, 0.961, 0.74, 0.799, 0.782, 0.811]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n",
        "    AU_model = AU_fusion().to(device)\n",
        "    AU_model.load_state_dict(AU_best_model)\n",
        "    val_loss, f1s, f1t, acc = evaluate_model(AU_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMxiLbHLFQQM",
        "outputId": "0e610ba3-3cf1-4dd0-c003-dd31e5f06994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.299, Accuracy of 12 AU classes: [0.14, 0.205, 0.18, 0.493, 0.472, 0.39, 0.539, 0.043, 0.031, 0.045, 0.63, 0.12]\n",
            "Validation Loss: 0.245, Accuracy of 12 AU classes: [0.284, 0.091, 0.51, 0.888, 0.7, 0.848, 0.886, 0.151, 0.176, 0.21, 0.736, 0.721]\n",
            "Epoch: 2\n",
            "Train Loss: 0.266, Accuracy of 12 AU classes: [0.126, 0.336, 0.5, 0.633, 0.59, 0.511, 0.686, 0.226, 0.099, 0.189, 0.63, 0.191]\n",
            "Validation Loss: 0.212, Accuracy of 12 AU classes: [0.131, 0.908, 0.139, 0.879, 0.797, 0.841, 0.881, 0.988, 0.557, 0.829, 0.737, 0.788]\n",
            "Epoch: 3\n",
            "Train Loss: 0.248, Accuracy of 12 AU classes: [0.148, 0.541, 0.611, 0.679, 0.632, 0.569, 0.731, 0.474, 0.341, 0.444, 0.661, 0.419]\n",
            "Validation Loss: 0.196, Accuracy of 12 AU classes: [0.472, 0.909, 0.397, 0.878, 0.818, 0.844, 0.878, 0.989, 0.721, 0.936, 0.746, 0.895]\n",
            "Epoch: 4\n",
            "Train Loss: 0.237, Accuracy of 12 AU classes: [0.684, 0.643, 0.664, 0.703, 0.656, 0.601, 0.754, 0.599, 0.488, 0.576, 0.676, 0.533]\n",
            "Validation Loss: 0.187, Accuracy of 12 AU classes: [0.694, 0.91, 0.693, 0.878, 0.827, 0.869, 0.878, 0.989, 0.798, 0.936, 0.712, 0.893]\n",
            "Epoch: 5\n",
            "Train Loss: 0.229, Accuracy of 12 AU classes: [0.721, 0.705, 0.696, 0.717, 0.671, 0.623, 0.769, 0.673, 0.583, 0.656, 0.686, 0.597]\n",
            "Validation Loss: 0.181, Accuracy of 12 AU classes: [0.769, 0.92, 0.797, 0.878, 0.83, 0.878, 0.88, 0.989, 0.824, 0.936, 0.745, 0.89]\n",
            "Epoch: 6\n",
            "Train Loss: 0.224, Accuracy of 12 AU classes: [0.745, 0.746, 0.718, 0.727, 0.682, 0.639, 0.778, 0.723, 0.647, 0.708, 0.693, 0.638]\n",
            "Validation Loss: 0.176, Accuracy of 12 AU classes: [0.813, 0.923, 0.83, 0.879, 0.832, 0.884, 0.882, 0.989, 0.824, 0.936, 0.759, 0.885]\n",
            "Epoch: 7\n",
            "Train Loss: 0.219, Accuracy of 12 AU classes: [0.762, 0.774, 0.734, 0.735, 0.69, 0.651, 0.786, 0.758, 0.693, 0.746, 0.699, 0.666]\n",
            "Validation Loss: 0.173, Accuracy of 12 AU classes: [0.836, 0.922, 0.846, 0.88, 0.832, 0.888, 0.884, 0.989, 0.824, 0.936, 0.773, 0.88]\n",
            "Epoch: 8\n",
            "Train Loss: 0.215, Accuracy of 12 AU classes: [0.775, 0.794, 0.747, 0.74, 0.697, 0.661, 0.792, 0.785, 0.727, 0.774, 0.703, 0.687]\n",
            "Validation Loss: 0.17, Accuracy of 12 AU classes: [0.852, 0.922, 0.852, 0.882, 0.832, 0.888, 0.884, 0.987, 0.824, 0.936, 0.778, 0.878]\n",
            "Epoch: 9\n",
            "Train Loss: 0.212, Accuracy of 12 AU classes: [0.784, 0.809, 0.757, 0.745, 0.702, 0.668, 0.796, 0.805, 0.754, 0.796, 0.686, 0.703]\n",
            "Validation Loss: 0.168, Accuracy of 12 AU classes: [0.919, 0.922, 0.855, 0.897, 0.832, 0.889, 0.885, 0.983, 0.824, 0.936, 0.78, 0.874]\n",
            "Epoch: 10\n",
            "Train Loss: 0.21, Accuracy of 12 AU classes: [0.792, 0.821, 0.765, 0.749, 0.707, 0.675, 0.8, 0.82, 0.776, 0.814, 0.69, 0.715]\n",
            "Validation Loss: 0.166, Accuracy of 12 AU classes: [0.921, 0.922, 0.856, 0.883, 0.831, 0.89, 0.886, 0.973, 0.824, 0.936, 0.783, 0.873]\n",
            "2min 49s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agq8c0aRFQQM",
        "outputId": "6f49c990-1571-4325-9679-07e180693fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.524, f1_threshold: 0.233, accuracy: [0.921, 0.922, 0.856, 0.883, 0.831, 0.89, 0.886, 0.973, 0.824, 0.936, 0.783, 0.873]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, f1s, f1t, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnnmn1mlFQQN"
      },
      "source": [
        "##### EffNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tssAKXl5FQQN",
        "outputId": "95d3ed80-af39-44cc-d1dd-5db82455135f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.237, Accuracy of 12 AU classes: [0.747, 0.821, 0.8, 0.698, 0.657, 0.705, 0.8, 0.751, 0.048, 0.053, 0.686, 0.834]\n",
            "Validation Loss: 0.197, Accuracy of 12 AU classes: [0.903, 0.921, 0.852, 0.891, 0.852, 0.891, 0.888, 0.728, 0.187, 0.759, 0.791, 0.696]\n",
            "Epoch: 2\n",
            "Train Loss: 0.217, Accuracy of 12 AU classes: [0.797, 0.885, 0.842, 0.748, 0.703, 0.728, 0.826, 0.604, 0.251, 0.261, 0.717, 0.865]\n",
            "Validation Loss: 0.177, Accuracy of 12 AU classes: [0.893, 0.903, 0.862, 0.88, 0.859, 0.893, 0.902, 0.956, 0.381, 0.728, 0.804, 0.794]\n",
            "Epoch: 3\n",
            "Train Loss: 0.205, Accuracy of 12 AU classes: [0.88, 0.906, 0.859, 0.769, 0.723, 0.74, 0.837, 0.363, 0.411, 0.402, 0.734, 0.876]\n",
            "Validation Loss: 0.17, Accuracy of 12 AU classes: [0.899, 0.934, 0.89, 0.888, 0.859, 0.893, 0.887, 0.812, 0.567, 0.776, 0.789, 0.817]\n",
            "Epoch: 4\n",
            "Train Loss: 0.197, Accuracy of 12 AU classes: [0.883, 0.917, 0.868, 0.781, 0.736, 0.748, 0.844, 0.483, 0.512, 0.493, 0.745, 0.882]\n",
            "Validation Loss: 0.167, Accuracy of 12 AU classes: [0.898, 0.938, 0.879, 0.888, 0.856, 0.893, 0.892, 0.849, 0.668, 0.813, 0.781, 0.901]\n",
            "Epoch: 5\n",
            "Train Loss: 0.191, Accuracy of 12 AU classes: [0.884, 0.924, 0.875, 0.79, 0.746, 0.755, 0.848, 0.56, 0.583, 0.556, 0.753, 0.885]\n",
            "Validation Loss: 0.166, Accuracy of 12 AU classes: [0.897, 0.937, 0.888, 0.881, 0.853, 0.892, 0.892, 0.87, 0.734, 0.845, 0.782, 0.9]\n",
            "Epoch: 6\n",
            "Train Loss: 0.187, Accuracy of 12 AU classes: [0.886, 0.928, 0.88, 0.797, 0.753, 0.76, 0.852, 0.615, 0.634, 0.604, 0.76, 0.887]\n",
            "Validation Loss: 0.166, Accuracy of 12 AU classes: [0.898, 0.906, 0.886, 0.877, 0.866, 0.889, 0.895, 0.884, 0.788, 0.959, 0.785, 0.862]\n",
            "Epoch: 7\n",
            "Train Loss: 0.183, Accuracy of 12 AU classes: [0.887, 0.932, 0.885, 0.803, 0.759, 0.765, 0.855, 0.655, 0.673, 0.643, 0.765, 0.889]\n",
            "Validation Loss: 0.166, Accuracy of 12 AU classes: [0.903, 0.913, 0.882, 0.889, 0.866, 0.889, 0.896, 0.894, 0.804, 0.968, 0.786, 0.871]\n",
            "Epoch: 8\n",
            "Train Loss: 0.179, Accuracy of 12 AU classes: [0.888, 0.934, 0.888, 0.808, 0.764, 0.769, 0.857, 0.686, 0.704, 0.674, 0.77, 0.89]\n",
            "Validation Loss: 0.166, Accuracy of 12 AU classes: [0.904, 0.916, 0.879, 0.886, 0.865, 0.888, 0.896, 0.905, 0.803, 0.972, 0.763, 0.875]\n",
            "Epoch: 9\n",
            "Train Loss: 0.176, Accuracy of 12 AU classes: [0.889, 0.936, 0.891, 0.812, 0.769, 0.772, 0.86, 0.71, 0.728, 0.701, 0.774, 0.891]\n",
            "Validation Loss: 0.167, Accuracy of 12 AU classes: [0.906, 0.921, 0.873, 0.88, 0.866, 0.887, 0.894, 0.913, 0.805, 0.973, 0.764, 0.876]\n",
            "Epoch: 10\n",
            "Train Loss: 0.173, Accuracy of 12 AU classes: [0.89, 0.938, 0.894, 0.816, 0.773, 0.776, 0.862, 0.73, 0.747, 0.723, 0.778, 0.892]\n",
            "Validation Loss: 0.168, Accuracy of 12 AU classes: [0.908, 0.926, 0.87, 0.876, 0.866, 0.886, 0.893, 0.923, 0.806, 0.975, 0.77, 0.877]\n",
            "4min 18s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(AU_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRxpiLQiFQQN",
        "outputId": "e6ddf404-a6a8-4a4a-8fad-a98d89a57533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.542, f1_threshold: 0.25, accuracy: [0.897, 0.937, 0.888, 0.881, 0.853, 0.892, 0.892, 0.87, 0.734, 0.845, 0.782, 0.9]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n",
        "    AU_model = AU_fusion().to(device)\n",
        "    AU_model.load_state_dict(AU_best_model)\n",
        "    val_loss, f1s, f1t, acc = evaluate_model(AU_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYH9Pzv7FQQN",
        "outputId": "12f29d1d-6291-4952-a712-03b02f634e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.313, Accuracy of 12 AU classes: [0.281, 0.339, 0.204, 0.492, 0.504, 0.518, 0.513, 0.568, 0.064, 0.039, 0.627, 0.076]\n",
            "Validation Loss: 0.259, Accuracy of 12 AU classes: [0.119, 0.286, 0.662, 0.891, 0.806, 0.843, 0.889, 0.877, 0.282, 0.072, 0.711, 0.089]\n",
            "Epoch: 2\n",
            "Train Loss: 0.276, Accuracy of 12 AU classes: [0.395, 0.226, 0.296, 0.503, 0.6, 0.498, 0.673, 0.436, 0.05, 0.253, 0.627, 0.086]\n",
            "Validation Loss: 0.212, Accuracy of 12 AU classes: [0.882, 0.749, 0.833, 0.867, 0.818, 0.847, 0.878, 0.731, 0.194, 0.068, 0.721, 0.094]\n",
            "Epoch: 3\n",
            "Train Loss: 0.256, Accuracy of 12 AU classes: [0.558, 0.451, 0.466, 0.587, 0.635, 0.561, 0.636, 0.411, 0.266, 0.235, 0.635, 0.229]\n",
            "Validation Loss: 0.195, Accuracy of 12 AU classes: [0.474, 0.903, 0.193, 0.872, 0.825, 0.849, 0.88, 0.908, 0.769, 0.898, 0.668, 0.616]\n",
            "Epoch: 4\n",
            "Train Loss: 0.244, Accuracy of 12 AU classes: [0.637, 0.574, 0.554, 0.631, 0.532, 0.596, 0.68, 0.544, 0.44, 0.416, 0.648, 0.369]\n",
            "Validation Loss: 0.185, Accuracy of 12 AU classes: [0.699, 0.915, 0.52, 0.875, 0.832, 0.874, 0.882, 0.951, 0.825, 0.936, 0.71, 0.774]\n",
            "Epoch: 5\n",
            "Train Loss: 0.235, Accuracy of 12 AU classes: [0.683, 0.648, 0.608, 0.659, 0.557, 0.619, 0.707, 0.626, 0.545, 0.527, 0.658, 0.463]\n",
            "Validation Loss: 0.18, Accuracy of 12 AU classes: [0.781, 0.922, 0.726, 0.892, 0.837, 0.881, 0.885, 0.964, 0.825, 0.937, 0.734, 0.808]\n",
            "Epoch: 6\n",
            "Train Loss: 0.229, Accuracy of 12 AU classes: [0.712, 0.697, 0.644, 0.677, 0.575, 0.635, 0.784, 0.681, 0.616, 0.601, 0.667, 0.527]\n",
            "Validation Loss: 0.175, Accuracy of 12 AU classes: [0.838, 0.923, 0.809, 0.892, 0.84, 0.886, 0.886, 0.96, 0.825, 0.937, 0.753, 0.827]\n",
            "Epoch: 7\n",
            "Train Loss: 0.224, Accuracy of 12 AU classes: [0.733, 0.73, 0.671, 0.691, 0.589, 0.648, 0.792, 0.72, 0.667, 0.654, 0.674, 0.573]\n",
            "Validation Loss: 0.172, Accuracy of 12 AU classes: [0.868, 0.924, 0.837, 0.893, 0.842, 0.889, 0.886, 0.952, 0.825, 0.937, 0.761, 0.845]\n",
            "Epoch: 8\n",
            "Train Loss: 0.22, Accuracy of 12 AU classes: [0.748, 0.754, 0.691, 0.701, 0.6, 0.658, 0.799, 0.749, 0.704, 0.693, 0.679, 0.606]\n",
            "Validation Loss: 0.17, Accuracy of 12 AU classes: [0.879, 0.927, 0.848, 0.893, 0.843, 0.89, 0.887, 0.945, 0.825, 0.937, 0.767, 0.856]\n",
            "Epoch: 9\n",
            "Train Loss: 0.217, Accuracy of 12 AU classes: [0.76, 0.772, 0.707, 0.709, 0.609, 0.666, 0.804, 0.772, 0.734, 0.724, 0.684, 0.632]\n",
            "Validation Loss: 0.168, Accuracy of 12 AU classes: [0.883, 0.931, 0.855, 0.894, 0.844, 0.882, 0.887, 0.939, 0.825, 0.937, 0.771, 0.859]\n",
            "Epoch: 10\n",
            "Train Loss: 0.214, Accuracy of 12 AU classes: [0.769, 0.787, 0.719, 0.716, 0.617, 0.672, 0.808, 0.789, 0.757, 0.749, 0.687, 0.652]\n",
            "Validation Loss: 0.166, Accuracy of 12 AU classes: [0.923, 0.93, 0.858, 0.863, 0.844, 0.885, 0.888, 0.934, 0.825, 0.937, 0.772, 0.861]\n",
            "2min 44s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, f1s_best, f1t_best, accbest = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, auft, weights, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN5UKQCcFQQN",
        "outputId": "ec30a961-17cd-4991-e0d4-02e8cfe99423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: f1_score 0.513, f1_threshold: 0.217, accuracy: [0.923, 0.93, 0.858, 0.863, 0.844, 0.885, 0.888, 0.934, 0.825, 0.937, 0.772, 0.861]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: f1_score {f1s_best}, f1_threshold: {f1t_best}, accuracy: {accbest}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, f1s, f1t, acc = evaluate_model(mlp_model, val_loader, auft, weights)\n",
        "    print(f'best metric: f1_score {f1s}, f1_threshold: {f1t}, accuracy: {acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iIGwebQcq1G"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK0WMyfYn15t"
      },
      "source": [
        "##### EffNet + Wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dp-enALkGuL",
        "outputId": "4edda0f3-44ea-4444-ffa5-ff705b66fa46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AU_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: f1_score: 0.497, accuracy: [0.86, 0.889, 0.857, 0.79, 0.764, 0.8, 0.858, 0.901, 0.938, 0.932, 0.734, 0.803], f1_threshold: [0.2, 0.1, 0.30000000000000004, 0.30000000000000004, 0.5, 0.4, 0.4, 0.1, 0.1, 0.1, 0.4, 0.1]\n",
            "1min 29s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('AU_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_fusion_{viau}_loss.pth'))\n",
        "AU_model = AU_fusion().to(device)\n",
        "AU_model.load_state_dict(AU_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(AU_model, test_loader, auft, weights1)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(mlp_model, test_loader, visual_feat, auft, weights)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4MvRbcmUhvr",
        "outputId": "0e6d8e5c-c59a-40df-fb4f-3b51bb1aaa1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: f1_score: 0.513, accuracy: [0.875, 0.913, 0.858, 0.813, 0.757, 0.807, 0.863, 0.879, 0.954, 0.926, 0.746, 0.849], f1_threshold: [0.7000000000000001, 0.8, 0.7000000000000001, 0.6, 0.5, 0.6, 0.7000000000000001, 0.6, 0.8, 0.8, 0.30000000000000004, 0.7000000000000001]\n",
            "50.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDIL3d6Fn15u"
      },
      "source": [
        "##### EffNet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgzJR2ryhLo7",
        "outputId": "a1076627-2e25-4dc3-f40a-905a057693c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AU_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: f1_score: 0.494, accuracy: [0.886, 0.914, 0.86, 0.79, 0.772, 0.805, 0.867, 0.907, 0.925, 0.935, 0.76, 0.88], f1_threshold: [0.30000000000000004, 0.2, 0.30000000000000004, 0.4, 0.5, 0.4, 0.5, 0.1, 0.1, 0.2, 0.5, 0.2]\n",
            "1min 26s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('AU_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n",
        "AU_model = AU_fusion().to(device)\n",
        "AU_model.load_state_dict(AU_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(AU_model, test_loader, auft, weights1)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(mlp_model, test_loader, visual_feat, auft, weights)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E8BZdU8L5cM",
        "outputId": "0ba989d8-6d1e-4e98-992a-b05bbd064f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: f1_score: 0.52, accuracy: [0.875, 0.913, 0.849, 0.809, 0.773, 0.8, 0.865, 0.897, 0.932, 0.922, 0.759, 0.849], f1_threshold: [0.7000000000000001, 0.8, 0.6, 0.6, 0.5, 0.5, 0.7000000000000001, 0.7000000000000001, 0.7000000000000001, 0.8, 0.30000000000000004, 0.7000000000000001]\n",
            "48.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrE_pL5Gn15v"
      },
      "source": [
        "##### EffNet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('AU_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_fusion_{viau}.pth'))\n",
        "AU_model = AU_fusion().to(device)\n",
        "AU_model.load_state_dict(AU_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(AU_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udk-286Jvd6u",
        "outputId": "d8ba9b0f-c43b-4d3f-8143-d9009ef04fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AU_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: f1_score: 0.509, accuracy: [0.868, 0.911, 0.854, 0.794, 0.765, 0.793, 0.853, 0.905, 0.943, 0.951, 0.742, 0.838], f1_threshold: [0.7000000000000001, 0.8, 0.5, 0.5, 0.5, 0.5, 0.6, 0.5, 0.7000000000000001, 0.8, 0.30000000000000004, 0.6]\n",
            "1min 28s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}_f1s.pth'))\n",
        "mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(mlp_model, test_loader, visual_feat, auft, weights)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qfv82bH2TicB",
        "outputId": "e9542b29-9fd1-49e6-ff1d-ba5ec8942836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: f1_score: 0.521, accuracy: [0.878, 0.916, 0.864, 0.791, 0.772, 0.795, 0.86, 0.865, 0.933, 0.919, 0.757, 0.846], f1_threshold: [0.7000000000000001, 0.8, 0.7000000000000001, 0.5, 0.5, 0.5, 0.6, 0.6, 0.7000000000000001, 0.8, 0.30000000000000004, 0.7000000000000001]\n",
            "52.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensembling"
      ],
      "metadata": {
        "id": "5weqdQ5IlEH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EffNet_B2 + Wav2Vec2"
      ],
      "metadata": {
        "id": "UuF_NQH6og5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_mlp_{viau}.pth'))\n",
        "mlp_model_wav = MLPModel().to(device)\n",
        "mlp_model_wav.load_state_dict(mlp_best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtZL7YHmqP8V",
        "outputId": "8ffdca6e-7701-467d-947a-91e0884dd90f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(test_loader)\n",
        "all_preds_wav, all_targets_wav = [], []\n",
        "img = []\n",
        "for AU in iterator:\n",
        "    frame = AU['frame']\n",
        "    for imgname in frame:\n",
        "        img.append(imgname)\n",
        "    vis_feat, aud_feat, y = AU[visual_feat], AU[auft], AU['label']\n",
        "    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "    pred, au_pred = mlp_model_wav(vis_feat, aud_feat)\n",
        "    all_preds_wav.extend(au_pred.cpu().tolist())\n",
        "    all_targets_wav.extend(y.cpu().tolist())"
      ],
      "metadata": {
        "id": "Ebs1gDL4og5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EffNet_B2 + Vggish"
      ],
      "metadata": {
        "id": "bLuhTZ7yl-4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiUAUlU-mHO5",
        "outputId": "e1c2c990-a871-4ff1-cf16-b499972fa26c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(test_loader)\n",
        "all_preds, all_targets = [], []\n",
        "img1 = []\n",
        "for AU in iterator:\n",
        "    frame = AU['frame']\n",
        "    for imgname in frame:\n",
        "        img1.append(imgname)\n",
        "    vis_feat, aud_feat, y = AU[visual_feat], AU[auft], AU['label']\n",
        "    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "    pred, au_pred = mlp_model(vis_feat, aud_feat)\n",
        "    all_preds.extend(au_pred.cpu().tolist())\n",
        "    all_targets.extend(y.cpu().tolist())"
      ],
      "metadata": {
        "id": "YI4-pIuyl4oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EffNet_B0 + Vggish"
      ],
      "metadata": {
        "id": "l1GwAugmmQD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_best_model_b0 = torch.load(os.path.join(root,f'models/ABAW6/{vis1}/best_AU_mlp_{viau}.pth'))\n",
        "mlp_model_b0 = MLPModel_b0(num_classes = 12).to(device)\n",
        "mlp_model_b0.load_state_dict(mlp_best_model_b0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxKIp-N0lHwq",
        "outputId": "fc4ef31f-6ac3-4037-bfaa-f89c510ed444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(test_loader_b0)\n",
        "all_preds_b0, all_targets_b0 = [], []\n",
        "img = []\n",
        "for AU in iterator:\n",
        "    frame = AU['frame']\n",
        "    for imgname in frame:\n",
        "        img.append(imgname)\n",
        "    vis_feat, aud_feat, y = AU[visual_feat_1], AU[auft], AU['label']\n",
        "    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "    pred, au_pred = mlp_model_b0(vis_feat, aud_feat)\n",
        "    all_preds_b0.extend(au_pred.cpu().tolist())\n",
        "    all_targets_b0.extend(y.cpu().tolist())"
      ],
      "metadata": {
        "id": "n9oVhLfrmMqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ensemble"
      ],
      "metadata": {
        "id": "FtkuPPPAmXkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B2 + Wav2Vec2 + Vggish"
      ],
      "metadata": {
        "id": "FMJ6g7bgvyCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zipped_data = zip(img1, all_preds, all_targets)\n",
        "zipped_data_wav = zip(img, all_preds_wav, all_targets_wav)\n",
        "\n",
        "sorted_data = sorted(zipped_data)\n",
        "sorted_data_wav = sorted(zipped_data_wav)\n",
        "\n",
        "re_img_wav, re_all_preds_wav, re_all_targets_wav = zip(*sorted_data_wav)\n",
        "re_img, re_all_preds, re_all_targets = zip(*sorted_data)"
      ],
      "metadata": {
        "id": "FPx5OgzSuCsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1s, _, _, thresholds = compute_AU_F1(re_all_preds, re_all_targets_wav)\n",
        "round(f1s,3), thresholds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E_EYDKQuVPx",
        "outputId": "a83875cf-a443-4cac-a7f2-d0265aaef863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.52,\n",
              " [0.7000000000000001,\n",
              "  0.8,\n",
              "  0.6,\n",
              "  0.6,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.7000000000000001,\n",
              "  0.7000000000000001,\n",
              "  0.7000000000000001,\n",
              "  0.8,\n",
              "  0.30000000000000004,\n",
              "  0.7000000000000001])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w in np.linspace(0, 1, 11):\n",
        "    print(f'{w = }')\n",
        "    w = np.array([w])\n",
        "    y_ensemble = w * re_all_preds + (1 - w) * re_all_preds_wav\n",
        "    new_pred = (y_ensemble >= thresholds).astype(int)\n",
        "    f1_scores, f1_thresh, acc, threshold = compute_AU_F1(re_all_targets, new_pred,thresh=np.arange(0.1,1,0.1))\n",
        "    print(f'f1 score: {f1_scores:.3f}, accuracy: {acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAqn6PYPuexu",
        "outputId": "9d79f0ba-a16c-4d0c-9cd8-be5dbef609c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0.0\n",
            "f1 score: 0.512, accuracy: [0.875, 0.913, 0.839, 0.813, 0.757, 0.787, 0.863, 0.913, 0.922, 0.926, 0.746, 0.849]\n",
            "w = 0.1\n",
            "f1 score: 0.515, accuracy: [0.877, 0.914, 0.843, 0.814, 0.759, 0.79, 0.865, 0.914, 0.926, 0.929, 0.747, 0.851]\n",
            "w = 0.2\n",
            "f1 score: 0.517, accuracy: [0.878, 0.915, 0.846, 0.815, 0.762, 0.792, 0.865, 0.915, 0.929, 0.931, 0.749, 0.853]\n",
            "w = 0.30000000000000004\n",
            "f1 score: 0.520, accuracy: [0.88, 0.916, 0.848, 0.816, 0.764, 0.794, 0.866, 0.915, 0.932, 0.931, 0.751, 0.854]\n",
            "w = 0.4\n",
            "f1 score: 0.521, accuracy: [0.88, 0.917, 0.85, 0.816, 0.766, 0.797, 0.866, 0.915, 0.934, 0.932, 0.752, 0.855]\n",
            "w = 0.5\n",
            "f1 score: 0.522, accuracy: [0.88, 0.917, 0.851, 0.816, 0.767, 0.798, 0.866, 0.914, 0.935, 0.931, 0.754, 0.855]\n",
            "w = 0.6000000000000001\n",
            "f1 score: 0.523, accuracy: [0.88, 0.917, 0.852, 0.815, 0.769, 0.799, 0.867, 0.912, 0.936, 0.93, 0.755, 0.855]\n",
            "w = 0.7000000000000001\n",
            "f1 score: 0.523, accuracy: [0.88, 0.916, 0.852, 0.814, 0.77, 0.8, 0.866, 0.909, 0.936, 0.929, 0.756, 0.854]\n",
            "w = 0.8\n",
            "f1 score: 0.522, accuracy: [0.878, 0.915, 0.852, 0.813, 0.772, 0.8, 0.866, 0.906, 0.936, 0.927, 0.758, 0.853]\n",
            "w = 0.9\n",
            "f1 score: 0.521, accuracy: [0.877, 0.914, 0.851, 0.811, 0.772, 0.801, 0.865, 0.901, 0.935, 0.924, 0.759, 0.851]\n",
            "w = 1.0\n",
            "f1 score: 0.520, accuracy: [0.875, 0.913, 0.849, 0.809, 0.773, 0.8, 0.865, 0.897, 0.932, 0.922, 0.759, 0.849]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight=np.array([0.6])\n",
        "y_ensemble = weight * re_all_preds + (1 - weight) * re_all_preds_wav\n",
        "new_pred = ((y_ensemble >= thresholds) * 1)\n",
        "f1_scores, f1_thresh, acc, threshold = compute_AU_F1(re_all_targets, new_pred,thresh=np.arange(0.1,1,0.1))\n",
        "print(f'f1 score: {f1_scores:.3f}, accuracy: {acc}, threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKwyE25lwfSC",
        "outputId": "c45933fb-1ab5-4118-fbd3-ba1cbbb2560d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 score: 0.523, accuracy: [0.88, 0.917, 0.852, 0.815, 0.769, 0.799, 0.867, 0.912, 0.936, 0.93, 0.755, 0.855], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B2 + EffNet_B0 + Vggish"
      ],
      "metadata": {
        "id": "658gH2BQvkKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zipped_data = zip(img1, all_preds, all_targets)\n",
        "zipped_data_b0 = zip(img, all_preds_b0, all_targets_b0)\n",
        "\n",
        "sorted_data = sorted(zipped_data)\n",
        "sorted_data_b0 = sorted(zipped_data_b0)\n",
        "\n",
        "re_img_b0, re_all_preds_b0, re_all_targets_b0 = zip(*sorted_data_b0)\n",
        "re_img, re_all_preds, re_all_targets = zip(*sorted_data)"
      ],
      "metadata": {
        "id": "bp1C3vjCmbcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1s, _, _, thresholds = compute_AU_F1(re_all_preds_b0, re_all_targets)\n",
        "round(f1s,3), thresholds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tt_kRQFoDvA",
        "outputId": "a4d82782-4073-44b4-9fb8-b57d0194dd80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.52,\n",
              " [0.8,\n",
              "  0.9,\n",
              "  0.6,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.6,\n",
              "  0.7000000000000001,\n",
              "  0.7000000000000001,\n",
              "  0.7000000000000001,\n",
              "  0.8,\n",
              "  0.30000000000000004,\n",
              "  0.7000000000000001])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w in np.linspace(0, 1, 11):\n",
        "    print(f'{w = }')\n",
        "    w = np.array([w])\n",
        "    y_ensemble = w * re_all_preds + (1 - w) * re_all_preds_b0\n",
        "    new_pred = (y_ensemble >= thresholds).astype(int)\n",
        "    f1_scores, f1_thresh, acc, threshold = compute_AU_F1(re_all_targets, new_pred,thresh=np.arange(0.1,1,0.1))\n",
        "    print(f'f1 score: {f1_scores:.3f}, accuracy: {acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6scMc5IVBGI",
        "outputId": "33202f9f-2712-492d-f121-5016faad2093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0.0\n",
            "f1 score: 0.520, accuracy: [0.877, 0.915, 0.853, 0.784, 0.78, 0.807, 0.868, 0.932, 0.948, 0.933, 0.777, 0.849], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.1\n",
            "f1 score: 0.524, accuracy: [0.883, 0.92, 0.857, 0.787, 0.783, 0.811, 0.87, 0.934, 0.951, 0.935, 0.778, 0.853], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.2\n",
            "f1 score: 0.528, accuracy: [0.888, 0.923, 0.86, 0.79, 0.786, 0.815, 0.873, 0.936, 0.954, 0.936, 0.779, 0.857], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.30000000000000004\n",
            "f1 score: 0.531, accuracy: [0.892, 0.925, 0.864, 0.794, 0.788, 0.818, 0.875, 0.938, 0.956, 0.937, 0.778, 0.86], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.4\n",
            "f1 score: 0.532, accuracy: [0.894, 0.926, 0.867, 0.797, 0.79, 0.821, 0.876, 0.938, 0.958, 0.937, 0.778, 0.863], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.5\n",
            "f1 score: 0.533, accuracy: [0.895, 0.926, 0.868, 0.8, 0.791, 0.823, 0.876, 0.938, 0.96, 0.937, 0.777, 0.864], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.6000000000000001\n",
            "f1 score: 0.532, accuracy: [0.896, 0.927, 0.867, 0.8, 0.79, 0.825, 0.876, 0.935, 0.96, 0.936, 0.775, 0.864], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.7000000000000001\n",
            "f1 score: 0.530, accuracy: [0.897, 0.927, 0.864, 0.798, 0.787, 0.824, 0.874, 0.93, 0.958, 0.935, 0.773, 0.863], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.8\n",
            "f1 score: 0.527, accuracy: [0.896, 0.927, 0.859, 0.795, 0.783, 0.822, 0.872, 0.92, 0.953, 0.933, 0.769, 0.859], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.9\n",
            "f1 score: 0.522, accuracy: [0.895, 0.928, 0.854, 0.791, 0.778, 0.819, 0.869, 0.909, 0.944, 0.928, 0.765, 0.855], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 1.0\n",
            "f1 score: 0.517, accuracy: [0.892, 0.927, 0.849, 0.786, 0.773, 0.816, 0.865, 0.897, 0.932, 0.922, 0.759, 0.849], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight=np.array([0.5])\n",
        "y_ensemble = weight * re_all_preds + (1 - weight) * re_all_preds_b0\n",
        "new_pred = ((y_ensemble >= thresholds) * 1)\n",
        "f1_scores, f1_thresh, acc, threshold = compute_AU_F1(re_all_targets, new_pred,thresh=np.arange(0.1,1,0.1))\n",
        "print(f'f1 score: {f1_scores:.3f}, accuracy: {acc}, threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv4lZiBhpICn",
        "outputId": "66c9d45e-8ff8-4090-9a6f-ed1fec5a2870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 score: 0.533, accuracy: [0.895, 0.926, 0.868, 0.8, 0.791, 0.823, 0.876, 0.938, 0.96, 0.937, 0.777, 0.864], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_2hrWvEYRTB"
      },
      "source": [
        "### Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tsk = task[1]\n",
        "vis = vis_typ[0]\n",
        "viau = vis_aud[1]\n",
        "auft = audio_feat[1]"
      ],
      "metadata": {
        "id": "hYZpsgSCsm5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index, pred, lab = [], [], []\n",
        "test_vid = {}\n",
        "for i, val in enumerate(re_img):\n",
        "    ind = int(val.split('/')[1][:-4])\n",
        "    vname = val.split('/')[0]\n",
        "    if i == 0:\n",
        "        prename = vname\n",
        "        index.append(ind)\n",
        "        pred.append(new_pred[i])\n",
        "        lab.append(re_all_targets[i])\n",
        "    else:\n",
        "        if vname == prename:\n",
        "            index.append(ind)\n",
        "            pred.append(new_pred[i])\n",
        "            lab.append(re_all_targets[i])\n",
        "        else:\n",
        "            combined = list(zip(index, pred, lab))\n",
        "            combined_sorted = sorted(combined, key=lambda x: x[0])\n",
        "            index_list_sorted, pred_list_sorted, lab_list_sorted = zip(*combined_sorted)\n",
        "            test_vid[prename] = (list(index_list_sorted), list(pred_list_sorted), list(lab_list_sorted))\n",
        "            prename = vname\n",
        "            index, pred, lab = [], [], []\n",
        "            index.append(ind)\n",
        "            pred.append(new_pred[i])\n",
        "            lab.append(re_all_targets[i])"
      ],
      "metadata": {
        "id": "r7PVXOsVvRjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B2 + Wav2Vec2 + Vggish"
      ],
      "metadata": {
        "id": "s3wWM97DwCmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8ZjiiVbHzuK"
      },
      "outputs": [],
      "source": [
        "thresholds = np.array([0.7000000000000001,\n",
        "  0.8,\n",
        "  0.6,\n",
        "  0.6,\n",
        "  0.5,\n",
        "  0.5,\n",
        "  0.7000000000000001,\n",
        "  0.7000000000000001,\n",
        "  0.7000000000000001,\n",
        "  0.8,\n",
        "  0.30000000000000004,\n",
        "  0.7000000000000001])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj6YmjcFEbZA"
      },
      "outputs": [],
      "source": [
        "hyperparams=[(isMean,delta) for delta in [0, 5, 15, 30, 60, 100, 200] for isMean in [1,0] if not (isMean==0 and delta==0)]\n",
        "total_true=[]\n",
        "total_preds=[[] for _ in range(len(hyperparams))]\n",
        "timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n",
        "for videoname,(img, predict, label) in test_vid.items():\n",
        "    for i,ind in enumerate(img):\n",
        "        total_true.append(label[i])\n",
        "    preds_proba = smooth_prediction(img, predict)\n",
        "    for hInd,(isMean,delta) in enumerate(hyperparams):\n",
        "        preds=[]\n",
        "        start = time.time()\n",
        "        for i in range(len(preds_proba)):\n",
        "            i1=max(i-delta,0)\n",
        "            if isMean:\n",
        "                _, proba = slide_window(preds_proba, i, delta, 'mean')\n",
        "            else:\n",
        "                _, proba = slide_window(preds_proba, i, delta, 'median')\n",
        "            aus = (proba>=thresholds)*1\n",
        "            preds.append(aus)\n",
        "        for i,ind in enumerate(img):\n",
        "            if label[i][0]>=-1 and label[i][1]>=-1:\n",
        "                total_preds[hInd].append(preds[ind-1])\n",
        "        end = time.time()\n",
        "        timing_results[(isMean, delta)] += end - start\n",
        "total_true=np.array(total_true)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for hInd, (isMean, delta) in enumerate(hyperparams):\n",
        "    preds = np.array(total_preds[hInd])\n",
        "    accuracy = round((preds == total_true).mean(), 3)\n",
        "    f1 = round(np.mean([f1_score(y_true=total_true[:,i],y_pred=preds[:,i]) for i in range(preds.shape[1])]), 3)\n",
        "    mean_or_median = 'mean' if isMean else 'median'\n",
        "    time_taken = round(timing_results[(isMean, delta)],3)\n",
        "    print(f'{mean_or_median}; delta: {delta}; Acc: {accuracy}; F1: {f1}; Time: {time_taken}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXJKgkzozdiW",
        "outputId": "745c06d8-4a2a-4ed6-d51f-19eb4f6c22ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean; delta: 0; Acc: 0.857; F1: 0.523; Time: 7.502s\n",
            "mean; delta: 5; Acc: 0.869; F1: 0.525; Time: 7.965s\n",
            "median; delta: 5; Acc: 0.863; F1: 0.532; Time: 16.839s\n",
            "mean; delta: 15; Acc: 0.871; F1: 0.511; Time: 8.342s\n",
            "median; delta: 15; Acc: 0.866; F1: 0.534; Time: 17.291s\n",
            "mean; delta: 30; Acc: 0.87; F1: 0.492; Time: 8.718s\n",
            "median; delta: 30; Acc: 0.867; F1: 0.528; Time: 17.746s\n",
            "mean; delta: 60; Acc: 0.868; F1: 0.462; Time: 9.44s\n",
            "median; delta: 60; Acc: 0.866; F1: 0.515; Time: 19.077s\n",
            "mean; delta: 100; Acc: 0.864; F1: 0.431; Time: 11.532s\n",
            "median; delta: 100; Acc: 0.864; F1: 0.502; Time: 21.379s\n",
            "mean; delta: 200; Acc: 0.857; F1: 0.39; Time: 13.451s\n",
            "median; delta: 200; Acc: 0.858; F1: 0.472; Time: 25.658s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B2 + EffNet_B0 + Vggish"
      ],
      "metadata": {
        "id": "CrFMHIHawIU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0erTQzVtwIU6"
      },
      "outputs": [],
      "source": [
        "thresholds = np.array([0.8,\n",
        "  0.9,\n",
        "  0.6,\n",
        "  0.5,\n",
        "  0.5,\n",
        "  0.6,\n",
        "  0.7000000000000001,\n",
        "  0.7000000000000001,\n",
        "  0.7000000000000001,\n",
        "  0.8,\n",
        "  0.30000000000000004,\n",
        "  0.7000000000000001])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBeIjxgcwIU7"
      },
      "outputs": [],
      "source": [
        "hyperparams=[(isMean,delta) for delta in [0, 5, 15, 30, 60, 100, 200] for isMean in [1,0] if not (isMean==0 and delta==0)]\n",
        "total_true=[]\n",
        "total_preds=[[] for _ in range(len(hyperparams))]\n",
        "timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n",
        "for videoname,(img, predict, label) in test_vid.items():\n",
        "    for i,ind in enumerate(img):\n",
        "        total_true.append(label[i])\n",
        "    preds_proba = smooth_prediction(img, predict)\n",
        "    for hInd,(isMean,delta) in enumerate(hyperparams):\n",
        "        preds=[]\n",
        "        start = time.time()\n",
        "        for i in range(len(preds_proba)):\n",
        "            i1=max(i-delta,0)\n",
        "            if isMean:\n",
        "                _, proba = slide_window(preds_proba, i, delta, 'mean')\n",
        "            else:\n",
        "                _, proba = slide_window(preds_proba, i, delta, 'median')\n",
        "            aus = (proba>=thresholds)*1\n",
        "            preds.append(aus)\n",
        "        for i,ind in enumerate(img):\n",
        "            if label[i][0]>=-1 and label[i][1]>=-1:\n",
        "                total_preds[hInd].append(preds[ind-1])\n",
        "        end = time.time()\n",
        "        timing_results[(isMean, delta)] += end - start\n",
        "total_true=np.array(total_true)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for hInd, (isMean, delta) in enumerate(hyperparams):\n",
        "    preds = np.array(total_preds[hInd])\n",
        "    accuracy = round((preds == total_true).mean(), 3)\n",
        "    f1 = round(np.mean([f1_score(y_true=total_true[:,i],y_pred=preds[:,i]) for i in range(preds.shape[1])]), 3)\n",
        "    mean_or_median = 'mean' if isMean else 'median'\n",
        "    time_taken = round(timing_results[(isMean, delta)],3)\n",
        "    print(f'{mean_or_median}; delta: {delta}; Acc: {accuracy}; F1: {f1}; Time: {time_taken}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d852f189-bccd-41a5-c9be-cc3496e00c3d",
        "id": "8t7g7D8NwIU7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean; delta: 0; Acc: 0.871; F1: 0.534; Time: 8.697s\n",
            "mean; delta: 5; Acc: 0.878; F1: 0.524; Time: 9.522s\n",
            "median; delta: 5; Acc: 0.875; F1: 0.54; Time: 22.173s\n",
            "mean; delta: 15; Acc: 0.877; F1: 0.499; Time: 9.23s\n",
            "median; delta: 15; Acc: 0.877; F1: 0.535; Time: 19.396s\n",
            "mean; delta: 30; Acc: 0.876; F1: 0.472; Time: 9.172s\n",
            "median; delta: 30; Acc: 0.877; F1: 0.524; Time: 19.412s\n",
            "mean; delta: 60; Acc: 0.872; F1: 0.437; Time: 9.686s\n",
            "median; delta: 60; Acc: 0.874; F1: 0.506; Time: 20.104s\n",
            "mean; delta: 100; Acc: 0.867; F1: 0.41; Time: 10.824s\n",
            "median; delta: 100; Acc: 0.871; F1: 0.489; Time: 22.654s\n",
            "mean; delta: 200; Acc: 0.86; F1: 0.368; Time: 14.02s\n",
            "median; delta: 200; Acc: 0.864; F1: 0.457; Time: 27.107s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWSTa4TEzNLi"
      },
      "source": [
        "## VA Estimation Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D1rOOtUPGJS"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PpOnAdBZHEf"
      },
      "outputs": [],
      "source": [
        "# Cropped_aligned images\n",
        "vis = vis_typ[0]\n",
        "vis1 = vis_typ[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1E65uF_ZHEl"
      },
      "outputs": [],
      "source": [
        "# Cropped images\n",
        "vis = vis_typ[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6loHg0Oaml6s"
      },
      "source": [
        "#### Effnet + wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qaoMmvejsdR"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[0]\n",
        "viau = vis_aud[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grJntlFqms8l"
      },
      "source": [
        "#### Effnet + vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhVH9IjHmruz"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[1]\n",
        "viau = vis_aud[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SLN9gglm0XA"
      },
      "source": [
        "#### Effnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4LyVKQQm23-"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[2]\n",
        "viau = vis_aud[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvvlnRz-ZRXj"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROpV5jKu6Y8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3506d03b-7422-4f24-fa36-517104e1f0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 285/285 [03:19<00:00,  1.43it/s]\n"
          ]
        }
      ],
      "source": [
        "if auft == audio_feat[0]:\n",
        "    with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[0]}_visual.pkl'), 'rb') as f:\n",
        "        data1 = pickle.load(f)\n",
        "    with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[0]}.txt'), 'r') as f:\n",
        "        vidnames = f.read().splitlines()\n",
        "    task1 = task[2]\n",
        "    feature_a = 'audiofeat_wav2vec2'\n",
        "    feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n",
        "    filenames = os.listdir(feat_root)[:]\n",
        "    for vname in tqdm(vidnames):\n",
        "            feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n",
        "            for imgname, val in feature.items():\n",
        "                if imgname in data1[task1][vname]:\n",
        "                    data1[task1][vname][imgname].update({f'{feature_a}': val})\n",
        "            for img, value in list(data1[task1][vname].items()):\n",
        "                if len(value) < 3:\n",
        "                    data1[task1][vname].pop(img)\n",
        "elif auft == audio_feat[1]:\n",
        "    with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[0]}_visual.pkl'), 'rb') as f:\n",
        "        data1 = pickle.load(f)\n",
        "    with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[0]}.txt'), 'r') as f:\n",
        "        vidnames = f.read().splitlines()\n",
        "    task1 = task[2]\n",
        "    feature_a = 'audiofeat_vggish'\n",
        "    feat_root = os.path.join(root + '/models/ABAW6', feature_a)\n",
        "    filenames = os.listdir(feat_root)[:]\n",
        "    for vname in tqdm(vidnames):\n",
        "            feature = np.load(os.path.join(feat_root, f'{vname}.npy'), allow_pickle=True).tolist()\n",
        "            for imgname, val in feature.items():\n",
        "                if imgname in data1[task1][vname]:\n",
        "                    data1[task1][vname][imgname].update({f'{feature_a}': val})\n",
        "            for img, value in list(data1[task1][vname].items()):\n",
        "                if len(value) < 3:\n",
        "                    data1[task1][vname].pop(img)\n",
        "else:\n",
        "    with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[0]}_{viau}.pkl'), 'rb') as f:\n",
        "        data1 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCJoUSEEZRXk",
        "outputId": "313b6270-7f1c-41d8-ec2c-f1a0462c6107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 285/285 [00:00<00:00, 1425.58it/s]\n"
          ]
        }
      ],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[2]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[0]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data1[task1][vname])\n",
        "    for img in data1[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z24CCEkwZRXk"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data1, iname, dims, task1)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MwWOEO-ZRXk"
      },
      "source": [
        "#### Val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJuunI3sZRXk"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[1]}_{viau}.pkl'), 'rb') as f:\n",
        "    data2 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5riL0y7UZRXk",
        "outputId": "7c8ee9f2-b22a-4a8e-fd26-c7fdd836e991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 71/71 [00:00<00:00, 1084.70it/s]\n"
          ]
        }
      ],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[2]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[1]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data2[task1][vname])\n",
        "    for img in data2[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRI-XOs9ZRXk"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data2, iname, dims, task1)\n",
        "val_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM-R-dPNZRXk"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiWBsFEXZRXk"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n",
        "    data3 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUNvkmldZRXk",
        "outputId": "997b759c-c937-47b6-e3ab-872568ce835d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 1175.35it/s]\n"
          ]
        }
      ],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[2]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[2]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data3[task1][vname])\n",
        "    for img in data3[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAm67bXaZRXk"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data3, iname, dims, task1)\n",
        "test_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo7YJPxtPMXQ"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVrhnVSdrkn9"
      },
      "outputs": [],
      "source": [
        "class VA_fusion(nn.Module):\n",
        "    def __init__(self, batchsize = batch_size, audio_ft = auft, hidden_size = [512, 128, batch_size]):\n",
        "        super(VA_fusion, self).__init__()\n",
        "        self.batchsize = batchsize\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.hidden_size = hidden_size\n",
        "        self.feat_fc = nn.Conv1d(self.concat_dim, hidden_size[0], 1, padding=0)\n",
        "        self.activ = nn.LeakyReLU(0.1)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.conv1 = nn.Conv1d(hidden_size[0], hidden_size[1], 1, padding=0)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size[1], nhead=4, dim_feedforward=hidden_size[1], dropout=0.3)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "        self.vhead = nn.Sequential(\n",
        "                nn.Linear(hidden_size[1], hidden_size[2]),\n",
        "                nn.BatchNorm1d(hidden_size[2]),\n",
        "                nn.Linear(hidden_size[2], 1),\n",
        "                )\n",
        "        self.ahead = nn.Sequential(\n",
        "                nn.Linear(hidden_size[1], hidden_size[2]),\n",
        "                nn.BatchNorm1d(hidden_size[2]),\n",
        "                nn.Linear(hidden_size[2], 1),\n",
        "                )\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs,dim=1)\n",
        "        feat = torch.transpose(feat,0,1)\n",
        "        feat = self.feat_fc(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.conv1(feat)\n",
        "        out = torch.transpose(out,0,1)\n",
        "        out = self.transformer_encoder(out)\n",
        "        vout = self.vhead(out)\n",
        "        aout = self.ahead(out)\n",
        "\n",
        "        return vout, aout, torch.tanh(vout), torch.tanh(aout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGReEGQ_PQb7",
        "outputId": "e16eae90-8806-4872-904c-d943d0bb3417"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VA_fusion(\n",
              "  (feat_fc): Conv1d(2176, 512, kernel_size=(1,), stride=(1,))\n",
              "  (activ): LeakyReLU(negative_slope=0.1)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (vhead): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              "  (ahead): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "VA_model = VA_fusion().to(device)\n",
        "VA_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE3lHnVrxBMl"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, audio_ft = auft, num_classes=1):\n",
        "        super(MLPModel, self).__init__()\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.feat_fc = nn.Conv1d(self.concat_dim, 512, 1, padding=0)\n",
        "        self.vhead = nn.Sequential(\n",
        "            nn.Linear(512, 128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        self.ahead = nn.Sequential(\n",
        "            nn.Linear(self.concat_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs, dim=1)\n",
        "        vfeat = self.feat_fc(torch.transpose(feat,0,1))\n",
        "        vfeat = torch.transpose(vfeat,0,1)\n",
        "        vout = self.vhead(vfeat)\n",
        "        aout = self.ahead(feat)\n",
        "\n",
        "        return vout, aout, torch.tanh(vout), torch.tanh(aout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM0fnofbXuz5",
        "outputId": "9cb06aae-0317-4ac1-820d-8141f32f89fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPModel(\n",
              "  (feat_fc): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n",
              "  (vhead): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.1)\n",
              "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (ahead): Sequential(\n",
              "    (0): Linear(in_features=1536, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJLovdcDNKGK"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, au_feat):\n",
        "    model.eval()\n",
        "    total_loss = []\n",
        "    all_targets = []\n",
        "    all_preds = []\n",
        "    mse = []\n",
        "    with torch.no_grad():\n",
        "        iterator = iter(data_loader)\n",
        "        for i in range(len(data_loader)):\n",
        "            VA = next(iterator)\n",
        "            if au_feat == 'nope':\n",
        "                vis_feat, y = VA[visual_feat], VA['label']\n",
        "                vis_feat, y = vis_feat.to(device), y.to(device)\n",
        "                aud_feat = None\n",
        "            else:\n",
        "                vis_feat, aud_feat, y = VA[visual_feat], VA[au_feat], VA['label']\n",
        "                vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "            Vpred, Apred, v_pred, a_pred = model(vis_feat, aud_feat)\n",
        "            mse_loss, ccc_loss = compute_VA_loss(Vpred, Apred, y)\n",
        "            total_loss.append(ccc_loss.item())\n",
        "            mse.append(mse_loss.item())\n",
        "            preds = torch.cat((v_pred, a_pred), dim=1)\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_targets.extend(y.cpu().tolist())\n",
        "\n",
        "    ccc1, ccc2 = compute_VA_CCC(all_preds, all_targets)\n",
        "    return round(np.mean(total_loss),3), round(np.mean(mse),3), round(ccc1,3), round(ccc2,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtGySUkSxXcn"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERu2L4x_NKGK"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, VA_model.parameters()), lr=0.00005, betas=(0.9, 0.999), weight_decay=0.00001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doLSNbv2yXN7"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, mlp_model.parameters()), lr=0.0005, betas=(0.9, 0.999), weight_decay=0.0001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "ulTD4kf4k2Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, mod_type, train_loader, val_loader, epoch, batch_size, optim, scheduler, au_feat, vis_aud):\n",
        "\n",
        "    model.train(True)\n",
        "    model.eval()\n",
        "    loss_value = []\n",
        "    loss_train = []\n",
        "    loss_val = []\n",
        "    loss_mse = []\n",
        "    cc1best, cc2best = 0, 0\n",
        "\n",
        "    for e in range(epoch):\n",
        "        print(f'Training Epoch: {e+1}')\n",
        "        iterator = iter(train_loader)\n",
        "        for VA in iterator:\n",
        "                if au_feat == 'nope':\n",
        "                    vis_feat, y = VA[visual_feat_1], VA['label']\n",
        "                    vis_feat, y = vis_feat.to(device), y.to(device)\n",
        "                    aud_feat = None\n",
        "                else:\n",
        "                    vis_feat, aud_feat, y = VA[visual_feat_1], VA[au_feat], VA['label']\n",
        "                    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "                model.zero_grad()\n",
        "                Vpred, Apred, v_pred, a_pred = model(vis_feat, aud_feat)\n",
        "                mse_loss, ccc_loss = compute_VA_loss(Vpred, Apred, y)\n",
        "                ccc_loss.backward()\n",
        "                optim.step()\n",
        "                loss_value.append(ccc_loss.item())\n",
        "                loss_mse.append(mse_loss.item())\n",
        "                preds = torch.cat((v_pred, a_pred), dim=1)\n",
        "\n",
        "        avg_loss = round(np.mean(loss_value),3)\n",
        "        loss_train.append(avg_loss)\n",
        "        print(f'Train Loss: {avg_loss}, mse: {round(np.mean(loss_mse),3)}')\n",
        "\n",
        "        val_loss, mse, ccc1, ccc2 = evaluate_model(model, val_loader, au_feat)\n",
        "        loss_val.append(val_loss)\n",
        "\n",
        "        if ccc1 > cc1best:\n",
        "            torch.save(model.state_dict(), os.path.join(root,f'models/ABAW6/{vis}/best_VA_{mod_type}_{vis_aud}.pth'))\n",
        "            cc1best = ccc1\n",
        "            cc2best = ccc2\n",
        "            best_loss = val_loss\n",
        "            best_mse = mse\n",
        "\n",
        "        print(f'Validation Loss: {val_loss}, mse: {mse}')\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "    return loss_train, loss_val, best_loss, best_mse, cc1best, cc2best"
      ],
      "metadata": {
        "id": "8qOEk4JVk38s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, au_feat):\n",
        "    model.eval()\n",
        "    total_loss = []\n",
        "    all_targets = []\n",
        "    all_preds = []\n",
        "    mse = []\n",
        "    with torch.no_grad():\n",
        "        iterator = iter(data_loader)\n",
        "        for i in range(len(data_loader)):\n",
        "            VA = next(iterator)\n",
        "            if au_feat == 'nope':\n",
        "                vis_feat, y = VA[visual_feat_1], VA['label']\n",
        "                vis_feat, y = vis_feat.to(device), y.to(device)\n",
        "                aud_feat = None\n",
        "            else:\n",
        "                vis_feat, aud_feat, y = VA[visual_feat_1], VA[au_feat], VA['label']\n",
        "                vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "            Vpred, Apred, v_pred, a_pred = model(vis_feat, aud_feat)\n",
        "            mse_loss, ccc_loss = compute_VA_loss(Vpred, Apred, y)\n",
        "            total_loss.append(ccc_loss.item())\n",
        "            mse.append(mse_loss.item())\n",
        "            preds = torch.cat((v_pred, a_pred), dim=1)\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_targets.extend(y.cpu().tolist())\n",
        "\n",
        "    ccc1, ccc2 = compute_VA_CCC(all_preds, all_targets)\n",
        "    return round(np.mean(total_loss),3), round(np.mean(mse),3), round(ccc1,3), round(ccc2,3)"
      ],
      "metadata": {
        "id": "hXQ8CCHflZEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XbMm4GXClh9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxfN5hGJjXuJ"
      },
      "source": [
        "#### Cropped_aligned images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYUtuFWpp0GA"
      },
      "source": [
        "##### Effnet + Wav2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgSFLdF6QpAo",
        "outputId": "2b852f37-bde4-465a-a4c4-eeaef9d39660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 0.405, mse: 0.482\n",
            "Validation Loss: 1.42, mse: 0.35\n",
            "Training Epoch: 2\n",
            "Train Loss: 0.329, mse: 0.48\n",
            "Validation Loss: 1.42, mse: 0.35\n",
            "Training Epoch: 3\n",
            "Train Loss: 0.292, mse: 0.48\n",
            "Validation Loss: 1.461, mse: 0.329\n",
            "Training Epoch: 4\n",
            "Train Loss: 0.269, mse: 0.479\n",
            "Validation Loss: 1.464, mse: 0.339\n",
            "Training Epoch: 5\n",
            "Train Loss: 0.252, mse: 0.479\n",
            "Validation Loss: 1.491, mse: 0.337\n",
            "Training Epoch: 6\n",
            "Train Loss: 0.239, mse: 0.479\n",
            "Validation Loss: 1.473, mse: 0.334\n",
            "Training Epoch: 7\n",
            "Train Loss: 0.228, mse: 0.479\n",
            "Validation Loss: 1.481, mse: 0.339\n",
            "Training Epoch: 8\n",
            "Train Loss: 0.218, mse: 0.479\n",
            "Validation Loss: 1.479, mse: 0.343\n",
            "Training Epoch: 9\n",
            "Train Loss: 0.21, mse: 0.478\n",
            "Validation Loss: 1.5, mse: 0.346\n",
            "Training Epoch: 10\n",
            "Train Loss: 0.203, mse: 0.478\n",
            "Validation Loss: 1.467, mse: 0.347\n",
            "1h 48min 44s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0ueJmX8NKGL",
        "outputId": "c18705ef-d20d-41c0-89e1-ea9bc1053c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: CCC_Valence 0.352, CCC_Arousal: 0.48\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_fusion_{viau}.pth'))\n",
        "    VA_model = VA_fusion().to(device)\n",
        "    VA_model.load_state_dict(VA_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9uTDWD8NQ-j",
        "outputId": "946d1fe8-cc5f-49c1-f5e3-1e5fcbb9ea48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 0.522, mse: 0.484\n",
            "Validation Loss: 1.396, mse: 0.338\n",
            "Training Epoch: 2\n",
            "Train Loss: 0.473, mse: 0.481\n",
            "Validation Loss: 1.424, mse: 0.346\n",
            "Training Epoch: 3\n",
            "Train Loss: 0.452, mse: 0.48\n",
            "Validation Loss: 1.426, mse: 0.346\n",
            "Training Epoch: 4\n",
            "Train Loss: 0.44, mse: 0.48\n",
            "Validation Loss: 1.412, mse: 0.34\n",
            "Training Epoch: 5\n",
            "Train Loss: 0.432, mse: 0.48\n",
            "Validation Loss: 1.433, mse: 0.342\n",
            "Training Epoch: 6\n",
            "Train Loss: 0.426, mse: 0.48\n",
            "Validation Loss: 1.44, mse: 0.338\n",
            "Training Epoch: 7\n",
            "Train Loss: 0.42, mse: 0.479\n",
            "Validation Loss: 1.435, mse: 0.339\n",
            "Training Epoch: 8\n",
            "Train Loss: 0.416, mse: 0.479\n",
            "Validation Loss: 1.432, mse: 0.338\n",
            "Training Epoch: 9\n",
            "Train Loss: 0.412, mse: 0.479\n",
            "Validation Loss: 1.432, mse: 0.346\n",
            "Training Epoch: 10\n",
            "Train Loss: 0.409, mse: 0.479\n",
            "Validation Loss: 1.434, mse: 0.338\n",
            "45min 37s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "169Uv6e79hXa",
        "outputId": "fac7f80e-72a2-4982-de37-6c9e9f9e9ab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: CCC_Valence 0.38, CCC_Arousal: 0.445\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}_mse.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKP6ysxfPDWm"
      },
      "source": [
        "##### Effnet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6971bc66-c726-4142-c4f6-4913cf5d073b",
        "id": "pJr0SV49PDW0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 0.422, mse: 0.482\n",
            "Validation Loss: 1.354, mse: 0.335\n",
            "Training Epoch: 2\n",
            "Train Loss: 0.331, mse: 0.48\n",
            "Validation Loss: 1.393, mse: 0.35\n",
            "Training Epoch: 3\n",
            "Train Loss: 0.286, mse: 0.48\n",
            "Validation Loss: 1.423, mse: 0.336\n",
            "Training Epoch: 4\n",
            "Train Loss: 0.256, mse: 0.479\n",
            "Validation Loss: 1.443, mse: 0.343\n",
            "Training Epoch: 5\n",
            "Train Loss: 0.234, mse: 0.479\n",
            "Validation Loss: 1.447, mse: 0.345\n",
            "Training Epoch: 6\n",
            "Train Loss: 0.217, mse: 0.479\n",
            "Validation Loss: 1.43, mse: 0.345\n",
            "Training Epoch: 7\n",
            "Train Loss: 0.204, mse: 0.479\n",
            "Validation Loss: 1.465, mse: 0.342\n",
            "Training Epoch: 8\n",
            "Train Loss: 0.192, mse: 0.479\n",
            "Validation Loss: 1.467, mse: 0.333\n",
            "Training Epoch: 9\n",
            "Train Loss: 0.182, mse: 0.479\n",
            "Validation Loss: 1.49, mse: 0.335\n",
            "Training Epoch: 10\n",
            "Train Loss: 0.173, mse: 0.478\n",
            "Validation Loss: 1.466, mse: 0.341\n",
            "1h 38min 41s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59a00d1-6c06-4f71-ee4b-3298ce510791",
        "id": "ECMLmm_mPDW1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: CCC_Valence 0.367, CCC_Arousal: 0.381\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_fusion_{viau}.pth'))\n",
        "    VA_model = VA_fusion().to(device)\n",
        "    VA_model.load_state_dict(VA_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1802095-5f5f-40db-89ac-1e784664a7cc",
        "id": "ju35-Mt8PDW1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 1.218, mse: 0.461\n",
            "Validation Loss: 1.176, mse: 0.29\n",
            "Training Epoch: 2\n",
            "Train Loss: 1.178, mse: 0.468\n",
            "Validation Loss: 1.183, mse: 0.308\n",
            "Training Epoch: 3\n",
            "Train Loss: 1.157, mse: 0.469\n",
            "Validation Loss: 1.195, mse: 0.315\n",
            "Training Epoch: 4\n",
            "Train Loss: 1.144, mse: 0.469\n",
            "Validation Loss: 1.174, mse: 0.308\n",
            "Training Epoch: 5\n",
            "Train Loss: 1.135, mse: 0.471\n",
            "Validation Loss: 1.216, mse: 0.335\n",
            "Training Epoch: 6\n",
            "Train Loss: 1.126, mse: 0.471\n",
            "Validation Loss: 1.234, mse: 0.341\n",
            "Training Epoch: 7\n",
            "Train Loss: 1.12, mse: 0.472\n",
            "Validation Loss: 1.232, mse: 0.341\n",
            "Training Epoch: 8\n",
            "Train Loss: 1.113, mse: 0.473\n",
            "Validation Loss: 1.212, mse: 0.352\n",
            "Training Epoch: 9\n",
            "Train Loss: 1.107, mse: 0.474\n",
            "Validation Loss: 1.218, mse: 0.352\n",
            "Training Epoch: 10\n",
            "Train Loss: 1.102, mse: 0.475\n",
            "Validation Loss: 1.222, mse: 0.37\n",
            "1min 17s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c188f28-7c80-442c-a6a8-77035f3ad0bb",
        "id": "jF0MvzpUPDW1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best metric: CCC_Valence 0.412, CCC_Arousal: 0.486, Mean CCC: 0.449\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, Mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7u9hUAvqCkN"
      },
      "source": [
        "##### Effnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0IqDIwEuZfE",
        "outputId": "16881cf1-3730-4c9f-a91c-20309843f996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 0.458, mse: 0.483\n",
            "Validation Loss: 1.45, mse: 0.276\n",
            "Training Epoch: 2\n",
            "Train Loss: 0.377, mse: 0.481\n",
            "Validation Loss: 1.492, mse: 0.274\n",
            "Training Epoch: 3\n",
            "Train Loss: 0.338, mse: 0.48\n",
            "Validation Loss: 1.537, mse: 0.286\n",
            "Training Epoch: 4\n",
            "Train Loss: 0.312, mse: 0.48\n",
            "Validation Loss: 1.532, mse: 0.256\n",
            "Training Epoch: 5\n",
            "Train Loss: 0.294, mse: 0.48\n",
            "Validation Loss: 1.564, mse: 0.261\n",
            "Training Epoch: 6\n",
            "Train Loss: 0.279, mse: 0.479\n",
            "Validation Loss: 1.636, mse: 0.27\n",
            "Training Epoch: 7\n",
            "Train Loss: 0.266, mse: 0.479\n",
            "Validation Loss: 1.606, mse: 0.29\n",
            "Training Epoch: 8\n",
            "Train Loss: 0.256, mse: 0.479\n",
            "Validation Loss: 1.503, mse: 0.277\n",
            "Training Epoch: 9\n",
            "Train Loss: 0.248, mse: 0.479\n",
            "Validation Loss: 1.628, mse: 0.275\n",
            "Training Epoch: 10\n",
            "Train Loss: 0.24, mse: 0.479\n",
            "Validation Loss: 1.566, mse: 0.279\n",
            "1h 27min 13s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8qfvZ1yz1wS",
        "outputId": "14f15e1b-dd6f-4d0f-a668-6f7b8d6bf4eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best metric: CCC_Valence 0.185, CCC_Arousal: 0.338\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_fusion_{viau}.pth'))\n",
        "    VA_model = VA_fusion().to(device)\n",
        "    VA_model.load_state_dict(VA_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpEjTc_6OGoR",
        "outputId": "3958b486-f607-4f5f-abe8-aa941df2fd26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 1.281, mse: 0.467\n",
            "Validation Loss: 1.254, mse: 0.284\n",
            "Training Epoch: 2\n",
            "Train Loss: 1.236, mse: 0.466\n",
            "Validation Loss: 1.245, mse: 0.299\n",
            "Training Epoch: 3\n",
            "Train Loss: 1.214, mse: 0.468\n",
            "Validation Loss: 1.24, mse: 0.313\n",
            "Training Epoch: 4\n",
            "Train Loss: 1.197, mse: 0.467\n",
            "Validation Loss: 1.236, mse: 0.317\n",
            "Training Epoch: 5\n",
            "Train Loss: 1.184, mse: 0.469\n",
            "Validation Loss: 1.231, mse: 0.303\n",
            "Training Epoch: 6\n",
            "Train Loss: 1.175, mse: 0.469\n",
            "Validation Loss: 1.253, mse: 0.308\n",
            "Training Epoch: 7\n",
            "Train Loss: 1.169, mse: 0.469\n",
            "Validation Loss: 1.249, mse: 0.306\n",
            "Training Epoch: 8\n",
            "Train Loss: 1.161, mse: 0.471\n",
            "Validation Loss: 1.245, mse: 0.324\n",
            "Training Epoch: 9\n",
            "Train Loss: 1.155, mse: 0.472\n",
            "Validation Loss: 1.243, mse: 0.325\n",
            "Training Epoch: 10\n",
            "Train Loss: 1.15, mse: 0.473\n",
            "Validation Loss: 1.254, mse: 0.317\n",
            "1min 9s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"
      ],
      "metadata": {
        "id": "pSqtPFJzlkL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daK4JRW_jjjz"
      },
      "source": [
        "#### Cropped images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQG-B8Bujjjz"
      },
      "source": [
        "##### Effnet + Wav2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYvHRCrZjjj0",
        "outputId": "43e91ee3-e73e-4559-fc1d-e33a973fa025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 0.903, mse: 0.504\n",
            "Validation Loss: 1.389, mse: 0.254\n",
            "Training Epoch: 2\n",
            "Train Loss: 0.793, mse: 0.494\n",
            "Validation Loss: 1.377, mse: 0.243\n",
            "Training Epoch: 3\n",
            "Train Loss: 0.74, mse: 0.491\n",
            "Validation Loss: 1.356, mse: 0.244\n",
            "Training Epoch: 4\n",
            "Train Loss: 0.704, mse: 0.49\n",
            "Validation Loss: 1.324, mse: 0.232\n",
            "Training Epoch: 5\n",
            "Train Loss: 0.678, mse: 0.489\n",
            "Validation Loss: 1.32, mse: 0.206\n",
            "Training Epoch: 6\n",
            "Train Loss: 0.657, mse: 0.488\n",
            "Validation Loss: 1.384, mse: 0.236\n",
            "Training Epoch: 7\n",
            "Train Loss: 0.639, mse: 0.488\n",
            "Validation Loss: 1.384, mse: 0.236\n",
            "Training Epoch: 8\n",
            "Train Loss: 0.624, mse: 0.488\n",
            "Validation Loss: 1.329, mse: 0.22\n",
            "Training Epoch: 9\n",
            "Train Loss: 0.611, mse: 0.487\n",
            "Validation Loss: 1.31, mse: 0.253\n",
            "Training Epoch: 10\n",
            "Train Loss: 0.599, mse: 0.487\n",
            "Validation Loss: 1.295, mse: 0.228\n",
            "3min 35s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7IVnP_sjjj0",
        "outputId": "c106a45d-9a5e-4655-aaeb-e8e1e6050786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: CCC_Valence 0.329, CCC_Arousal: 0.39\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n",
        "    VA_model = VA_fusion().to(device)\n",
        "    VA_model.load_state_dict(VA_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5zzFa3GUUMz",
        "outputId": "8baa7981-fad1-4a8a-ee5a-c7f45c46bea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 1.279, mse: 0.464\n",
            "Validation Loss: 1.261, mse: 0.315\n",
            "Training Epoch: 2\n",
            "Train Loss: 1.231, mse: 0.47\n",
            "Validation Loss: 1.242, mse: 0.302\n",
            "Training Epoch: 3\n",
            "Train Loss: 1.209, mse: 0.472\n",
            "Validation Loss: 1.202, mse: 0.277\n",
            "Training Epoch: 4\n",
            "Train Loss: 1.192, mse: 0.473\n",
            "Validation Loss: 1.218, mse: 0.275\n",
            "Training Epoch: 5\n",
            "Train Loss: 1.179, mse: 0.473\n",
            "Validation Loss: 1.178, mse: 0.264\n",
            "Training Epoch: 6\n",
            "Train Loss: 1.166, mse: 0.474\n",
            "Validation Loss: 1.139, mse: 0.254\n",
            "Training Epoch: 7\n",
            "Train Loss: 1.158, mse: 0.475\n",
            "Validation Loss: 1.163, mse: 0.262\n",
            "Training Epoch: 8\n",
            "Train Loss: 1.15, mse: 0.476\n",
            "Validation Loss: 1.147, mse: 0.263\n",
            "Training Epoch: 9\n",
            "Train Loss: 1.144, mse: 0.477\n",
            "Validation Loss: 1.153, mse: 0.265\n",
            "Training Epoch: 10\n",
            "Train Loss: 1.137, mse: 0.477\n",
            "Validation Loss: 1.161, mse: 0.257\n",
            "1min 33s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP3ZYh89UXE1",
        "outputId": "4b41b6d1-f6c0-47f8-c557-8c0ab7155626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: CCC_Valence 0.501, CCC_Arousal: 0.523\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TELrToCUaxP"
      },
      "source": [
        "##### Effnet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tIb6S0cUaxP",
        "outputId": "5dffcb03-d2c8-4087-c91b-070ba3a2f526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 0.904, mse: 0.484\n",
            "Validation Loss: 1.319, mse: 0.281\n",
            "Training Epoch: 2\n",
            "Train Loss: 0.808, mse: 0.483\n",
            "Validation Loss: 1.338, mse: 0.283\n",
            "Training Epoch: 3\n",
            "Train Loss: 0.753, mse: 0.482\n",
            "Validation Loss: 1.357, mse: 0.28\n",
            "Training Epoch: 4\n",
            "Train Loss: 0.712, mse: 0.482\n",
            "Validation Loss: 1.367, mse: 0.275\n",
            "Training Epoch: 5\n",
            "Train Loss: 0.678, mse: 0.481\n",
            "Validation Loss: 1.375, mse: 0.271\n",
            "Training Epoch: 6\n",
            "Train Loss: 0.649, mse: 0.481\n",
            "Validation Loss: 1.383, mse: 0.269\n",
            "Training Epoch: 7\n",
            "Train Loss: 0.622, mse: 0.481\n",
            "Validation Loss: 1.4, mse: 0.269\n",
            "Training Epoch: 8\n",
            "Train Loss: 0.599, mse: 0.48\n",
            "Validation Loss: 1.417, mse: 0.269\n",
            "Training Epoch: 9\n",
            "Train Loss: 0.577, mse: 0.48\n",
            "Validation Loss: 1.434, mse: 0.273\n",
            "Training Epoch: 10\n",
            "Train Loss: 0.556, mse: 0.48\n",
            "Validation Loss: 1.445, mse: 0.272\n",
            "3min 7s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVjp84klUaxP",
        "outputId": "b4cb27b1-7708-4ea8-c817-82ef5684f580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: CCC_Valence 0.215, CCC_Arousal: 0.456\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n",
        "    VA_model = VA_fusion().to(device)\n",
        "    VA_model.load_state_dict(VA_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFf8pMDtUaxQ",
        "outputId": "a37c0dd7-7bbb-4778-969b-022a7ff7698a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 1.286, mse: 0.45\n",
            "Validation Loss: 1.298, mse: 0.352\n",
            "Training Epoch: 2\n",
            "Train Loss: 1.241, mse: 0.458\n",
            "Validation Loss: 1.312, mse: 0.349\n",
            "Training Epoch: 3\n",
            "Train Loss: 1.221, mse: 0.462\n",
            "Validation Loss: 1.321, mse: 0.349\n",
            "Training Epoch: 4\n",
            "Train Loss: 1.207, mse: 0.464\n",
            "Validation Loss: 1.328, mse: 0.347\n",
            "Training Epoch: 5\n",
            "Train Loss: 1.197, mse: 0.465\n",
            "Validation Loss: 1.333, mse: 0.346\n",
            "Training Epoch: 6\n",
            "Train Loss: 1.189, mse: 0.466\n",
            "Validation Loss: 1.338, mse: 0.345\n",
            "Training Epoch: 7\n",
            "Train Loss: 1.182, mse: 0.467\n",
            "Validation Loss: 1.343, mse: 0.345\n",
            "Training Epoch: 8\n",
            "Train Loss: 1.176, mse: 0.468\n",
            "Validation Loss: 1.346, mse: 0.345\n",
            "Training Epoch: 9\n",
            "Train Loss: 1.171, mse: 0.468\n",
            "Validation Loss: 1.35, mse: 0.345\n",
            "Training Epoch: 10\n",
            "Train Loss: 1.166, mse: 0.469\n",
            "Validation Loss: 1.354, mse: 0.345\n",
            "1min 18s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3DhVcYTUaxQ",
        "outputId": "23b132f6-c71b-4d93-dc5f-5a9c97604a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: CCC_Valence 0.34, CCC_Arousal: 0.337\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrKjh40GUfo8"
      },
      "source": [
        "##### Effnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usoBNxvqUfpD",
        "outputId": "e4b1f127-4f2e-4de0-854a-7aeb644d174d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 0.961, mse: 0.488\n",
            "Validation Loss: 1.517, mse: 0.328\n",
            "Training Epoch: 2\n",
            "Train Loss: 0.859, mse: 0.488\n",
            "Validation Loss: 1.522, mse: 0.31\n",
            "Training Epoch: 3\n",
            "Train Loss: 0.805, mse: 0.487\n",
            "Validation Loss: 1.546, mse: 0.298\n",
            "Training Epoch: 4\n",
            "Train Loss: 0.766, mse: 0.487\n",
            "Validation Loss: 1.558, mse: 0.288\n",
            "Training Epoch: 5\n",
            "Train Loss: 0.735, mse: 0.487\n",
            "Validation Loss: 1.572, mse: 0.282\n",
            "Training Epoch: 6\n",
            "Train Loss: 0.708, mse: 0.487\n",
            "Validation Loss: 1.584, mse: 0.276\n",
            "Training Epoch: 7\n",
            "Train Loss: 0.685, mse: 0.487\n",
            "Validation Loss: 1.599, mse: 0.275\n",
            "Training Epoch: 8\n",
            "Train Loss: 0.664, mse: 0.487\n",
            "Validation Loss: 1.604, mse: 0.273\n",
            "Training Epoch: 9\n",
            "Train Loss: 0.644, mse: 0.487\n",
            "Validation Loss: 1.619, mse: 0.273\n",
            "Training Epoch: 10\n",
            "Train Loss: 0.626, mse: 0.486\n",
            "Validation Loss: 1.632, mse: 0.274\n",
            "2min 56s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss,best_mse, cccv, ccca = train(VA_model, model_type[0], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyDBMb7nUfpE",
        "outputId": "44795669-65a3-4f01-819f-2932f712011d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: CCC_Valence 0.141, CCC_Arousal: 0.324\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n",
        "    VA_model = VA_fusion().to(device)\n",
        "    VA_model.load_state_dict(VA_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-972eOPUfpF",
        "outputId": "20c1d000-1a12-4540-f6ac-e8a0e15a8b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Epoch: 1\n",
            "Train Loss: 1.357, mse: 0.469\n",
            "Validation Loss: 1.34, mse: 0.352\n",
            "Training Epoch: 2\n",
            "Train Loss: 1.315, mse: 0.473\n",
            "Validation Loss: 1.34, mse: 0.336\n",
            "Training Epoch: 3\n",
            "Train Loss: 1.296, mse: 0.474\n",
            "Validation Loss: 1.338, mse: 0.332\n",
            "Training Epoch: 4\n",
            "Train Loss: 1.283, mse: 0.475\n",
            "Validation Loss: 1.333, mse: 0.329\n",
            "Training Epoch: 5\n",
            "Train Loss: 1.274, mse: 0.476\n",
            "Validation Loss: 1.328, mse: 0.326\n",
            "Training Epoch: 6\n",
            "Train Loss: 1.266, mse: 0.477\n",
            "Validation Loss: 1.323, mse: 0.323\n",
            "Training Epoch: 7\n",
            "Train Loss: 1.26, mse: 0.477\n",
            "Validation Loss: 1.318, mse: 0.32\n",
            "Training Epoch: 8\n",
            "Train Loss: 1.254, mse: 0.478\n",
            "Validation Loss: 1.314, mse: 0.318\n",
            "Training Epoch: 9\n",
            "Train Loss: 1.248, mse: 0.478\n",
            "Validation Loss: 1.311, mse: 0.316\n",
            "Training Epoch: 10\n",
            "Train Loss: 1.244, mse: 0.478\n",
            "Validation Loss: 1.309, mse: 0.314\n",
            "1min 9s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "loss_train, loss_val, best_loss, best_mse, cccv, ccca = train(mlp_model, model_type[1], train_loader, val_loader, 10, 32, optimizer, scheduler, auft, viau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEKYHuAsUfpF",
        "outputId": "f6dfaedf-5937-44ba-c882-31696ea7cab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best metric: CCC_Valence 0.376, CCC_Arousal: 0.366\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f'best metric: CCC_Valence {cccv}, CCC_Arousal: {ccca}')\n",
        "except:\n",
        "    mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_mlp_{viau}.pth'))\n",
        "    mlp_model = MLPModel().to(device)\n",
        "    mlp_model.load_state_dict(mlp_best_model)\n",
        "    val_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, val_loader, auft)\n",
        "    print(f'best metric: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew3FhCIJpw8_"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ns5iRAm4oYx"
      },
      "source": [
        "##### EffNet + Wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('VA_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n",
        "VA_model = VA_fusion().to(device)\n",
        "VA_model.load_state_dict(VA_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPvOsc7N_8H0",
        "outputId": "1ffad90f-0558-47d0-f63d-d7e2b1100da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: CCC_Valence 0.294, CCC_Arousal: 0.495, mean CCC: 0.394\n",
            "58.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV42AtZG6TeT",
        "outputId": "84e6f24d-8a36-4889-ea8b-fde58389597b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: CCC_Valence 0.391, CCC_Arousal: 0.527, mean CCC: 0.459\n",
            "29 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt5aLtmJOpAo"
      },
      "source": [
        "##### EffNet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f71d80-a4c1-45cc-c9fd-c08f9e9d6736",
        "id": "JyoYc56hTST_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: CCC_Valence 0.293, CCC_Arousal: 0.458, mean CCC: 0.376\n",
            "57.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('VA_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_fusion_{viau}.pth'))\n",
        "VA_model = VA_fusion().to(device)\n",
        "VA_model.load_state_dict(VA_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5adc2300-5014-42df-8144-bb47ef3076b8",
        "id": "at7sa9hFTd3p"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: CCC_Valence 0.365, CCC_Arousal: 0.516, mean CCC: 0.44\n",
            "28.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjhZnJHIVTYi"
      },
      "source": [
        "##### EffNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83eb007a-b4f2-4f73-8648-2e04215f2faa",
        "id": "sV4dFA244oYy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: CCC_Valence 0.27, CCC_Arousal: 0.377, mean CCC: 0.324\n",
            "55.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('VA_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_fusion_{viau}.pth'))\n",
        "VA_model = VA_fusion().to(device)\n",
        "VA_model.load_state_dict(VA_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqlMzaNU6F59",
        "outputId": "622fa89a-00f4-4fa7-b91e-29e9b71e8aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: CCC_Valence 0.366, CCC_Arousal: 0.464, mean CCC: 0.415\n",
            "26.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensembling"
      ],
      "metadata": {
        "id": "gUqT9ZA9Z40c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EffNet_B2 + Wav2vec2"
      ],
      "metadata": {
        "id": "iSAgNzqdZ40d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wav2vec2\n",
        "vis = vis_typ[0]\n",
        "auft = audio_feat[0]\n",
        "viau = vis_aud[0]"
      ],
      "metadata": {
        "id": "8RtLom-RZ7FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n",
        "mlp_model_wav = MLPModel().to(device)\n",
        "mlp_model_wav.load_state_dict(mlp_best_model)\n",
        "evaluate_model(mlp_model_wav, test_loader, auft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6yJ-D9Udfda",
        "outputId": "a5471c7a-b754-4f9f-aee7-276346f73ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.488, 0.276, 0.391, 0.527)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(test_loader)\n",
        "all_preds, all_targets = [], []\n",
        "img1 = []\n",
        "for i in range(len(test_loader)):\n",
        "    VA = next(iterator)\n",
        "    frame = VA['frame']\n",
        "    for imgname in frame:\n",
        "        img1.append(imgname)\n",
        "    vis_feat, aud_feat, y = VA[visual_feat], VA[auft], VA['label']\n",
        "    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "    Vpred, Apred, v_pred, a_pred = mlp_model_wav(vis_feat, aud_feat)\n",
        "    preds = torch.cat((v_pred, a_pred), dim=1)\n",
        "    all_preds.extend(preds.cpu().tolist())\n",
        "    all_targets.extend(y.cpu().tolist())"
      ],
      "metadata": {
        "id": "9n_-zAcaZ40d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_VA_CCC(all_preds, all_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFk2TB74b2xB",
        "outputId": "9fda28d0-2b9a-422d-f0ec-f21b7d7e4a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3908915320970614, 0.5266098927052195)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EffNet_B2 + Vggish"
      ],
      "metadata": {
        "id": "jN-gJd1dZ40d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_best_mod = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n",
        "mlp_model_vgg = MLPModel().to(device)\n",
        "mlp_model_vgg.load_state_dict(mlp_best_mod)\n",
        "evaluate_model(mlp_model_vgg, test_loader, auft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFFBWqdafHR3",
        "outputId": "368b5718-a309-4bc4-c51e-056073e3b676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.483, 0.287, 0.354, 0.533)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(test_loader)\n",
        "all_preds_vgg, all_targets_vgg = [], []\n",
        "img = []\n",
        "for i in range(len(test_loader)):\n",
        "    VA = next(iterator)\n",
        "    frame = VA['frame']\n",
        "    for imgname in frame:\n",
        "        img.append(imgname)\n",
        "    vis_feat, aud_feat, y = VA[visual_feat], VA[auft], VA['label']\n",
        "    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "    Vpred, Apred, v_pred, a_pred = mlp_model_vgg(vis_feat, aud_feat)\n",
        "    preds = torch.cat((v_pred, a_pred), dim=1)\n",
        "    all_preds_vgg.extend(preds.cpu().tolist())\n",
        "    all_targets_vgg.extend(y.cpu().tolist())"
      ],
      "metadata": {
        "id": "GwGYrnWcZ40e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_VA_CCC(all_preds_vgg, all_targets_vgg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpheRjTmf9NP",
        "outputId": "22759895-6a04-4aa2-e72d-f366cfa19342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.35368417669976915, 0.53293580621636)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ensemble"
      ],
      "metadata": {
        "id": "dCpokCoNZ40e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zipped_data = zip(img1, all_preds, all_targets)\n",
        "zipped_data_vgg = zip(img, all_preds_vgg, all_targets_vgg)\n",
        "\n",
        "sorted_data = sorted(zipped_data)\n",
        "sorted_data_vgg = sorted(zipped_data_vgg)\n",
        "\n",
        "re_img_vgg, re_all_preds_vgg, re_all_targets_vgg = zip(*sorted_data_vgg)\n",
        "re_img, re_all_preds, re_all_targets = zip(*sorted_data)"
      ],
      "metadata": {
        "id": "feHjSBg-Z40e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_VA_CCC(re_all_preds_vgg, re_all_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51eea9d-9e33-401f-c833-a69d3a698f14",
        "id": "rsoJrx8bZ40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3536841766997691, 0.53293580621636)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w in np.linspace(0, 1, 11):\n",
        "    print(f'{w = }')\n",
        "    w = np.array([w])\n",
        "    y_ensemble = w * re_all_preds + (1 - w) * re_all_preds_vgg\n",
        "    cccv, ccca = compute_VA_CCC(y_ensemble, re_all_targets)\n",
        "    print(f'CCCV: {cccv:.3f}, CCCA: {ccca:.3f}, Mean CCCC: {(cccv+ccca)/2:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80f4399-aa9e-4f20-ea6f-52e0fac2659b",
        "id": "TDOqsyVHZ40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0.0\n",
            "CCCV: 0.354, CCCA: 0.533, Mean CCCC: 0.443\n",
            "w = 0.1\n",
            "CCCV: 0.364, CCCA: 0.538, Mean CCCC: 0.451\n",
            "w = 0.2\n",
            "CCCV: 0.374, CCCA: 0.541, Mean CCCC: 0.458\n",
            "w = 0.30000000000000004\n",
            "CCCV: 0.382, CCCA: 0.544, Mean CCCC: 0.463\n",
            "w = 0.4\n",
            "CCCV: 0.389, CCCA: 0.545, Mean CCCC: 0.467\n",
            "w = 0.5\n",
            "CCCV: 0.393, CCCA: 0.545, Mean CCCC: 0.469\n",
            "w = 0.6000000000000001\n",
            "CCCV: 0.396, CCCA: 0.544, Mean CCCC: 0.470\n",
            "w = 0.7000000000000001\n",
            "CCCV: 0.398, CCCA: 0.541, Mean CCCC: 0.470\n",
            "w = 0.8\n",
            "CCCV: 0.397, CCCA: 0.538, Mean CCCC: 0.467\n",
            "w = 0.9\n",
            "CCCV: 0.395, CCCA: 0.533, Mean CCCC: 0.464\n",
            "w = 1.0\n",
            "CCCV: 0.391, CCCA: 0.527, Mean CCCC: 0.459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vid = zip(img1, y_ensemble, re_all_targets)"
      ],
      "metadata": {
        "id": "v35SaX4jZ40f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight=np.array([0.6])\n",
        "y_ensemble = weight * re_all_preds + (1 - weight) * re_all_preds_vgg\n",
        "cccv, ccca = compute_VA_CCC(y_ensemble, re_all_targets)\n",
        "print(f'CCCV: {cccv:.3f}, CCCA: {ccca:.3f}, Mean CCCC: {(cccv+ccca)/2:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45001d6a-2229-4358-c6f0-e2cf5a1a4355",
        "id": "DtzxzId4Z40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CCCV: 0.396, CCCA: 0.544, Mean CCCC: 0.470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmE8Fo_YSyzW"
      },
      "source": [
        "### Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index, pred, lab = [], [], []\n",
        "test_vid = {}\n",
        "for i, val in enumerate(re_img):\n",
        "    ind = int(val.split('/')[1][:-4])\n",
        "    vname = val.split('/')[0]\n",
        "    if i == 0:\n",
        "        prename = vname\n",
        "        index.append(ind)\n",
        "        pred.append(y_ensemble[i])\n",
        "        lab.append(re_all_targets[i])\n",
        "    else:\n",
        "        if vname == prename:\n",
        "            index.append(ind)\n",
        "            pred.append(y_ensemble[i])\n",
        "            lab.append(re_all_targets[i])\n",
        "        else:\n",
        "            combined = list(zip(index, pred, lab))\n",
        "            combined_sorted = sorted(combined, key=lambda x: x[0])\n",
        "            index_list_sorted, pred_list_sorted, lab_list_sorted = zip(*combined_sorted)\n",
        "            test_vid[prename] = (list(index_list_sorted), list(pred_list_sorted), list(lab_list_sorted))\n",
        "            prename = vname\n",
        "            index, pred, lab = [], [], []\n",
        "            index.append(ind)\n",
        "            pred.append(y_ensemble[i])\n",
        "            lab.append(re_all_targets[i])"
      ],
      "metadata": {
        "id": "ZT-I9tDYhhTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69oAXZnpax2r"
      },
      "outputs": [],
      "source": [
        "hyperparams=[(isMean,delta) for delta in [0, 5, 15, 30, 60, 100, 200] for isMean in [1,0] if not (isMean==0 and delta==0)]\n",
        "total_true=[]\n",
        "total_preds=[[] for _ in range(len(hyperparams))]\n",
        "timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n",
        "for videoname,(img, predict, label) in test_vid.items():\n",
        "    for i,ind in enumerate(img):\n",
        "        total_true.append(label[i])\n",
        "    preds_proba = smooth_prediction(img, predict)\n",
        "    for hInd,(isMean,delta) in enumerate(hyperparams):\n",
        "        preds=[]\n",
        "        start = time.time()\n",
        "        for i in range(len(preds_proba)):\n",
        "            i1=max(i-delta,0)\n",
        "            if isMean:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'mean')\n",
        "            else:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'median')\n",
        "            preds.append(proba)\n",
        "        for i, ind in enumerate(img):\n",
        "            if label[i][0]>=-1 and label[i][1]>=-1:\n",
        "                total_preds[hInd].append(preds[ind-1])\n",
        "        end = time.time()\n",
        "        timing_results[(isMean, delta)] += end - start\n",
        "total_true=np.array(total_true)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams=[(isMean,delta) for delta in [15, 20, 25, 30, 35, 40] for isMean in [1,0] if not (isMean==0 and delta==0)]\n",
        "total_true=[]\n",
        "total_preds=[[] for _ in range(len(hyperparams))]\n",
        "timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n",
        "for videoname,(img, predict, label) in test_vid.items():\n",
        "    for i,ind in enumerate(img):\n",
        "        total_true.append(label[i])\n",
        "    preds_proba = smooth_prediction(img, predict)\n",
        "    for hInd,(isMean,delta) in enumerate(hyperparams):\n",
        "        preds=[]\n",
        "        start = time.time()\n",
        "        for i in range(len(preds_proba)):\n",
        "            i1=max(i-delta,0)\n",
        "            if isMean:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'mean')\n",
        "            else:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'median')\n",
        "            preds.append(proba)\n",
        "        for i, ind in enumerate(img):\n",
        "            if label[i][0]>=-1 and label[i][1]>=-1:\n",
        "                total_preds[hInd].append(preds[ind-1])\n",
        "        end = time.time()\n",
        "        timing_results[(isMean, delta)] += end - start\n",
        "total_true=np.array(total_true)"
      ],
      "metadata": {
        "id": "RaZ6omEPW0_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for hInd, (isMean, delta) in enumerate(hyperparams):\n",
        "    preds = np.array(total_preds[hInd])\n",
        "    ccc1, ccc2 = compute_VA_CCC(preds, total_true)\n",
        "    mean_or_median = 'mean' if isMean else 'median'\n",
        "    time_taken = round(timing_results[(isMean, delta)],3)\n",
        "    print(f'{mean_or_median}; delta: {delta}; CCCV: {ccc1:.3f}; CCCA: {ccc2:.3f}; Mean CCC: {(ccc1+ccc2)/2:.3f} Time: {time_taken}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpS0ik2UiFij",
        "outputId": "12b4e1e0-adb7-49c5-e3c9-dca88c633eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean; delta: 0; CCCV: 0.399; CCCA: 0.546; Mean CCC: 0.473 Time: 5.635s\n",
            "mean; delta: 5; CCCV: 0.411; CCCA: 0.569; Mean CCC: 0.490 Time: 6.035s\n",
            "median; delta: 5; CCCV: 0.411; CCCA: 0.567; Mean CCC: 0.489 Time: 13.1s\n",
            "mean; delta: 15; CCCV: 0.418; CCCA: 0.585; Mean CCC: 0.502 Time: 5.745s\n",
            "median; delta: 15; CCCV: 0.419; CCCA: 0.584; Mean CCC: 0.502 Time: 13.37s\n",
            "mean; delta: 30; CCCV: 0.421; CCCA: 0.594; Mean CCC: 0.507 Time: 5.922s\n",
            "median; delta: 30; CCCV: 0.424; CCCA: 0.596; Mean CCC: 0.510 Time: 13.567s\n",
            "mean; delta: 60; CCCV: 0.414; CCCA: 0.585; Mean CCC: 0.500 Time: 6.428s\n",
            "median; delta: 60; CCCV: 0.420; CCCA: 0.594; Mean CCC: 0.507 Time: 14.063s\n",
            "mean; delta: 100; CCCV: 0.402; CCCA: 0.562; Mean CCC: 0.482 Time: 6.961s\n",
            "median; delta: 100; CCCV: 0.409; CCCA: 0.575; Mean CCC: 0.492 Time: 14.75s\n",
            "mean; delta: 200; CCCV: 0.377; CCCA: 0.516; Mean CCC: 0.447 Time: 7.423s\n",
            "median; delta: 200; CCCV: 0.379; CCCA: 0.531; Mean CCC: 0.455 Time: 16.118s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "97AGawC8PnRo",
        "pmzvE5czs3hX",
        "HoofZGhrU6Ih",
        "Kjx794E5bdTe",
        "ibOhaO0vAOvH",
        "sd8sIrCrwZIT",
        "RsUw4AxuwtBo",
        "8vFUXiAyjdSs",
        "47UnYX3TjhOd",
        "0_2hrWvEYRTB",
        "7twP-V0AHU5w"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}