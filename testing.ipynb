{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-2pUaxmWffM"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KZakgtAWYL4",
        "outputId": "07a6bfbd-a725-42cb-aa42-3808c565aa4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kz2jt7ThWh34"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch import optim\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cOgcvBAfjNTH"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uuVAI-13Affl"
      },
      "outputs": [],
      "source": [
        "root = '/content/drive/MyDrive/MSc/Thesis'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgoCljH14WXu"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HU8CMY9CS-7K"
      },
      "outputs": [],
      "source": [
        "def get_names(vid, id):\n",
        "    name = \"\"\n",
        "    if id>=0 and id<10:\n",
        "        name = f\"{vid}/0000\" + str(id) + \".jpg\"\n",
        "    elif id>=10 and id<100:\n",
        "        name = f\"{vid}/000\" + str(id) + \".jpg\"\n",
        "    elif id>=100 and id<1000:\n",
        "        name = f\"{vid}/00\" + str(id) + \".jpg\"\n",
        "    elif id>=1000 and id<10000:\n",
        "        name = f\"{vid}/0\" + str(id) + \".jpg\"\n",
        "    else:\n",
        "        name = f\"{vid}/\" + str(id) + \".jpg\"\n",
        "    return name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yObcGVx9re2M"
      },
      "outputs": [],
      "source": [
        "class ABAW_dataset1(Dataset):\n",
        "    def __init__(self, data, iname, dims, task):\n",
        "        self.data = data\n",
        "        self.iname = iname\n",
        "        self.task = task\n",
        "        self.feature_dims = dims\n",
        "    def __getitem__(self, index):\n",
        "        frame = self.iname[index]\n",
        "        vname = frame.split('/')[0]\n",
        "        data = self.data[self.task][vname][frame]\n",
        "        data['frame'] = frame\n",
        "        data['vid'] = vname\n",
        "        data['label'] = self.data[self.task][vname][frame]['label']\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "            return self.feature_dims"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97AGawC8PnRo"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZvRVuFzAkfc"
      },
      "source": [
        "### Compute loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zrddZOWU9WGc"
      },
      "outputs": [],
      "source": [
        "def compute_EXP_loss(pred, label, weights):\n",
        "    cri_exp = nn.CrossEntropyLoss(weights)\n",
        "    cls_loss = cri_exp(pred, label)\n",
        "    return cls_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_AU_loss(pred, label, class_weights):\n",
        "    class_weights = torch.from_numpy(class_weights).to(device)\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, label.float(), reduction='none')\n",
        "    weights = (class_weights[:, 0]**(1 - label)) * (class_weights[:, 1]**label)\n",
        "    weighted_bce = bce * weights\n",
        "    loss = torch.mean(weighted_bce)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "C3HV86yF6N0z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o2BwARxBRcwF"
      },
      "outputs": [],
      "source": [
        "def CCC_loss(x, y):\n",
        "    x, y = x.view(-1), y.view(-1)\n",
        "    vx = x - torch.mean(x)\n",
        "    vy = y - torch.mean(y)\n",
        "    rho =  torch.sum(vx * vy) / (torch.sqrt(torch.sum(torch.pow(vx, 2))) * torch.sqrt(torch.sum(torch.pow(vy, 2)))+1e-8)\n",
        "    x_m, y_m = torch.mean(x), torch.mean(y)\n",
        "    x_s, y_s = torch.std(x), torch.std(y)\n",
        "    ccc = 2*rho*x_s*y_s/(torch.pow(x_s, 2) + torch.pow(y_s, 2) + torch.pow(x_m - y_m, 2))\n",
        "    return 1-ccc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V7kdT2piBc_G"
      },
      "outputs": [],
      "source": [
        "def compute_VA_loss(Vout,Aout,label):\n",
        "    ccc_loss = CCC_loss(Vout[:,0],label[:,0]) + CCC_loss(Aout[:,0],label[:,1])\n",
        "    mse_loss = nn.MSELoss()(Vout,label[:,0]) + nn.MSELoss()(Aout,label[:,1])\n",
        "    return mse_loss,ccc_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd_uu91kAhJE"
      },
      "source": [
        "### Compute F1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rhrQC_kWxaqa"
      },
      "outputs": [],
      "source": [
        "def compute_EXP_F1(pred, target):\n",
        "    pred_labels = np.argmax(pred, axis=1)\n",
        "    target_labels = np.argmax(target, axis=1)\n",
        "    macro_f1 = f1_score(target_labels,pred_labels,average='macro')\n",
        "    acc = accuracy_score(target_labels, pred_labels)\n",
        "    return macro_f1, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ke0-RvC2mi69"
      },
      "outputs": [],
      "source": [
        "def f1s_max_AU(label, pred, thresh, i=0):\n",
        "    pred = np.array(pred)\n",
        "    label = np.array(label)\n",
        "    label = label[:,i]\n",
        "    pred = pred[:,i]\n",
        "    acc = []\n",
        "    F1 = []\n",
        "    for i in thresh:\n",
        "        new_pred = ((pred >= i) * 1).flatten()\n",
        "        acc.append(accuracy_score(label.flatten(), new_pred))\n",
        "        F1.append(f1_score(label.flatten(), new_pred))\n",
        "\n",
        "    F1_MAX = max(F1)\n",
        "    if F1_MAX < 0 or math.isnan(F1_MAX):\n",
        "        F1_MAX = 0\n",
        "        F1_THRESH = 0\n",
        "        accuracy = 0\n",
        "    else:\n",
        "        idx_thresh = np.argmax(F1)\n",
        "        F1_THRESH = thresh[idx_thresh]\n",
        "        accuracy = acc[idx_thresh]\n",
        "    return F1, F1_MAX, F1_THRESH, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QdDnfAtpmuKr"
      },
      "outputs": [],
      "source": [
        "def compute_AU_F1(pred,label,thresh=np.arange(0.1,1,0.1)):\n",
        "    F1s = []\n",
        "    F1t = []\n",
        "    acc = []\n",
        "    for i in range(12):\n",
        "        F1, F1_MAX, F1_THRESH, accuracy = f1s_max_AU(label,pred,thresh,i)\n",
        "        F1s.append(F1_MAX)\n",
        "        F1t.append(F1_THRESH)\n",
        "        acc.append(accuracy)\n",
        "    acc = [round(a,3) for a in acc]\n",
        "    return np.mean(F1s),np.mean(F1t),acc, F1t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSYSCjAnLcf3"
      },
      "source": [
        "### Concordance Correlation Coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TDWknJ9zKyQA"
      },
      "outputs": [],
      "source": [
        "def CCC_score(x, y):\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    vx = x - np.mean(x)\n",
        "    vy = y - np.mean(y)\n",
        "    rho = np.sum(vx * vy) / (np.sqrt(np.sum(vx**2)) * np.sqrt(np.sum(vy**2)))\n",
        "    x_m = np.mean(x)\n",
        "    y_m = np.mean(y)\n",
        "    x_s = np.std(x)\n",
        "    y_s = np.std(y)\n",
        "    ccc = 2*rho*x_s*y_s/(x_s**2 + y_s**2 + (x_m - y_m)**2)\n",
        "    return ccc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NMqwnXAhKLcO"
      },
      "outputs": [],
      "source": [
        "def compute_VA_CCC(x,y):\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    x[x>1] = 1\n",
        "    x[x<-1] = -1\n",
        "    ccc1 = CCC_score(x[:,0],y[:,0])\n",
        "    ccc2 = CCC_score(x[:,1],y[:,1])\n",
        "\n",
        "    return ccc1,ccc2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmzvE5czs3hX"
      },
      "source": [
        "# Smooth utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "m9Nyajzm5_ww"
      },
      "outputs": [],
      "source": [
        "def smooth_prediction(img, predict):\n",
        "    cur_ind = 0\n",
        "    preds_proba = []\n",
        "    if img:\n",
        "        for i in range(img[-1]):\n",
        "            if img[cur_ind] - 1 == i:\n",
        "                preds_proba.append(predict[cur_ind])\n",
        "                cur_ind += 1\n",
        "            else:\n",
        "                if cur_ind == 0:\n",
        "                    preds_proba.append(predict[cur_ind])\n",
        "                else:\n",
        "                    w = (i - img[cur_ind - 1] + 1) / (img[cur_ind] - img[cur_ind - 1])\n",
        "                    pred = w * predict[cur_ind - 1] + (1 - w) * predict[cur_ind]\n",
        "                    preds_proba.append(pred)\n",
        "        return np.array([p.cpu().detach().numpy() for p in preds_proba]) #np.array(preds_proba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8y-D0QuO9ubK"
      },
      "outputs": [],
      "source": [
        "def slide_window(preds_proba, i, delta, typ):\n",
        "    i1 = max(i - delta, 0)\n",
        "    if typ == 'mean':\n",
        "        proba = np.mean(preds_proba[i1:i+delta+1], axis=0)\n",
        "    elif typ == 'median':\n",
        "        proba = np.median(preds_proba[i1:i+delta+1], axis=0)\n",
        "    else:\n",
        "        proba = np.mean(preds_proba[i1:i+delta+1:int(typ)], axis=0)\n",
        "    return np.argmax(proba), proba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwMmqGyJwnZg"
      },
      "source": [
        "# Challenges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zhIC4geYv9HJ"
      },
      "outputs": [],
      "source": [
        "task = ['EXPR_Recognition_Challenge','AU_Detection_Challenge','VA_Estimation_Challenge']\n",
        "split = ['Train_Set', 'Validation_Set']\n",
        "typ = ['Train','Val','Test']\n",
        "vis_typ = ['cropped_aligned', 'cropped', 'cropped_aligned_b0']\n",
        "visual_feat = 'visualfeat_enet_b2_8_best'\n",
        "visual_feat_1 = 'visualfeat_enet_b0_8_va_mtl'\n",
        "audio_feat = ['audiofeat_wav2vec2','audiofeat_vggish','nope']\n",
        "vis_aud = ['visual_wav2vec2','visual_vggish','visual']\n",
        "batch_size = 32\n",
        "model_type = ['fusion', 'mlp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoofZGhrU6Ih"
      },
      "source": [
        "## EXPR Recognition Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubi-WwKO6a1R"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PBMZbYSi7ya1"
      },
      "outputs": [],
      "source": [
        "# Cropped_aligned images\n",
        "vis = vis_typ[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56gxaRmr5pIT"
      },
      "source": [
        "#### Effnet + wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IPSj-MXy5pIT"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[0]\n",
        "viau = vis_aud[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx7Mwlg35pIT"
      },
      "source": [
        "#### Effnet + vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZpK72FP5pIT"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[1]\n",
        "viau = vis_aud[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM4A0l6x5pIU"
      },
      "source": [
        "#### Effnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5Utm-WR5pIU"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[2]\n",
        "viau = vis_aud[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekKJDTam5pIV"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0wIAuSo5pIV"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{task[0]}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n",
        "    data3 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPKBKszq5pIV",
        "outputId": "440ca65f-4ace-4f25-fc1a-b0d863da2078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [00:00<00:00, 1553.79it/s]\n"
          ]
        }
      ],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[0]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[0]}/{typ[2]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data3[task1][vname])\n",
        "    for img in data3[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBPEYhxI5pIW"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data3, iname, dims, task1)\n",
        "test_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5qpMbpk6faK"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer encoder"
      ],
      "metadata": {
        "id": "XP9zBPUAWfHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKhsZIv4Wod4"
      },
      "outputs": [],
      "source": [
        "class EXP_fusion(nn.Module):\n",
        "    def __init__(self, batchsize = batch_size, audio_ft = auft, hidden_size = [512, 128, batch_size]):\n",
        "        super(EXP_fusion, self).__init__()\n",
        "        self.batchsize = batchsize\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.hidden_size = hidden_size\n",
        "        self.feat_fc = nn.Conv1d(self.concat_dim, hidden_size[0], 1, padding=0)\n",
        "        self.activ = nn.LeakyReLU(0.1)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.conv1 = nn.Conv1d(hidden_size[0], hidden_size[1], 1, padding=0)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size[1], nhead=4, dim_feedforward=hidden_size[1], dropout=0.3)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "        self.head = nn.Sequential(\n",
        "                nn.Linear(hidden_size[1], hidden_size[2]),\n",
        "                nn.BatchNorm1d(hidden_size[2]),\n",
        "                nn.Dropout(p=0.3),\n",
        "                nn.Linear(hidden_size[2], 8))\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs,dim=1)\n",
        "        feat = torch.transpose(feat,0,1)\n",
        "        feat = self.feat_fc(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.conv1(feat)\n",
        "        out = torch.transpose(out,0,1)\n",
        "        out = self.transformer_encoder(out)\n",
        "        out = self.head(out)\n",
        "\n",
        "        return out, torch.softmax(out, dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p25D4iquOI0I",
        "outputId": "efe97635-2128-4014-ad1d-7d9593015a47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EXP_fusion(\n",
              "  (feat_fc): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n",
              "  (activ): LeakyReLU(negative_slope=0.1)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=32, out_features=8, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "EXP_model = EXP_fusion().to(device)\n",
        "EXP_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MLP"
      ],
      "metadata": {
        "id": "ZKCxyUITWjBw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bvTwWPbHdiqK"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, audio_ft = auft, num_classes=8):\n",
        "        super(MLPModel, self).__init__()\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.activ = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(self.concat_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs, dim=1)\n",
        "        feat = self.fc1(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.fc2(feat)\n",
        "        return out, torch.softmax(out, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtPaQhzHfHkD",
        "outputId": "9382ce1a-8fbf-41bb-eef0-17d0f304def6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPModel(\n",
              "  (activ): ReLU()\n",
              "  (fc1): Linear(in_features=2176, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYow-aj8W6E8"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6Z-Yc8Zz4A7_"
      },
      "outputs": [],
      "source": [
        "weights = [0.4260, 4.1334, 5.9334, 6.8653, 0.7257, 0.9578, 2.0518, 0.4572]\n",
        "weights = torch.tensor(weights).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YiUYQl4UvwV6"
      },
      "outputs": [],
      "source": [
        "def one_hot_transfer(label, class_num):\n",
        "    one_hot = torch.eye(class_num)\n",
        "    one_hot = one_hot.to(device)\n",
        "    return one_hot[label]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjx794E5bdTe"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, au_feat, weight):\n",
        "    model.eval()\n",
        "    total_loss = []\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        iterator = iter(data_loader)\n",
        "        for EXPR in iterator:\n",
        "            if au_feat == 'nope':\n",
        "                vis_feat, y = EXPR[visual_feat], EXPR['label']\n",
        "                vis_feat, y = vis_feat.to(device), y.to(device)\n",
        "                aud_feat = None\n",
        "            else:\n",
        "                vis_feat, aud_feat, y = EXPR[visual_feat], EXPR[au_feat], EXPR['label']\n",
        "                vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "            y_onehot = one_hot_transfer(y, 8).to(device)\n",
        "            pred, exp_pred = model(vis_feat, aud_feat)\n",
        "            loss = compute_EXP_loss(pred, y_onehot, weight)\n",
        "            total_loss.append(loss.item())\n",
        "            all_preds.extend(exp_pred.cpu().tolist())\n",
        "            all_targets.extend(y_onehot.cpu().tolist())\n",
        "\n",
        "    f1_scores, acc = compute_EXP_F1(all_preds, all_targets)\n",
        "    return round(np.mean(total_loss),3), round(f1_scores,3), round(acc,3)"
      ],
      "metadata": {
        "id": "7SJugj3fZCQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUFrGk1S__N4"
      },
      "source": [
        "##### EffNet + Wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHHY7_sZ2Mwd",
        "outputId": "ef6724e9-e65f-4d00-c715-3056723babd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EXP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: f1_score 0.33, accuracy: 0.457\n",
            "34.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('EXP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_fusion_{viau}.pth'))\n",
        "EXP_model = EXP_fusion().to(device)\n",
        "EXP_model.load_state_dict(EXP_best_model)\n",
        "test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH42fbzzP4q-",
        "outputId": "d4c4a8d3-74d4-44d0-8d84-0eccfaacccd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: f1_score 0.394, accuracy: 0.488\n",
            "12.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be0A7dxPAGJx"
      },
      "source": [
        "##### EffNet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbSQ4IBSI_d6",
        "outputId": "0b669b23-713f-462b-a5b3-b96c6b441f31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EXP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: f1_score 0.316, accuracy: 0.475\n",
            "33 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('EXP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_fusion_{viau}.pth'))\n",
        "EXP_model = EXP_fusion().to(device)\n",
        "EXP_model.load_state_dict(EXP_best_model)\n",
        "test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OM1_LawOeZg",
        "outputId": "8c7f2f78-7282-4b65-81dc-17e99598c5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: f1_score 0.379, accuracy: 0.498\n",
            "11.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibOhaO0vAOvH"
      },
      "source": [
        "##### EffNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kcSYFJIZ42k",
        "outputId": "5ca3235e-4d5c-4829-abd6-fc89874bcec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EXP_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: f1_score 0.31, accuracy: 0.447\n",
            "29.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('EXP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "EXP_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_fusion_{viau}.pth'))\n",
        "EXP_model = EXP_fusion().to(device)\n",
        "EXP_model.load_state_dict(EXP_best_model)\n",
        "test_loss, f1s, acc = evaluate_model(EXP_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwduIUw-_EUU",
        "outputId": "c09762d9-1623-48eb-cd9c-cdf29739416e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: f1_score 0.327, accuracy: 0.431\n",
            "8.93 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model, strict=False)\n",
        "val_loss, f1s, acc = evaluate_model(mlp_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score {round(f1s,3)}, accuracy: {round(acc,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd8sIrCrwZIT"
      },
      "source": [
        "### Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gGEXpsL6IsME"
      },
      "outputs": [],
      "source": [
        "tsk = task[0]\n",
        "vis = vis_typ[0]\n",
        "viau = vis_aud[0]\n",
        "auft = audio_feat[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XOXjdlJRYS76"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, audio_ft = auft, num_classes=8):\n",
        "        super(MLPModel, self).__init__()\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.activ = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(self.concat_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        inputs = [vis_feat]\n",
        "        inputs.append(aud_feat)\n",
        "        feat = torch.cat(inputs, dim=0)\n",
        "        feat = self.fc1(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.fc2(feat)\n",
        "        return out, torch.softmax(out, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz30ISAQwj-v",
        "outputId": "5b3d3179-2740-431d-b91f-880b1375376b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_EXPR_model/best_EXPR_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "34D1o-JyEPeb"
      },
      "outputs": [],
      "source": [
        "anno_path = os.path.join(root,f'data/Annotations/{tsk}/{split[1]}')\n",
        "with open(os.path.join(root, f'data/Annotations/{tsk}/{typ[2]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{tsk}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnxZ2o1TD9mB",
        "outputId": "509f09be-1d1b-43af-ad91-73c316880837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [01:22<00:00,  1.18s/it]\n"
          ]
        }
      ],
      "source": [
        "test_vid = {}\n",
        "for vname in tqdm(vidnames):\n",
        "        img, predict, label = [], [], []\n",
        "        for imgname, val in sorted(data[tsk][vname].items()):\n",
        "            vis_feat = torch.tensor(val[visual_feat]).to(device)\n",
        "            if auft == 'nope':\n",
        "                aud_feat = None\n",
        "            else:\n",
        "                aud_feat = torch.tensor(val[auft]).to(device)\n",
        "            if tsk == task[2]:\n",
        "                _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n",
        "                preds = torch.tensor([vpred, apred])\n",
        "            else:\n",
        "                _, pred = mlp_model(vis_feat, aud_feat)\n",
        "                preds = torch.tensor(pred)\n",
        "            ind = int(imgname.split('/')[1][:-4])\n",
        "            img.append(ind)\n",
        "            predict.append(preds)\n",
        "            label.append(data[tsk][vname][imgname]['label'])\n",
        "        test_vid[vname] = (img, predict, label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams=[(isMean,delta) for delta in [0, 5, 15, 30, 60, 100, 200] for isMean in [1,0] if not (isMean==0 and delta==0)]\n",
        "total_true=[]\n",
        "total_preds=[[] for _ in range(len(hyperparams))]\n",
        "timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n",
        "for videoname,(img, predict, label) in test_vid.items():\n",
        "    for i,ind in enumerate(img):\n",
        "        total_true.append(label[i].cpu().numpy())\n",
        "    preds_proba = smooth_prediction(img, predict)\n",
        "    for hInd,(isMean,delta) in enumerate(hyperparams):\n",
        "        preds=[]\n",
        "        start = time.time()\n",
        "        for i in range(len(preds_proba)):\n",
        "            i1=max(i-delta,0)\n",
        "            if isMean:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'mean')\n",
        "            else:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'median')\n",
        "            preds.append(best_ind)\n",
        "        for i,ind in enumerate(img):\n",
        "            if label[i]>=0:\n",
        "                total_preds[hInd].append(preds[ind-1])\n",
        "        end = time.time()\n",
        "        timing_results[(isMean, delta)] += end - start\n",
        "total_true=np.array(total_true)"
      ],
      "metadata": {
        "id": "AWQOiHvVogjP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for hInd, (isMean, delta) in enumerate(hyperparams):\n",
        "    preds = np.array(total_preds[hInd])\n",
        "    accuracy = round((preds == total_true).mean(), 3)\n",
        "    f1 = round(f1_score(y_true=total_true, y_pred=preds, average='macro'), 3)\n",
        "    mean_or_median = 'mean' if isMean else 'median'\n",
        "    time_taken = round(timing_results[(isMean, delta)],3)\n",
        "    print(f'{mean_or_median}; delta: {delta}; Acc: {accuracy}; F1: {f1}; Time: {time_taken}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ5rX14EVeuR",
        "outputId": "ad602aff-a5c9-444c-842d-5cbdad72f8a8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean; delta: 0; Acc: 0.488; F1: 0.394; Time: 7.318s\n",
            "mean; delta: 5; Acc: 0.512; F1: 0.418; Time: 7.51s\n",
            "median; delta: 5; Acc: 0.509; F1: 0.417; Time: 16.631s\n",
            "mean; delta: 15; Acc: 0.524; F1: 0.429; Time: 7.611s\n",
            "median; delta: 15; Acc: 0.523; F1: 0.43; Time: 17.372s\n",
            "mean; delta: 30; Acc: 0.533; F1: 0.44; Time: 8.184s\n",
            "median; delta: 30; Acc: 0.533; F1: 0.442; Time: 18.457s\n",
            "mean; delta: 60; Acc: 0.537; F1: 0.443; Time: 7.855s\n",
            "median; delta: 60; Acc: 0.538; F1: 0.449; Time: 20.305s\n",
            "mean; delta: 100; Acc: 0.539; F1: 0.448; Time: 8.482s\n",
            "median; delta: 100; Acc: 0.541; F1: 0.454; Time: 22.453s\n",
            "mean; delta: 200; Acc: 0.538; F1: 0.437; Time: 10.033s\n",
            "median; delta: 200; Acc: 0.544; F1: 0.457; Time: 27.948s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for hInd, (isMean, delta) in enumerate(hyperparams):\n",
        "    preds = np.array(total_preds[hInd])\n",
        "    accuracy = round((preds == total_true).mean(), 3)\n",
        "    f1 = round(f1_score(y_true=total_true, y_pred=preds, average='macro'), 3)\n",
        "    mean_or_median = 'mean' if isMean else 'median'\n",
        "    time_taken = round(timing_results[(isMean, delta)],3)\n",
        "    print(f'{mean_or_median}; delta: {delta}; Acc: {accuracy}; F1: {f1}; Time: {time_taken}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AqaXWC7-tL3",
        "outputId": "1d11cfa1-0b6c-4a7a-d3a5-1c2a99dccb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean; delta: 0; Acc: 0.488; F1: 0.394; Time: 9.554s\n",
            "mean; delta: 15; Acc: 0.524; F1: 0.429; Time: 9.97s\n",
            "median; delta: 15; Acc: 0.523; F1: 0.43; Time: 20.216s\n",
            "mean; delta: 30; Acc: 0.533; F1: 0.44; Time: 9.814s\n",
            "median; delta: 30; Acc: 0.533; F1: 0.442; Time: 20.944s\n",
            "mean; delta: 60; Acc: 0.537; F1: 0.443; Time: 9.286s\n",
            "median; delta: 60; Acc: 0.538; F1: 0.449; Time: 22.854s\n",
            "mean; delta: 100; Acc: 0.539; F1: 0.448; Time: 9.671s\n",
            "median; delta: 100; Acc: 0.541; F1: 0.454; Time: 25.641s\n",
            "mean; delta: 200; Acc: 0.538; F1: 0.437; Time: 11.106s\n",
            "median; delta: 200; Acc: 0.544; F1: 0.457; Time: 31.662s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsUw4AxuwtBo"
      },
      "source": [
        "### Adaptive Frame Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67stcFBHvCJh"
      },
      "outputs": [],
      "source": [
        "delta = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG-oVHWuHpFS",
        "outputId": "bcc7eb56-7f9c-4026-ce43-077f3fc99e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199/199 [02:23<00:00,  1.39it/s]\n"
          ]
        }
      ],
      "source": [
        "anno_path = os.path.join(root,f'data/Annotations/{tsk}/{split[0]}')\n",
        "with open(os.path.join(root, f'data/Annotations/{tsk}/{typ[0]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "with open(os.path.join(root,f'models/ABAW6/{vis}/EXPR/{tsk}_{typ[0]}_{viau}.pkl'), 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "train_vid = {}\n",
        "for vname in tqdm(vidnames):\n",
        "        img, predict, label = [], [], []\n",
        "        for imgname, val in sorted(data[tsk][vname].items()):\n",
        "            vis_feat = torch.tensor(val[visual_feat]).to(device)\n",
        "            if auft == 'nope':\n",
        "                aud_feat = None\n",
        "            else:\n",
        "                aud_feat = torch.tensor(val[auft]).to(device)\n",
        "            if tsk == task[2]:\n",
        "                _, _, vpred, apred = mlp_model(vis_feat, aud_feat)\n",
        "                preds = torch.tensor([vpred, apred])\n",
        "            else:\n",
        "                _, pred = mlp_model(vis_feat, aud_feat)\n",
        "                preds = torch.tensor(pred)\n",
        "            ind = int(imgname.split('/')[1][:-4])\n",
        "            img.append(ind)\n",
        "            predict.append(preds)\n",
        "            label.append(data[tsk][vname][imgname]['label'])\n",
        "        train_vid[vname] = (img, predict, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ayc_zt0GHf-V"
      },
      "outputs": [],
      "source": [
        "stride2scores={}\n",
        "for stride in [200, 100, 50, 25, 10]:\n",
        "    total_true, predictions, max_decision_values = [],[],[]\n",
        "    for vidname, (img, predict, label) in train_vid.items():\n",
        "        index = []\n",
        "        for i,ind in enumerate(img):\n",
        "            total_true.append(label[i].cpu().numpy())\n",
        "            index.append(ind-1)\n",
        "        preds_proba = smooth_prediction(img, predict)\n",
        "        for i in range(len(index)):\n",
        "            best_ind, proba = slide_window(preds_proba, index[i], delta, stride)\n",
        "            predictions.append(best_ind)\n",
        "            max_decision_values.append(proba[best_ind])\n",
        "    stride2scores[stride] = (np.array(total_true),np.array(predictions),np.array(max_decision_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvZ1nt6GGzaM"
      },
      "outputs": [],
      "source": [
        "def get_threshold(stride,fpr_corrected):\n",
        "    (total_true,predictions,max_decision_values) = stride2scores[stride]\n",
        "    mistakes = max_decision_values[predictions != total_true]\n",
        "    best_threshold = -1\n",
        "    for i, threshold in enumerate(sorted(max_decision_values[predictions == total_true])[::-1]):\n",
        "        tpr = i/len(predictions)\n",
        "        fpr = (mistakes > threshold).sum()/len(predictions)\n",
        "        if fpr > fpr_corrected:\n",
        "            if best_threshold == -1:\n",
        "                best_threshold = threshold\n",
        "            print(stride, 'best_threshold', best_threshold, i)\n",
        "            break\n",
        "        best_threshold = threshold\n",
        "    return best_threshold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stride2threshold = {}\n",
        "for stride in stride2scores:\n",
        "    fpr_corrected=0.05\n",
        "    stride2threshold[stride] = get_threshold(stride,fpr_corrected)\n",
        "stride2threshold[1] = 0\n",
        "print(stride2threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb_5USiNj1gY",
        "outputId": "24ec0a76-58c9-4078-c431-d42fdd072fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 best_threshold 0.5776766 230724\n",
            "100 best_threshold 0.5594592 241835\n",
            "50 best_threshold 0.54468054 251611\n",
            "25 best_threshold 0.538801 255266\n",
            "10 best_threshold 0.53340304 259316\n",
            "{200: 0.5776766, 100: 0.5594592, 50: 0.54468054, 25: 0.538801, 10: 0.53340304, 1: 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_strides=[\n",
        "    [200, 100, 50, 10, 1],\n",
        "    [50, 25, 1],\n",
        "    [50, 10, 1],\n",
        "    [200,50,1],\n",
        "    [100,50,1],\n",
        "    [200,1],\n",
        "    [100,1],\n",
        "    [50,1]\n",
        "]\n",
        "for s in stride2threshold.keys():\n",
        "    all_strides.append([s])\n",
        "\n",
        "for strides in all_strides:\n",
        "    print(strides)\n",
        "    last_stride=strides[-1]\n",
        "\n",
        "    total_true=[]\n",
        "    total_preds=[]\n",
        "    total_frames_processed,total_frames=0,0\n",
        "    time_each = []\n",
        "    start = time.time()\n",
        "    for videoname, (img, predict, label) in test_vid.items():\n",
        "        emotional_img=[]\n",
        "        start1 = time.time()\n",
        "        for i,ind in enumerate(img):\n",
        "            total_true.append(label[i].cpu().numpy())\n",
        "            emotional_img.append(ind-1)\n",
        "        cur_ind=0\n",
        "        preds_proba=[]\n",
        "        for i in range(img[-1]):\n",
        "            if img[cur_ind]-1==i:\n",
        "                preds_proba.append(predict[cur_ind])\n",
        "                cur_ind+=1\n",
        "            else:\n",
        "                if cur_ind==0:\n",
        "                    preds_proba.append(predict[cur_ind])\n",
        "                else:\n",
        "                    w=(i-img[cur_ind-1]+1)/(img[cur_ind]-img[cur_ind-1])\n",
        "                    pred=w*predict[cur_ind-1]+(1-w)*predict[cur_ind]\n",
        "                    preds_proba.append(pred)\n",
        "\n",
        "        preds_proba=np.array([p.cpu().numpy() for p in preds_proba])\n",
        "\n",
        "        preds=-np.ones(len(emotional_img))\n",
        "        end1 = time.time()\n",
        "        time_each.append(end1 - start1)\n",
        "        for stride in strides:\n",
        "            threshold=stride2threshold[stride]\n",
        "            for i in range(len(emotional_img)):\n",
        "                if preds[i]<0:\n",
        "                    i1=max(emotional_img[i]-delta,0)\n",
        "                    cur_preds=preds_proba[i1:emotional_img[i]+delta+1:stride]\n",
        "                    proba=np.median(cur_preds,axis=0)\n",
        "                    best_ind=np.argmax(proba)\n",
        "                    if proba[best_ind]>=threshold or stride==last_stride:\n",
        "                        total_frames_processed+=len(cur_preds)\n",
        "                        total_frames+=len(preds_proba[i1:emotional_img[i]+delta+1])\n",
        "                        preds[i]=best_ind\n",
        "        for p in preds:\n",
        "            total_preds.append(p)\n",
        "    end = time.time()\n",
        "    elapsed_time = end - start - sum(time_each)\n",
        "    total_true=np.array(total_true)\n",
        "    preds=np.array(total_preds)\n",
        "    print('Acc:',round((preds==total_true).mean(),3), 'F1:',round(f1_score(y_true=total_true,y_pred=preds, average=\"macro\"),3))\n",
        "    print(total_frames_processed,total_frames,round(total_frames_processed/total_frames,3))\n",
        "    print(f\"Time: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fq28CZopD88",
        "outputId": "b4b16d5d-3b7d-4b4d-9bd3-4467e06b8ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200, 100, 50, 10, 1]\n",
            "Acc: 0.543 F1: 0.454\n",
            "66718832 109811197 0.608\n",
            "Time: 50.22 seconds\n",
            "[50, 25, 1]\n",
            "Acc: 0.544 F1: 0.457\n",
            "75888291 109811197 0.691\n",
            "Time: 35.62 seconds\n",
            "[50, 10, 1]\n",
            "Acc: 0.544 F1: 0.457\n",
            "74809429 109811197 0.681\n",
            "Time: 35.97 seconds\n",
            "[200, 50, 1]\n",
            "Acc: 0.543 F1: 0.455\n",
            "72017266 109811197 0.656\n",
            "Time: 34.61 seconds\n",
            "[100, 50, 1]\n",
            "Acc: 0.544 F1: 0.456\n",
            "75050361 109811197 0.683\n",
            "Time: 35.59 seconds\n",
            "[200, 1]\n",
            "Acc: 0.543 F1: 0.455\n",
            "81565887 109811197 0.743\n",
            "Time: 29.01 seconds\n",
            "[100, 1]\n",
            "Acc: 0.544 F1: 0.456\n",
            "80951438 109811197 0.737\n",
            "Time: 28.60 seconds\n",
            "[50, 1]\n",
            "Acc: 0.544 F1: 0.457\n",
            "79911794 109811197 0.728\n",
            "Time: 28.14 seconds\n",
            "[200]\n",
            "Acc: 0.498 F1: 0.411\n",
            "816872 109811197 0.007\n",
            "Time: 13.12 seconds\n",
            "[100]\n",
            "Acc: 0.514 F1: 0.429\n",
            "1364594 109811197 0.012\n",
            "Time: 13.06 seconds\n",
            "[50]\n",
            "Acc: 0.527 F1: 0.443\n",
            "2459930 109811197 0.022\n",
            "Time: 13.21 seconds\n",
            "[25]\n",
            "Acc: 0.536 F1: 0.45\n",
            "4650733 109811197 0.042\n",
            "Time: 13.65 seconds\n",
            "[10]\n",
            "Acc: 0.541 F1: 0.455\n",
            "11223222 109811197 0.102\n",
            "Time: 13.91 seconds\n",
            "[1]\n",
            "Acc: 0.544 F1: 0.457\n",
            "109811197 109811197 1.0\n",
            "Time: 21.66 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zISVpqDy0Io"
      },
      "source": [
        "## AU Detection Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t9KlVut-8gN"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lIzVtIHXjAa"
      },
      "outputs": [],
      "source": [
        "# Cropped_aligned images\n",
        "vis = vis_typ[0]\n",
        "vis1 = vis_typ[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-e2tiWc-8gO"
      },
      "source": [
        "#### Effnet + wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z8HNxRg-8gO"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[0]\n",
        "viau = vis_aud[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEbORJYi-8gO"
      },
      "source": [
        "#### Effnet + vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmDO2-je-8gO"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[1]\n",
        "viau = vis_aud[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWvFn9TS-8gP"
      },
      "source": [
        "#### Effnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZmMqmOj-8gP"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[2]\n",
        "viau = vis_aud[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsoLTqkz-8gP"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B2"
      ],
      "metadata": {
        "id": "8vFUXiAyjdSs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuLsvz6J-8gQ"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis}/AU/{task[1]}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n",
        "    data3 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGQmZCVl-8gQ",
        "outputId": "593d5733-59ce-4dde-fa9f-a3e886198af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 105/105 [00:00<00:00, 1203.40it/s]\n"
          ]
        }
      ],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[1]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[2]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data3[task1][vname])\n",
        "    for img in data3[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x8-0eME-8gQ"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data3, iname, dims, task1)\n",
        "test_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B0"
      ],
      "metadata": {
        "id": "47UnYX3TjhOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis1}/AU/{task[1]}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n",
        "    data3 = pickle.load(f)"
      ],
      "metadata": {
        "id": "xaPuN8q4oVOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5MqbxDkH4bi"
      },
      "outputs": [],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[1]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[1]}/{typ[2]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data3[task1][vname])\n",
        "    for img in data3[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEXe6RgdH4bj"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data3, iname, dims, task1)\n",
        "test_loader_b0 = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKO9rSzdO9t5"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOHiQBFK4DEG"
      },
      "outputs": [],
      "source": [
        "class AU_fusion(nn.Module):\n",
        "    def __init__(self, batchsize = batch_size, audio_ft = auft, hidden_size = [512, 128, batch_size]):\n",
        "        super(AU_fusion, self).__init__()\n",
        "        self.batchsize = batchsize\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.hidden_size = hidden_size\n",
        "        self.feat_fc = nn.Conv1d(self.concat_dim, hidden_size[0], 1, padding=0)\n",
        "        self.activ = nn.LeakyReLU(0.1)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.conv1 = nn.Conv1d(hidden_size[0], hidden_size[1], 1, padding=0)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size[1], nhead=4, dim_feedforward=hidden_size[1], dropout=0.3)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "        self.head = nn.Sequential(\n",
        "                nn.Linear(hidden_size[1], hidden_size[2]),\n",
        "                nn.BatchNorm1d(hidden_size[2]),\n",
        "                nn.Linear(hidden_size[2], 12))\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs,dim=1)\n",
        "        feat = torch.transpose(feat,0,1)\n",
        "        feat = self.feat_fc(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.conv1(feat)\n",
        "        out = torch.transpose(out,0,1)\n",
        "        out = self.transformer_encoder(out)\n",
        "        out = self.head(out)\n",
        "\n",
        "        return out, torch.sigmoid(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7xqRYlKLJn0",
        "outputId": "5dd0908a-4211-4be6-fe80-c8f251a2aff8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AU_fusion(\n",
              "  (feat_fc): Conv1d(1408, 512, kernel_size=(1,), stride=(1,))\n",
              "  (activ): LeakyReLU(negative_slope=0.1)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Linear(in_features=32, out_features=12, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "AU_model = AU_fusion().to(device)\n",
        "AU_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djfaj8fAZVZw"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, audio_ft = auft, num_classes=12):\n",
        "        super(MLPModel, self).__init__()\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.activ = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(self.concat_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs, dim=1)\n",
        "        feat = self.fc1(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.fc2(feat)\n",
        "        return out, torch.sigmoid(out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel_b0(nn.Module):\n",
        "    def __init__(self, audio_ft = auft, num_classes=12):\n",
        "        super(MLPModel_b0, self).__init__()\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2048 #1280+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1408 #1280+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1280 #1280    #visual only\n",
        "        self.activ = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(self.concat_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs, dim=1)\n",
        "        feat = self.fc1(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.fc2(feat)\n",
        "        return out, torch.sigmoid(out)"
      ],
      "metadata": {
        "id": "Et_Mmy63lZHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqfnlKCdKmBf",
        "outputId": "1da8cec3-f8af-4420-80b1-4c31f11f47e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPModel(\n",
              "  (activ): ReLU()\n",
              "  (fc1): Linear(in_features=1536, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8j8cnwsoHuX"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq_YlOzCA-ST"
      },
      "outputs": [],
      "source": [
        "weights = np.array([[ 0.57226017, 3.95972043],\n",
        " [ 0.52682923,  9.81819492],\n",
        " [ 0.59477327,  3.13787445],\n",
        " [ 0.67850442,  1.90052551],\n",
        " [ 0.82920966,  1.25939448],\n",
        " [ 0.75993937,  1.46176273],\n",
        " [ 0.65729582,  2.08936204],\n",
        " [ 0.51501993, 17.14454686],\n",
        " [ 0.5158745,  16.24852424],\n",
        " [ 0.51460124, 17.62183135],\n",
        " [ 1.33900046,  0.79797362],\n",
        " [ 0.54113187,  6.57801166]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights1 = torch.tensor([0.54733899, 0.44180561, 0.56990565, 0.61997328, 0.73956417,0.74692377, 0.72684634, 0.33222808, 0.17383676, 0.20608964, 0.83688068, 0.33890931]).to(device)"
      ],
      "metadata": {
        "id": "AzYKoN31qzE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iIGwebQcq1G"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUHE6yoY-bTy"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, vi_feat, au_feat, weight):\n",
        "    model.eval()\n",
        "    total_loss = []\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        iterator = iter(data_loader)\n",
        "        for AU in iterator:\n",
        "            if au_feat == 'nope':\n",
        "                vis_feat, y = AU[vi_feat], AU['label']\n",
        "                vis_feat, y = vis_feat.to(device), y.to(device)\n",
        "                aud_feat = None\n",
        "            else:\n",
        "                vis_feat, aud_feat, y = AU[vi_feat], AU[au_feat], AU['label']\n",
        "                vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "            pred, au_pred = model(vis_feat, aud_feat)\n",
        "            loss = compute_AU_loss(pred, y, weight)\n",
        "            total_loss.append(loss.item())\n",
        "            all_preds.extend(au_pred.cpu().tolist())\n",
        "            all_targets.extend(y.cpu().tolist())\n",
        "\n",
        "    f1_scores, f1_thresh, acc, threshold = compute_AU_F1(all_preds, all_targets)\n",
        "    return round(np.mean(total_loss),3), round(f1_scores,3), round(f1_thresh,3), acc, threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK0WMyfYn15t"
      },
      "source": [
        "##### EffNet + Wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dp-enALkGuL",
        "outputId": "4edda0f3-44ea-4444-ffa5-ff705b66fa46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AU_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: f1_score: 0.497, accuracy: [0.86, 0.889, 0.857, 0.79, 0.764, 0.8, 0.858, 0.901, 0.938, 0.932, 0.734, 0.803], f1_threshold: [0.2, 0.1, 0.30000000000000004, 0.30000000000000004, 0.5, 0.4, 0.4, 0.1, 0.1, 0.1, 0.4, 0.1]\n",
            "1min 29s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('AU_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_fusion_{viau}_loss.pth'))\n",
        "AU_model = AU_fusion().to(device)\n",
        "AU_model.load_state_dict(AU_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(AU_model, test_loader, auft, weights1)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(mlp_model, test_loader, visual_feat, auft, weights)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4MvRbcmUhvr",
        "outputId": "0e6d8e5c-c59a-40df-fb4f-3b51bb1aaa1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: f1_score: 0.513, accuracy: [0.875, 0.913, 0.858, 0.813, 0.757, 0.807, 0.863, 0.879, 0.954, 0.926, 0.746, 0.849], f1_threshold: [0.7000000000000001, 0.8, 0.7000000000000001, 0.6, 0.5, 0.6, 0.7000000000000001, 0.6, 0.8, 0.8, 0.30000000000000004, 0.7000000000000001]\n",
            "50.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDIL3d6Fn15u"
      },
      "source": [
        "##### EffNet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgzJR2ryhLo7",
        "outputId": "a1076627-2e25-4dc3-f40a-905a057693c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AU_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: f1_score: 0.494, accuracy: [0.886, 0.914, 0.86, 0.79, 0.772, 0.805, 0.867, 0.907, 0.925, 0.935, 0.76, 0.88], f1_threshold: [0.30000000000000004, 0.2, 0.30000000000000004, 0.4, 0.5, 0.4, 0.5, 0.1, 0.1, 0.2, 0.5, 0.2]\n",
            "1min 26s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('AU_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_fusion_{viau}.pth'))\n",
        "AU_model = AU_fusion().to(device)\n",
        "AU_model.load_state_dict(AU_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(AU_model, test_loader, auft, weights1)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(mlp_model, test_loader, visual_feat, auft, weights)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E8BZdU8L5cM",
        "outputId": "0ba989d8-6d1e-4e98-992a-b05bbd064f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: f1_score: 0.52, accuracy: [0.875, 0.913, 0.849, 0.809, 0.773, 0.8, 0.865, 0.897, 0.932, 0.922, 0.759, 0.849], f1_threshold: [0.7000000000000001, 0.8, 0.6, 0.6, 0.5, 0.5, 0.7000000000000001, 0.7000000000000001, 0.7000000000000001, 0.8, 0.30000000000000004, 0.7000000000000001]\n",
            "48.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrE_pL5Gn15v"
      },
      "source": [
        "##### EffNet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('AU_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "AU_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_fusion_{viau}.pth'))\n",
        "AU_model = AU_fusion().to(device)\n",
        "AU_model.load_state_dict(AU_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(AU_model, test_loader, auft, weights)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udk-286Jvd6u",
        "outputId": "d8ba9b0f-c43b-4d3f-8143-d9009ef04fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AU_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: f1_score: 0.509, accuracy: [0.868, 0.911, 0.854, 0.794, 0.765, 0.793, 0.853, 0.905, 0.943, 0.951, 0.742, 0.838], f1_threshold: [0.7000000000000001, 0.8, 0.5, 0.5, 0.5, 0.5, 0.6, 0.5, 0.7000000000000001, 0.8, 0.30000000000000004, 0.6]\n",
            "1min 28s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_mlp_{viau}_f1s.pth'))\n",
        "mlp_model = MLPModel(num_classes = 12).to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, f1s, f1t, acc, threshold = evaluate_model(mlp_model, test_loader, visual_feat, auft, weights)\n",
        "print(f'Test set: f1_score: {f1s}, accuracy: {acc}, f1_threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qfv82bH2TicB",
        "outputId": "e9542b29-9fd1-49e6-ff1d-ba5ec8942836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: f1_score: 0.521, accuracy: [0.878, 0.916, 0.864, 0.791, 0.772, 0.795, 0.86, 0.865, 0.933, 0.919, 0.757, 0.846], f1_threshold: [0.7000000000000001, 0.8, 0.7000000000000001, 0.5, 0.5, 0.5, 0.6, 0.6, 0.7000000000000001, 0.8, 0.30000000000000004, 0.7000000000000001]\n",
            "52.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensembling"
      ],
      "metadata": {
        "id": "5weqdQ5IlEH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EffNet_B2 + Wav2Vec2"
      ],
      "metadata": {
        "id": "UuF_NQH6og5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_mlp_{viau}.pth'))\n",
        "mlp_model_wav = MLPModel().to(device)\n",
        "mlp_model_wav.load_state_dict(mlp_best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtZL7YHmqP8V",
        "outputId": "8ffdca6e-7701-467d-947a-91e0884dd90f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(test_loader)\n",
        "all_preds_wav, all_targets_wav = [], []\n",
        "img = []\n",
        "for AU in iterator:\n",
        "    frame = AU['frame']\n",
        "    for imgname in frame:\n",
        "        img.append(imgname)\n",
        "    vis_feat, aud_feat, y = AU[visual_feat], AU[auft], AU['label']\n",
        "    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "    pred, au_pred = mlp_model_wav(vis_feat, aud_feat)\n",
        "    all_preds_wav.extend(au_pred.cpu().tolist())\n",
        "    all_targets_wav.extend(y.cpu().tolist())"
      ],
      "metadata": {
        "id": "Ebs1gDL4og5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EffNet_B2 + Vggish"
      ],
      "metadata": {
        "id": "bLuhTZ7yl-4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_AU_model/best_AU_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiUAUlU-mHO5",
        "outputId": "e1c2c990-a871-4ff1-cf16-b499972fa26c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(test_loader)\n",
        "all_preds, all_targets = [], []\n",
        "img1 = []\n",
        "for AU in iterator:\n",
        "    frame = AU['frame']\n",
        "    for imgname in frame:\n",
        "        img1.append(imgname)\n",
        "    vis_feat, aud_feat, y = AU[visual_feat], AU[auft], AU['label']\n",
        "    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "    pred, au_pred = mlp_model(vis_feat, aud_feat)\n",
        "    all_preds.extend(au_pred.cpu().tolist())\n",
        "    all_targets.extend(y.cpu().tolist())"
      ],
      "metadata": {
        "id": "YI4-pIuyl4oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EffNet_B0 + Vggish"
      ],
      "metadata": {
        "id": "l1GwAugmmQD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_best_model_b0 = torch.load(os.path.join(root,f'models/ABAW6/{vis1}/best_AU_mlp_{viau}_acc.pth'))\n",
        "mlp_model_b0 = MLPModel_b0(num_classes = 12).to(device)\n",
        "mlp_model_b0.load_state_dict(mlp_best_model_b0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxKIp-N0lHwq",
        "outputId": "fc4ef31f-6ac3-4037-bfaa-f89c510ed444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(test_loader_b0)\n",
        "all_preds_b0, all_targets_b0 = [], []\n",
        "img = []\n",
        "for AU in iterator:\n",
        "    frame = AU['frame']\n",
        "    for imgname in frame:\n",
        "        img.append(imgname)\n",
        "    vis_feat, aud_feat, y = AU[visual_feat_1], AU[auft], AU['label']\n",
        "    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "    pred, au_pred = mlp_model_b0(vis_feat, aud_feat)\n",
        "    all_preds_b0.extend(au_pred.cpu().tolist())\n",
        "    all_targets_b0.extend(y.cpu().tolist())"
      ],
      "metadata": {
        "id": "n9oVhLfrmMqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ensemble"
      ],
      "metadata": {
        "id": "FtkuPPPAmXkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B2 + Wav2Vec2 + Vggish"
      ],
      "metadata": {
        "id": "FMJ6g7bgvyCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zipped_data = zip(img1, all_preds, all_targets)\n",
        "zipped_data_wav = zip(img, all_preds_wav, all_targets_wav)\n",
        "\n",
        "sorted_data = sorted(zipped_data)\n",
        "sorted_data_wav = sorted(zipped_data_wav)\n",
        "\n",
        "re_img_wav, re_all_preds_wav, re_all_targets_wav = zip(*sorted_data_wav)\n",
        "re_img, re_all_preds, re_all_targets = zip(*sorted_data)"
      ],
      "metadata": {
        "id": "FPx5OgzSuCsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1s, _, _, thresholds = compute_AU_F1(re_all_preds, re_all_targets_wav)\n",
        "round(f1s,3), thresholds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E_EYDKQuVPx",
        "outputId": "a83875cf-a443-4cac-a7f2-d0265aaef863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.52,\n",
              " [0.7000000000000001,\n",
              "  0.8,\n",
              "  0.6,\n",
              "  0.6,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.7000000000000001,\n",
              "  0.7000000000000001,\n",
              "  0.7000000000000001,\n",
              "  0.8,\n",
              "  0.30000000000000004,\n",
              "  0.7000000000000001])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w in np.linspace(0, 1, 11):\n",
        "    print(f'{w = }')\n",
        "    w = np.array([w])\n",
        "    y_ensemble = w * re_all_preds + (1 - w) * re_all_preds_wav\n",
        "    new_pred = (y_ensemble >= thresholds).astype(int)\n",
        "    f1_scores, f1_thresh, acc, threshold = compute_AU_F1(re_all_targets, new_pred,thresh=np.arange(0.1,1,0.1))\n",
        "    print(f'f1 score: {f1_scores:.3f}, accuracy: {acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAqn6PYPuexu",
        "outputId": "9d79f0ba-a16c-4d0c-9cd8-be5dbef609c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0.0\n",
            "f1 score: 0.512, accuracy: [0.875, 0.913, 0.839, 0.813, 0.757, 0.787, 0.863, 0.913, 0.922, 0.926, 0.746, 0.849]\n",
            "w = 0.1\n",
            "f1 score: 0.515, accuracy: [0.877, 0.914, 0.843, 0.814, 0.759, 0.79, 0.865, 0.914, 0.926, 0.929, 0.747, 0.851]\n",
            "w = 0.2\n",
            "f1 score: 0.517, accuracy: [0.878, 0.915, 0.846, 0.815, 0.762, 0.792, 0.865, 0.915, 0.929, 0.931, 0.749, 0.853]\n",
            "w = 0.30000000000000004\n",
            "f1 score: 0.520, accuracy: [0.88, 0.916, 0.848, 0.816, 0.764, 0.794, 0.866, 0.915, 0.932, 0.931, 0.751, 0.854]\n",
            "w = 0.4\n",
            "f1 score: 0.521, accuracy: [0.88, 0.917, 0.85, 0.816, 0.766, 0.797, 0.866, 0.915, 0.934, 0.932, 0.752, 0.855]\n",
            "w = 0.5\n",
            "f1 score: 0.522, accuracy: [0.88, 0.917, 0.851, 0.816, 0.767, 0.798, 0.866, 0.914, 0.935, 0.931, 0.754, 0.855]\n",
            "w = 0.6000000000000001\n",
            "f1 score: 0.523, accuracy: [0.88, 0.917, 0.852, 0.815, 0.769, 0.799, 0.867, 0.912, 0.936, 0.93, 0.755, 0.855]\n",
            "w = 0.7000000000000001\n",
            "f1 score: 0.523, accuracy: [0.88, 0.916, 0.852, 0.814, 0.77, 0.8, 0.866, 0.909, 0.936, 0.929, 0.756, 0.854]\n",
            "w = 0.8\n",
            "f1 score: 0.522, accuracy: [0.878, 0.915, 0.852, 0.813, 0.772, 0.8, 0.866, 0.906, 0.936, 0.927, 0.758, 0.853]\n",
            "w = 0.9\n",
            "f1 score: 0.521, accuracy: [0.877, 0.914, 0.851, 0.811, 0.772, 0.801, 0.865, 0.901, 0.935, 0.924, 0.759, 0.851]\n",
            "w = 1.0\n",
            "f1 score: 0.520, accuracy: [0.875, 0.913, 0.849, 0.809, 0.773, 0.8, 0.865, 0.897, 0.932, 0.922, 0.759, 0.849]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight=np.array([0.6])\n",
        "y_ensemble = weight * re_all_preds + (1 - weight) * re_all_preds_wav\n",
        "new_pred = ((y_ensemble >= thresholds) * 1)\n",
        "f1_scores, f1_thresh, acc, threshold = compute_AU_F1(re_all_targets, new_pred,thresh=np.arange(0.1,1,0.1))\n",
        "print(f'f1 score: {f1_scores:.3f}, accuracy: {acc}, threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKwyE25lwfSC",
        "outputId": "c45933fb-1ab5-4118-fbd3-ba1cbbb2560d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 score: 0.523, accuracy: [0.88, 0.917, 0.852, 0.815, 0.769, 0.799, 0.867, 0.912, 0.936, 0.93, 0.755, 0.855], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B2 + EffNet_B0 + Vggish"
      ],
      "metadata": {
        "id": "658gH2BQvkKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zipped_data = zip(img1, all_preds, all_targets)\n",
        "zipped_data_b0 = zip(img, all_preds_b0, all_targets_b0)\n",
        "\n",
        "sorted_data = sorted(zipped_data)\n",
        "sorted_data_b0 = sorted(zipped_data_b0)\n",
        "\n",
        "re_img_b0, re_all_preds_b0, re_all_targets_b0 = zip(*sorted_data_b0)\n",
        "re_img, re_all_preds, re_all_targets = zip(*sorted_data)"
      ],
      "metadata": {
        "id": "bp1C3vjCmbcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1s, _, _, thresholds = compute_AU_F1(re_all_preds_b0, re_all_targets)\n",
        "round(f1s,3), thresholds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tt_kRQFoDvA",
        "outputId": "a4d82782-4073-44b4-9fb8-b57d0194dd80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.52,\n",
              " [0.8,\n",
              "  0.9,\n",
              "  0.6,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.6,\n",
              "  0.7000000000000001,\n",
              "  0.7000000000000001,\n",
              "  0.7000000000000001,\n",
              "  0.8,\n",
              "  0.30000000000000004,\n",
              "  0.7000000000000001])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w in np.linspace(0, 1, 11):\n",
        "    print(f'{w = }')\n",
        "    w = np.array([w])\n",
        "    y_ensemble = w * re_all_preds + (1 - w) * re_all_preds_b0\n",
        "    new_pred = (y_ensemble >= thresholds).astype(int)\n",
        "    f1_scores, f1_thresh, acc, threshold = compute_AU_F1(re_all_targets, new_pred,thresh=np.arange(0.1,1,0.1))\n",
        "    print(f'f1 score: {f1_scores:.3f}, accuracy: {acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6scMc5IVBGI",
        "outputId": "33202f9f-2712-492d-f121-5016faad2093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0.0\n",
            "f1 score: 0.520, accuracy: [0.877, 0.915, 0.853, 0.784, 0.78, 0.807, 0.868, 0.932, 0.948, 0.933, 0.777, 0.849], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.1\n",
            "f1 score: 0.524, accuracy: [0.883, 0.92, 0.857, 0.787, 0.783, 0.811, 0.87, 0.934, 0.951, 0.935, 0.778, 0.853], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.2\n",
            "f1 score: 0.528, accuracy: [0.888, 0.923, 0.86, 0.79, 0.786, 0.815, 0.873, 0.936, 0.954, 0.936, 0.779, 0.857], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.30000000000000004\n",
            "f1 score: 0.531, accuracy: [0.892, 0.925, 0.864, 0.794, 0.788, 0.818, 0.875, 0.938, 0.956, 0.937, 0.778, 0.86], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.4\n",
            "f1 score: 0.532, accuracy: [0.894, 0.926, 0.867, 0.797, 0.79, 0.821, 0.876, 0.938, 0.958, 0.937, 0.778, 0.863], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.5\n",
            "f1 score: 0.533, accuracy: [0.895, 0.926, 0.868, 0.8, 0.791, 0.823, 0.876, 0.938, 0.96, 0.937, 0.777, 0.864], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.6000000000000001\n",
            "f1 score: 0.532, accuracy: [0.896, 0.927, 0.867, 0.8, 0.79, 0.825, 0.876, 0.935, 0.96, 0.936, 0.775, 0.864], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.7000000000000001\n",
            "f1 score: 0.530, accuracy: [0.897, 0.927, 0.864, 0.798, 0.787, 0.824, 0.874, 0.93, 0.958, 0.935, 0.773, 0.863], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.8\n",
            "f1 score: 0.527, accuracy: [0.896, 0.927, 0.859, 0.795, 0.783, 0.822, 0.872, 0.92, 0.953, 0.933, 0.769, 0.859], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 0.9\n",
            "f1 score: 0.522, accuracy: [0.895, 0.928, 0.854, 0.791, 0.778, 0.819, 0.869, 0.909, 0.944, 0.928, 0.765, 0.855], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
            "w = 1.0\n",
            "f1 score: 0.517, accuracy: [0.892, 0.927, 0.849, 0.786, 0.773, 0.816, 0.865, 0.897, 0.932, 0.922, 0.759, 0.849], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight=np.array([0.5])\n",
        "y_ensemble = weight * re_all_preds + (1 - weight) * re_all_preds_b0\n",
        "new_pred = ((y_ensemble >= thresholds) * 1)\n",
        "f1_scores, f1_thresh, acc, threshold = compute_AU_F1(re_all_targets, new_pred,thresh=np.arange(0.1,1,0.1))\n",
        "print(f'f1 score: {f1_scores:.3f}, accuracy: {acc}, threshold: {threshold}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv4lZiBhpICn",
        "outputId": "66c9d45e-8ff8-4090-9a6f-ed1fec5a2870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 score: 0.533, accuracy: [0.895, 0.926, 0.868, 0.8, 0.791, 0.823, 0.876, 0.938, 0.96, 0.937, 0.777, 0.864], threshold: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_2hrWvEYRTB"
      },
      "source": [
        "### Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tsk = task[1]\n",
        "vis = vis_typ[0]\n",
        "viau = vis_aud[1]\n",
        "auft = audio_feat[1]"
      ],
      "metadata": {
        "id": "hYZpsgSCsm5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index, pred, lab = [], [], []\n",
        "test_vid = {}\n",
        "for i, val in enumerate(re_img):\n",
        "    ind = int(val.split('/')[1][:-4])\n",
        "    vname = val.split('/')[0]\n",
        "    if i == 0:\n",
        "        prename = vname\n",
        "        index.append(ind)\n",
        "        pred.append(new_pred[i])\n",
        "        lab.append(re_all_targets[i])\n",
        "    else:\n",
        "        if vname == prename:\n",
        "            index.append(ind)\n",
        "            pred.append(new_pred[i])\n",
        "            lab.append(re_all_targets[i])\n",
        "        else:\n",
        "            combined = list(zip(index, pred, lab))\n",
        "            combined_sorted = sorted(combined, key=lambda x: x[0])\n",
        "            index_list_sorted, pred_list_sorted, lab_list_sorted = zip(*combined_sorted)\n",
        "            test_vid[prename] = (list(index_list_sorted), list(pred_list_sorted), list(lab_list_sorted))\n",
        "            prename = vname\n",
        "            index, pred, lab = [], [], []\n",
        "            index.append(ind)\n",
        "            pred.append(new_pred[i])\n",
        "            lab.append(re_all_targets[i])"
      ],
      "metadata": {
        "id": "r7PVXOsVvRjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B2 + Wav2Vec2 + Vggish"
      ],
      "metadata": {
        "id": "s3wWM97DwCmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8ZjiiVbHzuK"
      },
      "outputs": [],
      "source": [
        "thresholds = np.array([0.7000000000000001,\n",
        "  0.8,\n",
        "  0.6,\n",
        "  0.6,\n",
        "  0.5,\n",
        "  0.5,\n",
        "  0.7000000000000001,\n",
        "  0.7000000000000001,\n",
        "  0.7000000000000001,\n",
        "  0.8,\n",
        "  0.30000000000000004,\n",
        "  0.7000000000000001])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj6YmjcFEbZA"
      },
      "outputs": [],
      "source": [
        "hyperparams=[(isMean,delta) for delta in [0, 5, 15, 30, 60, 100, 200] for isMean in [1,0] if not (isMean==0 and delta==0)]\n",
        "total_true=[]\n",
        "total_preds=[[] for _ in range(len(hyperparams))]\n",
        "timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n",
        "for videoname,(img, predict, label) in test_vid.items():\n",
        "    for i,ind in enumerate(img):\n",
        "        total_true.append(label[i])\n",
        "    preds_proba = smooth_prediction(img, predict)\n",
        "    for hInd,(isMean,delta) in enumerate(hyperparams):\n",
        "        preds=[]\n",
        "        start = time.time()\n",
        "        for i in range(len(preds_proba)):\n",
        "            i1=max(i-delta,0)\n",
        "            if isMean:\n",
        "                _, proba = slide_window(preds_proba, i, delta, 'mean')\n",
        "            else:\n",
        "                _, proba = slide_window(preds_proba, i, delta, 'median')\n",
        "            aus = (proba>=thresholds)*1\n",
        "            preds.append(aus)\n",
        "        for i,ind in enumerate(img):\n",
        "            if label[i][0]>=-1 and label[i][1]>=-1:\n",
        "                total_preds[hInd].append(preds[ind-1])\n",
        "        end = time.time()\n",
        "        timing_results[(isMean, delta)] += end - start\n",
        "total_true=np.array(total_true)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for hInd, (isMean, delta) in enumerate(hyperparams):\n",
        "    preds = np.array(total_preds[hInd])\n",
        "    accuracy = round((preds == total_true).mean(), 3)\n",
        "    f1 = round(np.mean([f1_score(y_true=total_true[:,i],y_pred=preds[:,i]) for i in range(preds.shape[1])]), 3)\n",
        "    mean_or_median = 'mean' if isMean else 'median'\n",
        "    time_taken = round(timing_results[(isMean, delta)],3)\n",
        "    print(f'{mean_or_median}; delta: {delta}; Acc: {accuracy}; F1: {f1}; Time: {time_taken}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXJKgkzozdiW",
        "outputId": "745c06d8-4a2a-4ed6-d51f-19eb4f6c22ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean; delta: 0; Acc: 0.857; F1: 0.523; Time: 7.502s\n",
            "mean; delta: 5; Acc: 0.869; F1: 0.525; Time: 7.965s\n",
            "median; delta: 5; Acc: 0.863; F1: 0.532; Time: 16.839s\n",
            "mean; delta: 15; Acc: 0.871; F1: 0.511; Time: 8.342s\n",
            "median; delta: 15; Acc: 0.866; F1: 0.534; Time: 17.291s\n",
            "mean; delta: 30; Acc: 0.87; F1: 0.492; Time: 8.718s\n",
            "median; delta: 30; Acc: 0.867; F1: 0.528; Time: 17.746s\n",
            "mean; delta: 60; Acc: 0.868; F1: 0.462; Time: 9.44s\n",
            "median; delta: 60; Acc: 0.866; F1: 0.515; Time: 19.077s\n",
            "mean; delta: 100; Acc: 0.864; F1: 0.431; Time: 11.532s\n",
            "median; delta: 100; Acc: 0.864; F1: 0.502; Time: 21.379s\n",
            "mean; delta: 200; Acc: 0.857; F1: 0.39; Time: 13.451s\n",
            "median; delta: 200; Acc: 0.858; F1: 0.472; Time: 25.658s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### EffNet_B2 + EffNet_B0 + Vggish"
      ],
      "metadata": {
        "id": "CrFMHIHawIU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0erTQzVtwIU6"
      },
      "outputs": [],
      "source": [
        "thresholds = np.array([0.8,\n",
        "  0.9,\n",
        "  0.6,\n",
        "  0.5,\n",
        "  0.5,\n",
        "  0.6,\n",
        "  0.7000000000000001,\n",
        "  0.7000000000000001,\n",
        "  0.7000000000000001,\n",
        "  0.8,\n",
        "  0.30000000000000004,\n",
        "  0.7000000000000001])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBeIjxgcwIU7"
      },
      "outputs": [],
      "source": [
        "hyperparams=[(isMean,delta) for delta in [0, 5, 15, 30, 60, 100, 200] for isMean in [1,0] if not (isMean==0 and delta==0)]\n",
        "total_true=[]\n",
        "total_preds=[[] for _ in range(len(hyperparams))]\n",
        "timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n",
        "for videoname,(img, predict, label) in test_vid.items():\n",
        "    for i,ind in enumerate(img):\n",
        "        total_true.append(label[i])\n",
        "    preds_proba = smooth_prediction(img, predict)\n",
        "    for hInd,(isMean,delta) in enumerate(hyperparams):\n",
        "        preds=[]\n",
        "        start = time.time()\n",
        "        for i in range(len(preds_proba)):\n",
        "            i1=max(i-delta,0)\n",
        "            if isMean:\n",
        "                _, proba = slide_window(preds_proba, i, delta, 'mean')\n",
        "            else:\n",
        "                _, proba = slide_window(preds_proba, i, delta, 'median')\n",
        "            aus = (proba>=thresholds)*1\n",
        "            preds.append(aus)\n",
        "        for i,ind in enumerate(img):\n",
        "            if label[i][0]>=-1 and label[i][1]>=-1:\n",
        "                total_preds[hInd].append(preds[ind-1])\n",
        "        end = time.time()\n",
        "        timing_results[(isMean, delta)] += end - start\n",
        "total_true=np.array(total_true)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for hInd, (isMean, delta) in enumerate(hyperparams):\n",
        "    preds = np.array(total_preds[hInd])\n",
        "    accuracy = round((preds == total_true).mean(), 3)\n",
        "    f1 = round(np.mean([f1_score(y_true=total_true[:,i],y_pred=preds[:,i]) for i in range(preds.shape[1])]), 3)\n",
        "    mean_or_median = 'mean' if isMean else 'median'\n",
        "    time_taken = round(timing_results[(isMean, delta)],3)\n",
        "    print(f'{mean_or_median}; delta: {delta}; Acc: {accuracy}; F1: {f1}; Time: {time_taken}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d852f189-bccd-41a5-c9be-cc3496e00c3d",
        "id": "8t7g7D8NwIU7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean; delta: 0; Acc: 0.871; F1: 0.534; Time: 8.697s\n",
            "mean; delta: 5; Acc: 0.878; F1: 0.524; Time: 9.522s\n",
            "median; delta: 5; Acc: 0.875; F1: 0.54; Time: 22.173s\n",
            "mean; delta: 15; Acc: 0.877; F1: 0.499; Time: 9.23s\n",
            "median; delta: 15; Acc: 0.877; F1: 0.535; Time: 19.396s\n",
            "mean; delta: 30; Acc: 0.876; F1: 0.472; Time: 9.172s\n",
            "median; delta: 30; Acc: 0.877; F1: 0.524; Time: 19.412s\n",
            "mean; delta: 60; Acc: 0.872; F1: 0.437; Time: 9.686s\n",
            "median; delta: 60; Acc: 0.874; F1: 0.506; Time: 20.104s\n",
            "mean; delta: 100; Acc: 0.867; F1: 0.41; Time: 10.824s\n",
            "median; delta: 100; Acc: 0.871; F1: 0.489; Time: 22.654s\n",
            "mean; delta: 200; Acc: 0.86; F1: 0.368; Time: 14.02s\n",
            "median; delta: 200; Acc: 0.864; F1: 0.457; Time: 27.107s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWSTa4TEzNLi"
      },
      "source": [
        "## VA Estimation Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D1rOOtUPGJS"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PpOnAdBZHEf"
      },
      "outputs": [],
      "source": [
        "# Cropped_aligned images\n",
        "vis = vis_typ[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1E65uF_ZHEl"
      },
      "outputs": [],
      "source": [
        "# Cropped images\n",
        "vis = vis_typ[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6loHg0Oaml6s"
      },
      "source": [
        "#### Effnet + wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qaoMmvejsdR"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[0]\n",
        "viau = vis_aud[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grJntlFqms8l"
      },
      "source": [
        "#### Effnet + vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhVH9IjHmruz"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[1]\n",
        "viau = vis_aud[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SLN9gglm0XA"
      },
      "source": [
        "#### Effnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4LyVKQQm23-"
      },
      "outputs": [],
      "source": [
        "auft = audio_feat[2]\n",
        "viau = vis_aud[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM-R-dPNZRXk"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiWBsFEXZRXk"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root,f'models/ABAW6/{vis}/VA/{task[2]}_{typ[2]}_{viau}.pkl'), 'rb') as f:\n",
        "    data3 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUNvkmldZRXk",
        "outputId": "997b759c-c937-47b6-e3ab-872568ce835d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 1175.35it/s]\n"
          ]
        }
      ],
      "source": [
        "dims = 0\n",
        "iname = []\n",
        "task1 = task[2]\n",
        "with open(os.path.join(root, f'data/Annotations/{task[2]}/{typ[2]}.txt'), 'r') as f:\n",
        "    vidnames = f.read().splitlines()\n",
        "for vname in tqdm(vidnames):\n",
        "    dims += len(data3[task1][vname])\n",
        "    for img in data3[task1][vname].keys():\n",
        "        iname.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAm67bXaZRXk"
      },
      "outputs": [],
      "source": [
        "dataset = ABAW_dataset1(data3, iname, dims, task1)\n",
        "test_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo7YJPxtPMXQ"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVrhnVSdrkn9"
      },
      "outputs": [],
      "source": [
        "class VA_fusion(nn.Module):\n",
        "    def __init__(self, batchsize = batch_size, audio_ft = auft, hidden_size = [512, 128, batch_size]):\n",
        "        super(VA_fusion, self).__init__()\n",
        "        self.batchsize = batchsize\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.hidden_size = hidden_size\n",
        "        self.feat_fc = nn.Conv1d(self.concat_dim, hidden_size[0], 1, padding=0)\n",
        "        self.activ = nn.LeakyReLU(0.1)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.conv1 = nn.Conv1d(hidden_size[0], hidden_size[1], 1, padding=0)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size[1], nhead=4, dim_feedforward=hidden_size[1], dropout=0.3)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "        self.vhead = nn.Sequential(\n",
        "                nn.Linear(hidden_size[1], hidden_size[2]),\n",
        "                nn.BatchNorm1d(hidden_size[2]),\n",
        "                nn.Linear(hidden_size[2], 1),\n",
        "                )\n",
        "        self.ahead = nn.Sequential(\n",
        "                nn.Linear(hidden_size[1], hidden_size[2]),\n",
        "                nn.BatchNorm1d(hidden_size[2]),\n",
        "                nn.Linear(hidden_size[2], 1),\n",
        "                )\n",
        "\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs,dim=1)\n",
        "        feat = torch.transpose(feat,0,1)\n",
        "        feat = self.feat_fc(feat)\n",
        "        feat = self.activ(feat)\n",
        "        out = self.conv1(feat)\n",
        "        out = torch.transpose(out,0,1)\n",
        "        out = self.transformer_encoder(out)\n",
        "        vout = self.vhead(out)\n",
        "        aout = self.ahead(out)\n",
        "\n",
        "        return vout, aout, torch.tanh(vout), torch.tanh(aout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGReEGQ_PQb7",
        "outputId": "e16eae90-8806-4872-904c-d943d0bb3417"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VA_fusion(\n",
              "  (feat_fc): Conv1d(2176, 512, kernel_size=(1,), stride=(1,))\n",
              "  (activ): LeakyReLU(negative_slope=0.1)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (vhead): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              "  (ahead): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "VA_model = VA_fusion().to(device)\n",
        "VA_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE3lHnVrxBMl"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, audio_ft = auft, num_classes=1):\n",
        "        super(MLPModel, self).__init__()\n",
        "        if audio_ft == 'audiofeat_wav2vec2':\n",
        "            self.concat_dim = 2176    #1408+768\n",
        "        elif audio_ft == 'audiofeat_vggish':\n",
        "            self.concat_dim = 1536    #1408+128\n",
        "        elif audio_ft == 'nope':\n",
        "            self.concat_dim = 1408    #visual only\n",
        "        self.feat_fc = nn.Conv1d(self.concat_dim, 512, 1, padding=0)\n",
        "        self.vhead = nn.Sequential(\n",
        "            nn.Linear(512, 128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        self.ahead = nn.Sequential(\n",
        "            nn.Linear(self.concat_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, vis_feat, aud_feat):\n",
        "        if aud_feat == None:\n",
        "            feat = vis_feat\n",
        "        else:\n",
        "            inputs = [vis_feat]\n",
        "            inputs.append(aud_feat)\n",
        "            feat = torch.cat(inputs, dim=1)\n",
        "        vfeat = self.feat_fc(torch.transpose(feat,0,1))\n",
        "        vfeat = torch.transpose(vfeat,0,1)\n",
        "        vout = self.vhead(vfeat)\n",
        "        aout = self.ahead(feat)\n",
        "\n",
        "        return vout, aout, torch.tanh(vout), torch.tanh(aout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM0fnofbXuz5",
        "outputId": "9cb06aae-0317-4ac1-820d-8141f32f89fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPModel(\n",
              "  (feat_fc): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n",
              "  (vhead): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.1)\n",
              "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (ahead): Sequential(\n",
              "    (0): Linear(in_features=1536, out_features=128, bias=True)\n",
              "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJLovdcDNKGK"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, au_feat):\n",
        "    model.eval()\n",
        "    total_loss = []\n",
        "    all_targets = []\n",
        "    all_preds = []\n",
        "    mse = []\n",
        "    with torch.no_grad():\n",
        "        iterator = iter(data_loader)\n",
        "        for i in range(len(data_loader)):\n",
        "            VA = next(iterator)\n",
        "            if au_feat == 'nope':\n",
        "                vis_feat, y = VA[visual_feat], VA['label']\n",
        "                vis_feat, y = vis_feat.to(device), y.to(device)\n",
        "                aud_feat = None\n",
        "            else:\n",
        "                vis_feat, aud_feat, y = VA[visual_feat], VA[au_feat], VA['label']\n",
        "                vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "            Vpred, Apred, v_pred, a_pred = model(vis_feat, aud_feat)\n",
        "            mse_loss, ccc_loss = compute_VA_loss(Vpred, Apred, y)\n",
        "            total_loss.append(ccc_loss.item())\n",
        "            mse.append(mse_loss.item())\n",
        "            preds = torch.cat((v_pred, a_pred), dim=1)\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_targets.extend(y.cpu().tolist())\n",
        "\n",
        "    ccc1, ccc2 = compute_VA_CCC(all_preds, all_targets)\n",
        "    return round(np.mean(total_loss),3), round(np.mean(mse),3), round(ccc1,3), round(ccc2,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew3FhCIJpw8_"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ns5iRAm4oYx"
      },
      "source": [
        "##### EffNet + Wav2vec2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('VA_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_fusion_{viau}.pth'))\n",
        "VA_model = VA_fusion().to(device)\n",
        "VA_model.load_state_dict(VA_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPvOsc7N_8H0",
        "outputId": "1ffad90f-0558-47d0-f63d-d7e2b1100da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: CCC_Valence 0.294, CCC_Arousal: 0.495, mean CCC: 0.394\n",
            "58.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV42AtZG6TeT",
        "outputId": "84e6f24d-8a36-4889-ea8b-fde58389597b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_wav2vec2\n",
            "Test set: CCC_Valence 0.391, CCC_Arousal: 0.527, mean CCC: 0.459\n",
            "29 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt5aLtmJOpAo"
      },
      "source": [
        "##### EffNet + Vggish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f71d80-a4c1-45cc-c9fd-c08f9e9d6736",
        "id": "JyoYc56hTST_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: CCC_Valence 0.293, CCC_Arousal: 0.458, mean CCC: 0.376\n",
            "57.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('VA_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_fusion_{viau}.pth'))\n",
        "VA_model = VA_fusion().to(device)\n",
        "VA_model.load_state_dict(VA_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5adc2300-5014-42df-8144-bb47ef3076b8",
        "id": "at7sa9hFTd3p"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & audiofeat_vggish\n",
            "Test set: CCC_Valence 0.365, CCC_Arousal: 0.516, mean CCC: 0.44\n",
            "28.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjhZnJHIVTYi"
      },
      "source": [
        "##### EffNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83eb007a-b4f2-4f73-8648-2e04215f2faa",
        "id": "sV4dFA244oYy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VA_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: CCC_Valence 0.27, CCC_Arousal: 0.377, mean CCC: 0.324\n",
            "55.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('VA_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "VA_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_fusion_{viau}.pth'))\n",
        "VA_model = VA_fusion().to(device)\n",
        "VA_model.load_state_dict(VA_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(VA_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqlMzaNU6F59",
        "outputId": "622fa89a-00f4-4fa7-b91e-29e9b71e8aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP_model\n",
            "visualfeat_enet_b2_8_best & nope\n",
            "Test set: CCC_Valence 0.366, CCC_Arousal: 0.464, mean CCC: 0.415\n",
            "26.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "print('MLP_model')\n",
        "print(visual_feat + ' & ' + auft)\n",
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n",
        "mlp_model = MLPModel().to(device)\n",
        "mlp_model.load_state_dict(mlp_best_model)\n",
        "test_loss, mse, ccc1, ccc2 = evaluate_model(mlp_model, test_loader, auft)\n",
        "print(f'Test set: CCC_Valence {ccc1}, CCC_Arousal: {ccc2}, mean CCC: {round((ccc1+ccc2)/2,3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensembling"
      ],
      "metadata": {
        "id": "gUqT9ZA9Z40c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EffNet_B2 + Wav2vec2"
      ],
      "metadata": {
        "id": "iSAgNzqdZ40d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wav2vec2\n",
        "vis = vis_typ[0]\n",
        "auft = audio_feat[0]\n",
        "viau = vis_aud[0]"
      ],
      "metadata": {
        "id": "8RtLom-RZ7FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_best_model = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n",
        "mlp_model_wav = MLPModel().to(device)\n",
        "mlp_model_wav.load_state_dict(mlp_best_model)\n",
        "evaluate_model(mlp_model_wav, test_loader, auft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6yJ-D9Udfda",
        "outputId": "a5471c7a-b754-4f9f-aee7-276346f73ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.488, 0.276, 0.391, 0.527)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(test_loader)\n",
        "all_preds, all_targets = [], []\n",
        "img1 = []\n",
        "for i in range(len(test_loader)):\n",
        "    VA = next(iterator)\n",
        "    frame = VA['frame']\n",
        "    for imgname in frame:\n",
        "        img1.append(imgname)\n",
        "    vis_feat, aud_feat, y = VA[visual_feat], VA[auft], VA['label']\n",
        "    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "    Vpred, Apred, v_pred, a_pred = mlp_model_wav(vis_feat, aud_feat)\n",
        "    preds = torch.cat((v_pred, a_pred), dim=1)\n",
        "    all_preds.extend(preds.cpu().tolist())\n",
        "    all_targets.extend(y.cpu().tolist())"
      ],
      "metadata": {
        "id": "9n_-zAcaZ40d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_VA_CCC(all_preds, all_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFk2TB74b2xB",
        "outputId": "9fda28d0-2b9a-422d-f0ec-f21b7d7e4a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3908915320970614, 0.5266098927052195)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EffNet_B2 + Vggish"
      ],
      "metadata": {
        "id": "jN-gJd1dZ40d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_best_mod = torch.load(os.path.join(root,f'models/ABAW6/{vis}/best_VA_model/best_VA_mlp_{viau}.pth'))\n",
        "mlp_model_vgg = MLPModel().to(device)\n",
        "mlp_model_vgg.load_state_dict(mlp_best_mod)\n",
        "evaluate_model(mlp_model_vgg, test_loader, auft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFFBWqdafHR3",
        "outputId": "368b5718-a309-4bc4-c51e-056073e3b676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.483, 0.287, 0.354, 0.533)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(test_loader)\n",
        "all_preds_vgg, all_targets_vgg = [], []\n",
        "img = []\n",
        "for i in range(len(test_loader)):\n",
        "    VA = next(iterator)\n",
        "    frame = VA['frame']\n",
        "    for imgname in frame:\n",
        "        img.append(imgname)\n",
        "    vis_feat, aud_feat, y = VA[visual_feat], VA[auft], VA['label']\n",
        "    vis_feat, aud_feat, y = vis_feat.to(device), aud_feat.to(device), y.to(device)\n",
        "    Vpred, Apred, v_pred, a_pred = mlp_model_vgg(vis_feat, aud_feat)\n",
        "    preds = torch.cat((v_pred, a_pred), dim=1)\n",
        "    all_preds_vgg.extend(preds.cpu().tolist())\n",
        "    all_targets_vgg.extend(y.cpu().tolist())"
      ],
      "metadata": {
        "id": "GwGYrnWcZ40e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_VA_CCC(all_preds_vgg, all_targets_vgg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpheRjTmf9NP",
        "outputId": "22759895-6a04-4aa2-e72d-f366cfa19342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.35368417669976915, 0.53293580621636)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ensemble"
      ],
      "metadata": {
        "id": "dCpokCoNZ40e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zipped_data = zip(img1, all_preds, all_targets)\n",
        "zipped_data_vgg = zip(img, all_preds_vgg, all_targets_vgg)\n",
        "\n",
        "sorted_data = sorted(zipped_data)\n",
        "sorted_data_vgg = sorted(zipped_data_vgg)\n",
        "\n",
        "re_img_vgg, re_all_preds_vgg, re_all_targets_vgg = zip(*sorted_data_vgg)\n",
        "re_img, re_all_preds, re_all_targets = zip(*sorted_data)"
      ],
      "metadata": {
        "id": "feHjSBg-Z40e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_VA_CCC(re_all_preds_vgg, re_all_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51eea9d-9e33-401f-c833-a69d3a698f14",
        "id": "rsoJrx8bZ40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3536841766997691, 0.53293580621636)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w in np.linspace(0, 1, 11):\n",
        "    print(f'{w = }')\n",
        "    w = np.array([w])\n",
        "    y_ensemble = w * re_all_preds + (1 - w) * re_all_preds_vgg\n",
        "    cccv, ccca = compute_VA_CCC(y_ensemble, re_all_targets)\n",
        "    print(f'CCCV: {cccv:.3f}, CCCA: {ccca:.3f}, Mean CCCC: {(cccv+ccca)/2:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80f4399-aa9e-4f20-ea6f-52e0fac2659b",
        "id": "TDOqsyVHZ40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0.0\n",
            "CCCV: 0.354, CCCA: 0.533, Mean CCCC: 0.443\n",
            "w = 0.1\n",
            "CCCV: 0.364, CCCA: 0.538, Mean CCCC: 0.451\n",
            "w = 0.2\n",
            "CCCV: 0.374, CCCA: 0.541, Mean CCCC: 0.458\n",
            "w = 0.30000000000000004\n",
            "CCCV: 0.382, CCCA: 0.544, Mean CCCC: 0.463\n",
            "w = 0.4\n",
            "CCCV: 0.389, CCCA: 0.545, Mean CCCC: 0.467\n",
            "w = 0.5\n",
            "CCCV: 0.393, CCCA: 0.545, Mean CCCC: 0.469\n",
            "w = 0.6000000000000001\n",
            "CCCV: 0.396, CCCA: 0.544, Mean CCCC: 0.470\n",
            "w = 0.7000000000000001\n",
            "CCCV: 0.398, CCCA: 0.541, Mean CCCC: 0.470\n",
            "w = 0.8\n",
            "CCCV: 0.397, CCCA: 0.538, Mean CCCC: 0.467\n",
            "w = 0.9\n",
            "CCCV: 0.395, CCCA: 0.533, Mean CCCC: 0.464\n",
            "w = 1.0\n",
            "CCCV: 0.391, CCCA: 0.527, Mean CCCC: 0.459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_vid = zip(img1, y_ensemble, re_all_targets)"
      ],
      "metadata": {
        "id": "v35SaX4jZ40f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight=np.array([0.6])\n",
        "y_ensemble = weight * re_all_preds + (1 - weight) * re_all_preds_vgg\n",
        "cccv, ccca = compute_VA_CCC(y_ensemble, re_all_targets)\n",
        "print(f'CCCV: {cccv:.3f}, CCCA: {ccca:.3f}, Mean CCCC: {(cccv+ccca)/2:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45001d6a-2229-4358-c6f0-e2cf5a1a4355",
        "id": "DtzxzId4Z40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CCCV: 0.396, CCCA: 0.544, Mean CCCC: 0.470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmE8Fo_YSyzW"
      },
      "source": [
        "### Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index, pred, lab = [], [], []\n",
        "test_vid = {}\n",
        "for i, val in enumerate(re_img):\n",
        "    ind = int(val.split('/')[1][:-4])\n",
        "    vname = val.split('/')[0]\n",
        "    if i == 0:\n",
        "        prename = vname\n",
        "        index.append(ind)\n",
        "        pred.append(y_ensemble[i])\n",
        "        lab.append(re_all_targets[i])\n",
        "    else:\n",
        "        if vname == prename:\n",
        "            index.append(ind)\n",
        "            pred.append(y_ensemble[i])\n",
        "            lab.append(re_all_targets[i])\n",
        "        else:\n",
        "            combined = list(zip(index, pred, lab))\n",
        "            combined_sorted = sorted(combined, key=lambda x: x[0])\n",
        "            index_list_sorted, pred_list_sorted, lab_list_sorted = zip(*combined_sorted)\n",
        "            test_vid[prename] = (list(index_list_sorted), list(pred_list_sorted), list(lab_list_sorted))\n",
        "            prename = vname\n",
        "            index, pred, lab = [], [], []\n",
        "            index.append(ind)\n",
        "            pred.append(y_ensemble[i])\n",
        "            lab.append(re_all_targets[i])"
      ],
      "metadata": {
        "id": "ZT-I9tDYhhTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69oAXZnpax2r"
      },
      "outputs": [],
      "source": [
        "hyperparams=[(isMean,delta) for delta in [0, 5, 15, 30, 60, 100, 200] for isMean in [1,0] if not (isMean==0 and delta==0)]\n",
        "total_true=[]\n",
        "total_preds=[[] for _ in range(len(hyperparams))]\n",
        "timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n",
        "for videoname,(img, predict, label) in test_vid.items():\n",
        "    for i,ind in enumerate(img):\n",
        "        total_true.append(label[i])\n",
        "    preds_proba = smooth_prediction(img, predict)\n",
        "    for hInd,(isMean,delta) in enumerate(hyperparams):\n",
        "        preds=[]\n",
        "        start = time.time()\n",
        "        for i in range(len(preds_proba)):\n",
        "            i1=max(i-delta,0)\n",
        "            if isMean:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'mean')\n",
        "            else:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'median')\n",
        "            preds.append(proba)\n",
        "        for i, ind in enumerate(img):\n",
        "            if label[i][0]>=-1 and label[i][1]>=-1:\n",
        "                total_preds[hInd].append(preds[ind-1])\n",
        "        end = time.time()\n",
        "        timing_results[(isMean, delta)] += end - start\n",
        "total_true=np.array(total_true)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams=[(isMean,delta) for delta in [15, 20, 25, 30, 35, 40] for isMean in [1,0] if not (isMean==0 and delta==0)]\n",
        "total_true=[]\n",
        "total_preds=[[] for _ in range(len(hyperparams))]\n",
        "timing_results = {(isMean, delta): 0 for isMean, delta in hyperparams}\n",
        "for videoname,(img, predict, label) in test_vid.items():\n",
        "    for i,ind in enumerate(img):\n",
        "        total_true.append(label[i])\n",
        "    preds_proba = smooth_prediction(img, predict)\n",
        "    for hInd,(isMean,delta) in enumerate(hyperparams):\n",
        "        preds=[]\n",
        "        start = time.time()\n",
        "        for i in range(len(preds_proba)):\n",
        "            i1=max(i-delta,0)\n",
        "            if isMean:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'mean')\n",
        "            else:\n",
        "                best_ind, proba = slide_window(preds_proba, i, delta, 'median')\n",
        "            preds.append(proba)\n",
        "        for i, ind in enumerate(img):\n",
        "            if label[i][0]>=-1 and label[i][1]>=-1:\n",
        "                total_preds[hInd].append(preds[ind-1])\n",
        "        end = time.time()\n",
        "        timing_results[(isMean, delta)] += end - start\n",
        "total_true=np.array(total_true)"
      ],
      "metadata": {
        "id": "RaZ6omEPW0_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for hInd, (isMean, delta) in enumerate(hyperparams):\n",
        "    preds = np.array(total_preds[hInd])\n",
        "    ccc1, ccc2 = compute_VA_CCC(preds, total_true)\n",
        "    mean_or_median = 'mean' if isMean else 'median'\n",
        "    time_taken = round(timing_results[(isMean, delta)],3)\n",
        "    print(f'{mean_or_median}; delta: {delta}; CCCV: {ccc1:.3f}; CCCA: {ccc2:.3f}; Mean CCC: {(ccc1+ccc2)/2:.3f} Time: {time_taken}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpS0ik2UiFij",
        "outputId": "12b4e1e0-adb7-49c5-e3c9-dca88c633eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean; delta: 0; CCCV: 0.399; CCCA: 0.546; Mean CCC: 0.473 Time: 5.635s\n",
            "mean; delta: 5; CCCV: 0.411; CCCA: 0.569; Mean CCC: 0.490 Time: 6.035s\n",
            "median; delta: 5; CCCV: 0.411; CCCA: 0.567; Mean CCC: 0.489 Time: 13.1s\n",
            "mean; delta: 15; CCCV: 0.418; CCCA: 0.585; Mean CCC: 0.502 Time: 5.745s\n",
            "median; delta: 15; CCCV: 0.419; CCCA: 0.584; Mean CCC: 0.502 Time: 13.37s\n",
            "mean; delta: 30; CCCV: 0.421; CCCA: 0.594; Mean CCC: 0.507 Time: 5.922s\n",
            "median; delta: 30; CCCV: 0.424; CCCA: 0.596; Mean CCC: 0.510 Time: 13.567s\n",
            "mean; delta: 60; CCCV: 0.414; CCCA: 0.585; Mean CCC: 0.500 Time: 6.428s\n",
            "median; delta: 60; CCCV: 0.420; CCCA: 0.594; Mean CCC: 0.507 Time: 14.063s\n",
            "mean; delta: 100; CCCV: 0.402; CCCA: 0.562; Mean CCC: 0.482 Time: 6.961s\n",
            "median; delta: 100; CCCV: 0.409; CCCA: 0.575; Mean CCC: 0.492 Time: 14.75s\n",
            "mean; delta: 200; CCCV: 0.377; CCCA: 0.516; Mean CCC: 0.447 Time: 7.423s\n",
            "median; delta: 200; CCCV: 0.379; CCCA: 0.531; Mean CCC: 0.455 Time: 16.118s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "97AGawC8PnRo",
        "pmzvE5czs3hX",
        "HoofZGhrU6Ih",
        "Kjx794E5bdTe",
        "ibOhaO0vAOvH",
        "sd8sIrCrwZIT",
        "RsUw4AxuwtBo",
        "8vFUXiAyjdSs",
        "47UnYX3TjhOd",
        "0_2hrWvEYRTB",
        "7twP-V0AHU5w"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}